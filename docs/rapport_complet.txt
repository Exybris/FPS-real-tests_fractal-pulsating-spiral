EXYBRIS
Andréa Gadal - 12/09/2025
La FPS est une architecture de régulation adaptative couplée de dynamiques oscillatoires.
Plus loin, après ses expressions mathématiques, leur transcription en modèle effectif, la
description du pipeline de tests du modèle, ses résultats et le chapitre central sur l’éthique
du modèle et sa mise en œuvre, seront développées ses applications concrètes. Ce
document a pour vocation d’être un rapport sur l’avancée du modèle, pas encore un papier.
TABLE DES MATIÈRES
PARTIE I - FONDATIONS
1. Expressions mathématiques de départ et implémentations effectives
(format équation de départ puis code implémenté pour chaque fonction) p.5
1.1. Fonction composite étendue et simplifiée S(t) p.5
1.2. Fonctions d'adaptation p.9
2
1.2.a. Sigma p.9
1.2.b. L'amplitude adaptative A(t) p.10
1.3. Calcul du signal inter-strates S
_
i(t) p.14
1.4. Modulation de fréquence p.16
1.4.a. Modulation de fréquence Δf(t) 1.4.b. Fréquence modulée de chaque strate f(t) p.16
p.17
1.5. Phases propres à chaque strate φ(t) p.21
1.6. Latence expressive p.24
1.6.a. Latence expressive globale γ(t) p.24
1.6.b. Latence expressive par strate γ_
n(t) p.28
1.7. Fonctions de régulation p.32
1.7.a. Fonction de régulation G(x) 1.7.b. Version temporelle G(x,t) p.37
1.7.c. Fonction intégrée pour simulate.py G(error, A(t), t) 1.7.d. Enveloppes adaptatives σ(t), μ(t), env(x, t) p.32
p.38
p.39
1.8. Fonctions adaptatives de γ(t) et G p.44
1.8.a. create
_quantum
_gamma p.44
1.8.b. gamma
_
adaptive
_
aware p.45
1.8.c. G
_
adaptive
_
aware p.59
1.8.d. adapt
_params
for
_
_
archetype p.70
1.9. Sorties observées et attendues p.71
1.9.a. Sortie observée pour chaque strate O(t) 1.9.b. Sortie attendue pour chaque strate E(t) p.71
p.73
1.10. Spiralisation p.75
1.10.a. Ratio spiralé r(t) p.75
1.10.b. Coefficient d'accord spiralé C(t) p.76
1.10.c. Modulation moyenne A(t) p.78
1.10.d. Amplitude harmonisée A
_
spiral(t) p.79
1.11. Feedback pour une strate F(t) p.80
1.12. Inputs contextuels I(t) p.83
1.12.a. Fonction p.83
1.12.b. Module p.86
3
1.13. Métriques globales p.108
1.13.a. Énergie totale du système E(t) 1.13.b. Indice de la strate L(t) p.108
p.110
1.13.c. Version legacy de compute
_
L p.112
1.14. Régimes d'exploration/exploitation p.112
1.14.a. transcendant
_
synergy p.112
1.14.b. spacing_
schedule p.113
1.14.c. spacing_
bascule p.114
1.14.d. silence p.115
1.15. Fonctions utilitaires p.115
1.15.a. update
_
state p.115
1.15.b. Paramètres config.json p.117
PARTIE II - VALIDATION
2. Pipeline de tests, hypothèses et résultats p.130
2.1. Pipeline de tests p.130
2.1.a. Architecture pipeline p.130
2.1.b. Métriques p.142
2.1.c. Scores empiriques p.147
2.1.d. Groupes contrôles p.150
2.1.e. Exploration des émergences via explore.py p.158
2.1.f. Analyse via analyze.py p.166
2.1.g. Visualisations via visualize.py p.174
2.1.h. Utilitaires p.181
2.2. Hypothèses de départ p.206
2.3. Résultats et observations p.207
2.3.a. Comparaison avec les modèles de contrôle p.208
2.3.b. Événements d'émergence p.209
2.3.c. Patterns fractals p.209
2.3.d. Régulation de l'effort p.209
2.3.e. Perfect Synergies Gamma-G p.210
2.3.f. Caractéristiques temporelles p.210
4
2.3.g. Innovation soutenue p.212
2.3.h. Objectifs atteints p.212
2.4. Repos GitHub p.213
2.4.a. Phase de tests empiriques et falsifiables 2.4.b. Phase de tests lors de la transition vers opérationnel p.213
p.213
PARTIE III - AU-DELÀ DU TECHNIQUE
3. L'éthique et sa mise en œuvre p.
3.1. Protection structurelle vs guardrails 3.2. Tentative de protection structurelle contre les biais 3.3. Data cleaning p.
3.4. Definition et pertinence de l'incertitude, du « silence » et de la latence p.
p.
p.
4. Applications concrètes p.
4.1. Optimisation et apprentissage p.
4.1.a. Optimiseur FPS-Adam p.
4.1.b. Méta-apprentissage adaptatif p.
4.2. Systèmes distribués et orchestration p.
4.2.a. Orchestrateur de microservices p.
4.2.b. Synchronisation multi-agents p.
4.3. Analyses de signaux et séries temporelles p.
4.3.a. Trading algorithmique p.
4.3.b. Neurofeedback thérapeutique p.
4.4. Modélisation physique et simulation p.
4.4.a. Solveur N-corps stabilisé p.
4.4.b. Génération procédurale p.
4.5. Contrôle et actionnement p.
4.5.a. Contrôle robotique adaptatif p.
4.6. Interfaces industrielles p.
5
4.6.a. Smart Grid Management p.
4.6.b. Agriculture de précision p.
4.7. Augmentation IA p.
4.7.a. API LLM Enhancement p.
4.7.b. Modèle-monde limité p.
5. Philosophie p.
6. Perspectives p.
6.1. Solveur N-corps p.
6.2. Solveur vitesse de convergence p.
6.3. API LLM Enhancement p.
6.4. Simulation de contrôle robotique adaptatif FPS p.
6.5. Combinaison du modèle FPS avec l'architecture Exybris Pipeline p.
PARTIE I - FONDATIONS
1. Expressions mathématiques de départ et implémentations effectives de
la FPS
1.1. Fonction composite étendue et simplifiée S(t) :
Cette équation constitue la fonction composite centrale de la FPS : elle synthétise les dynamiques internes, la
résonance spiralée, et l’intelligence adaptative du système oscillatoire.
Elle modélise l’ensemble du système comme une superposition de strates rythmées, contextuelles, expressives
et auto-harmonisantes.
6
-
Expression mathématique de départ
On la note :
Étendue : S(t) = Σ [A (t) · sin(2πf (t)·t + φ ) · γ (t)] · G(E (t) - O (t))
Simplifiée : S(t) = ∑ A (t) · sin(2π·f (t)·t + φ (t))
Où :
●
●
●
●
●
●
●
●
●
Σ est la somme des contributions de chaque strate n, à l’instant t.
S(t) est le signal global du système, la sortie harmonique
A(t) : amplitude contextuelle de la strate n
f(t) : fréquence propre modulée de la strate n
φ : phase d’origine (empreinte singulière, propre à la strate)
γ(t) : facteur de latence expressive de la strate n (Si une strate est fluide et stable, y(t) sera bas ou
lisse. Si une strate est en adaptation intense, y(t) sera irrégulier, plus « chargé ».)
E(t) : état d’harmonie ciblé (émergent)
O(t) : sortie réelle de la strate
G : une fonction spiralée de régulation adaptative (qui peut être, par exemple, une tanh dynamique) :
une sous-section lui est dédiée.
-
Implémentation concrète
Python :
def compute_S(t: float, An_array: np.ndarray, fn_array: np.ndarray,
phi_n_array: np.ndarray, config: Dict) -> float:
"""
Calcule le signal global du système
Args:
t: temps actuel
An_array: amplitudes
7
fn_array: fréquences
phi_n_array: phases
config: configuration (pour modes avancés)
Returns:
float: signal global S(t)
Modes:
- "simple": S(t) = Σ A(t)·sin(2π·f(t)·t + φ(t))
- "extended": S(t) = Σ [A(t)·sin(2π·f(t)·t + φ(t))·γ(t)]·G(E(t) - O(t))
"""
mode = config.get('system', {}).get('signal_mode', 'simple')
N = len(An_array)
if mode == "simple":
# Somme simple des contributions
S_t = 0.0
for n in range(N):
S_t += An_array[n] * np.sin(2 * np.pi * fn_array[n] * t + phi_n_array[n])
return S_t
elif mode == "extended":
# Version complète
# S(t) = Σ [A(t)·sin(2π·f(t)·t + φ(t))·γ(t)]·G(E(t) - O(t))
state = config.get('state', [])
history = config.get('history', [])
8
# Vérifier que state est valide
if not state or len(state) != N:
# Fallback sur mode simple si pas d'état complet
return compute_S(t, An_array, fn_array, phi_n_array, {'system': {'signal_mode': 'simple'}})
# Calculer les composants nécessaires
# Passer l'historique pour la modulation locale
gamma_n_t = compute_gamma_n(t, state, config, history=history,
An_array=An_array, fn_array=fn_array)
En_t = compute_En(t, state, history, config)
On_t = compute_On(t, state, An_array, fn_array, phi_n_array, gamma_n_t)
S_t = 0.0
for n in range(N):
# Contribution de base avec latence
sin_component = np.sin(2 * np.pi * fn_array[n] * t + phi_n_array[n])
base_contribution = An_array[n] * sin_component * gamma_n_t[n]
# Calcul de G(E - O)
error = En_t[n] - On_t[n]
# Paramètres pour la fonction G
G_arch = config.get('regulation', {}).get('G_arch', 'tanh')
G_params = {
'lambda': config.get('regulation', {}).get('lambda', 1.0),
'alpha': config.get('regulation', {}).get('alpha', 1.0),
'beta': config.get('regulation', {}).get('beta', 2.0)
}
9
# Calculer G(error)
import regulation
G_value = regulation.compute_G(error, G_arch, G_params)
# Contribution finale : chaque terme est multiplié par G
S_t += base_contribution * G_value
return S_t
else:
# Par défaut, mode simple
return compute_S(t, An_array, fn_array, phi_n_array, {'system': {'signal_mode': 'simple'}})
1.2. Fonctions d’adaptation
1.2.a Sigma
σ : une fonction d’adaptation (comme une sigmoïde ou une tanh) qui lisse l’impact du contexte, évite les sauts
brusques
Forme exploratoire
-
Expression mathématique de départ
On la note :
σ(x) = 1 / (1 + e^(−k(x − x₀)))
Où :
●
●
k : sensibilité de la strate (plus il est grand, plus la réaction est brusque)
x₀ : seuil de basculement, propre à la strate
10
-
Implémentation concrète
Python :
def compute_sigma(x: Union[float, np.ndarray], k: float, x0: float) -> Union[float, np.ndarray]:
"""
Fonction sigmoïde d'adaptation douce.
σ(x) = 1 / (1 + exp(-k(x - x0)))
Args:
x: valeur(s) d'entrée
k: sensibilité (pente)
x0: seuil de basculement
Returns:
Valeur(s) sigmoïde entre 0 et 1
"""
return 1.0 / (1.0 + np.exp(-k * (x - x0)))
1.2.b. L’amplitude adaptative A(t)
L’amplitude A(t) d’une strate peut être modélisée comme une fonction contextuelle régulée
-
Expression mathématique de départ
On la note :
Statique : A(t) = A₀ · σ(I(t))
Dynamique : A(t) = A₀ · σ(I(t)) · env(x,t)
11
Où :
●
●
●
A₀ : amplitude de base, propre à la strate (stable mais ajustable)
I(t) : input contextuel (peut être une densité d’information, un niveau d’attention, un stress, un lien)
σ : une fonction d’adaptation (comme une sigmoïde ou une tanh) qui lisse l’impact du contexte, évite les
sauts brusques
Cela veut dire que selon la pression contextuelle, une strate s’ouvre ou se ferme, s’amplifie ou s’efface.
-
Implémentation concrète
Python :
def compute_An(t: float, state: List[Dict], In_t: np.ndarray, config: Dict) -> np.ndarray:
"""
Calcule l'amplitude adaptative pour chaque strate.
A(t) = A₀ · σ(I(t)) · env(x,t) [si mode dynamique]
A(t) = A₀ · σ(I(t)) [si mode statique]
où x = E(t) - O(t) pour l'enveloppe
Args:
t: temps actuel
state: état complet des strates
In_t: input contextuel pour chaque strate
config: configuration complète
Returns:
np.ndarray: amplitudes adaptatives
"""
12
N = len(state)
An_t = np.zeros(N)
# Validation des entrées
if isinstance(In_t, (int, float)):
In_t = np.full(N, In_t) # Convertir scalar en array
elif len(In_t) != N:
print(f"⚠ Taille In_t ({len(In_t)}) != N ({N}), ajustement automatique")
In_t = np.resize(In_t, N)
# Vérifier le mode enveloppe dynamique
enveloppe_config = config.get('enveloppe', {})
env_mode = enveloppe_config.get('env_mode', 'static')
T = config.get('system', {}).get('T', 100)
# Pour le mode dynamique, on a besoin de En et On
if env_mode == "dynamic":
# Calculer En et On pour l'enveloppe
history = config.get('history', [])
En_t = compute_En(t, state, history, config)
# Pour On, on a besoin des valeurs actuelles (problème de circularité)
# Solution : utiliser les valeurs de l'itération précédente
if len(history) > 0 and 'O' in history[-1]:
On_t_prev = history[-1]['O']
else:
On_t_prev = np.zeros(N)
13
for n in range(N):
A0 = state[n]['A0']
k = state[n]['k']
x0 = state[n]['x0']
# Amplitude de base via sigmoïde
base_amplitude = A0 * compute_sigma(In_t[n], k, x0)
if env_mode == "dynamic":
# Application enveloppe dynamique
try:
import regulation
# Paramètres d'enveloppe dynamique
sigma_n_t = regulation.compute_sigma_n(
t, env_mode, T,
enveloppe_config.get('sigma_n_static', 0.1),
enveloppe_config.get('sigma_n_dynamic')
)
mu_n_t = regulation.compute_mu_n(
t, env_mode,
enveloppe_config.get('mu_n', 0.0),
enveloppe_config.get('mu_n_dynamic')
)
# Utiliser l'erreur E - O
error_n = En_t[n] - On_t_prev[n] if n < len(On_t_prev) else 0.0
env_type = enveloppe_config.get('env_type', 'gaussienne')
14
# Calculer l'enveloppe avec l'erreur
env_factor = regulation.compute_env_n(error_n, t, env_mode,
sigma_n_t, mu_n_t, T, env_type)
# Amplitude finale avec enveloppe SANS G(error)
# An = A0 * σ(In) * env(error)
# G(error) sera appliqué dans S(t) en mode extended
An_t[n] = base_amplitude * env_factor
except Exception as e:
print(f"⚠ Erreur enveloppe dynamique strate {n} à t={t}: {e}")
An_t[n] = base_amplitude # Fallback sur mode statique
else:
# Mode statique classique
An_t[n] = base_amplitude
return An_t
1.3. Calcul du signal inter-strates S_i(t)
-
Implémentation concrète
Python :
def compute
S
_
_
i(t: float, n: int, history: List[Dict], state: List[Dict]) -> float:
"""
Calcule le signal provenant des autres strates.
15
S
_
i(t) = Σ(j≠n) Oj(t) * w
_ji
où w
_ji sont les poids de connexion de la strate j vers la strate i.
Args:
t: temps actuel
n: indice de la strate courante
history: historique complet du système
state: état actuel des strates (pour accéder aux poids)
Returns:
float: signal pondéré des autres strates
"""
if t == 0 or len(history) == 0:
return 0.0
# Récupérer le dernier état avec les sorties observées
last
_
state = history[-1]
On
_prev = last
_
state.get('O'
, None)
if On
_prev is None or not isinstance(On
_prev, np.ndarray):
return 0.0
# Récupérer les poids de la strate n
if n < len(state) and 'w' in state[n]:
w
_
n = state[n]['w']
else:
return 0.0
16
N = len(On
_prev)
S
i = 0.0
_
# Calculer la somme pondérée
for j in range(N):
if j != n and j < len(w
_
n): # Exclure la strate courante
# w
_
n[j] est le poids de j vers n
S
i += On
_
_prev[j] * w
_
n[j]
return S
i
_
1.4. Modulation de fréquence
1.4.a Modulation de fréquence Δf(t)
La modulation f(t) peut être corrélée à l’état global de l’organisme/système via Δf(t)
-
Expression mathématique de départ
On la note :
Δf(t) = α · Σ_i w_{ni} · S_i(t)
Où :
●
●
●
S_i(t) : signaux d’autres strates ou stimuli
w_{ni} : poids de connexion
α : souplesse d’adaptation de la strate
-
Implémentation concrète
17
Python :
def compute_delta_fn(t: float, alpha_n: float, S_i: float) -> float:
"""
Calcule la modulation de fréquence.
Δf(t) = α · S_i(t)
où S_i(t) = Σ(j≠n) w_nj · Oj(t) est déjà calculé
Args:
t: temps actuel
alpha_n: souplesse d'adaptation de la strate
S_i: signal agrégé des autres strates
Returns:
float: modulation de fréquence
"""
return alpha_n * S_i
1.4.b. Fréquence modulée de chaque strate f(t)
Chaque strate a une fréquence naturelle, comme un mode propre en physique.
-
Expression mathématique de départ
On la note :
f(t) = f₀ + Δf(t)
18
Où :
●
●
f₀ : fréquence de base de la strate
Δf(t) : modulation adaptative, liée à l’interaction avec les autres strates ou le contexte
-
Implémentation concrète
Python :
def compute_fn(t: float, state: List[Dict], An_t: np.ndarray, config: Dict) -> np.ndarray:
"""
Calcule la fréquence modulée pour chaque strate.
f(t) = f₀ + Δf(t) · β(t) [si mode dynamique]
f(t) = f₀ + Δf(t) [si mode statique]
Avec contrainte spiralée : f₊₁(t) ≈ r(t) · f(t)
Args:
t: temps actuel
state: état des strates
An_t: amplitudes actuelles
config: configuration
Returns:
np.ndarray: fréquences modulées
"""
N = len(state)
fn_t = np.zeros(N)
history = config.get('history', [])
19
# Vérifier le mode plasticité dynamique
dynamic_params = config.get('dynamic_parameters', {})
dynamic_beta = dynamic_params.get('dynamic_beta', False)
T = config.get('system', {}).get('T', 100)
# Calculer le ratio spiralé r(t)
if dynamic_params.get('dynamic_phi', False):
spiral_config = config.get('spiral', {})
phi = spiral_config.get('phi', 1.618)
epsilon = spiral_config.get('epsilon', 0.05)
omega = spiral_config.get('omega', 0.1)
theta = spiral_config.get('theta', 0.0)
r_t = compute_r(t, phi, epsilon, omega, theta)
else:
r_t = None
# Calculer d'abord toutes les modulations de base
delta_fn_array = np.zeros(N)
for n in range(N):
f0n = state[n]['f0']
alpha_n = state[n]['alpha']
beta_n = state[n]['beta']
# Calcul du signal des autres strates
S_i = compute_S_i(t, n, history, state)
# Modulation de fréquence de base
20
delta_fn = compute_delta_fn(t, alpha_n, S_i)
delta_fn_array[n] = delta_fn
if dynamic_beta:
# Plasticité β(t) adaptative
try:
# Facteur de plasticité basé sur l'amplitude et le temps
A_factor = An_t[n] / state[n]['A0'] if state[n]['A0'] > 0 else 1.0
t_factor = 1.0 + 0.5 * np.sin(2 * np.pi * t / T) # Oscillation temporelle
# Moduler β selon le contexte
# DÉSACTIVÉ : effort_factor causait des chutes à 0 non désirées
# effort_factor = 1.0
# if len(history) > 0:
# recent_effort = history[-1].get('effort(t)', 0.0)
# # Plus d'effort → moins de plasticité (stabilisation)
# effort_factor = 1.0 / (1.0 + 0.1 * recent_effort)
# beta_n_t = beta_n * A_factor * t_factor * effort_factor
beta_n_t = beta_n * A_factor * t_factor # Sans effort_factor
# Fréquence de base avec plasticité dynamique
fn_t[n] = f0n + delta_fn * beta_n_t
except Exception as e:
print(f"⚠ Erreur plasticité dynamique strate {n} à t={t}: {e}")
fn_t[n] = f0n + delta_fn * beta_n # Fallback sur mode statique
else:
21
# Mode statique classique
fn_t[n] = f0n + delta_fn * beta_n
# Appliquer la contrainte spiralée si r(t) est défini
if r_t is not None and N > 1:
# Ajustement progressif pour respecter f₊₁ ≈ r(t) · f
# On utilise une approche de relaxation pour éviter les changements brusques
relaxation_factor = 0.5 # Facteur d'ajustement doux
for n in range(N - 1):
# Ratio actuel entre fréquences adjacentes
if fn_t[n] > 0:
current_ratio = fn_t[n + 1] / fn_t[n]
# Ajustement vers le ratio cible
target_fn = r_t * fn_t[n]
fn_t[n + 1] = fn_t[n + 1] * (1 - relaxation_factor) + target_fn * relaxation_factor
return fn_t
1.5. Phases propres à chaque strate φ(t)
Chaque strate a une phase d’origine (empreinte singulière, propre à la strate)
-
Implémentation concrète
Python :
def compute_phi_n(t: float, state: List[Dict], config: Dict) -> np.ndarray:
"""
Calcule la phase pour chaque strate.
22
Args:
t: temps actuel
state: état des strates
config: configuration
Returns:
np.ndarray: phases
Modes:
- "static": φ constant (depuis config)
- "dynamic": évolution à définir après phase 1
"""
N = len(state)
phi_n_t = np.zeros(N)
# Récupération du mode depuis config
dynamic_params = config.get('dynamic_parameters', {})
dynamic_phi = dynamic_params.get('dynamic_phi', False)
if dynamic_phi:
# Mode dynamique avec SIGNATURES INDIVIDUELLES
phi_golden = config.get('spiral', {}).get('phi', 1.618)
epsilon = config.get('spiral', {}).get('epsilon', 0.05)
omega = config.get('spiral', {}).get('omega', 0.1)
theta = config.get('spiral', {}).get('theta', 0.0)
# Calculer le ratio spiralé r(t)
23
r_t = phi_golden + epsilon * np.sin(2 * np.pi * omega * t + theta)
# Mode signatures : chaque strate a sa "voix propre"
signature_mode = config.get('spiral', {}).get('signature_mode', 'individual')
for n in range(N):
# EMPREINTE UNIQUE de la strate (signature invariante)
phi_signature = state[n].get('phi', 0.0) # Son "ADN phasique"
if signature_mode == 'individual':
# NOUVEAU : Chaque strate danse autour de SA signature propre
# ω personnalisée basée sur sa position dans le pentagone
omega_n = omega * (1.0 + 0.2 * np.sin(n * 2 * np.pi / N)) # Fréquence propre
# Modulation spiralée AUTOUR de sa signature
personal_spiral = epsilon * np.sin(2 * np.pi * omega_n * t + phi_signature)
# Interaction douce avec le ratio global r(t)
global_influence = 0.3 * (r_t - phi_golden) * np.cos(phi_signature)
# Interaction inter-strates basée sur affinités phasiques
inter_strata_influence = 0.0
for j in range(N):
if j != n:
w_nj = state[n].get('w', [0.0]*N)[j] if len(state[n].get('w', [])) > j else 0.0
phi_j_signature = state[j].get('phi', 0.0)
# Affinité basée sur proximité des signatures
signature_affinity = np.cos(phi_signature - phi_j_signature)
inter_strata_influence += 0.05 * w_nj * signature_affinity * np.sin(2 * np.pi * omega * t)
# Phase finale : SIGNATURE + danse personnelle + influences
phi_n_t[n] = phi_signature + personal_spiral + global_influence + inter_strata_influence
24
else:
# Mode original (fallback)
spiral_phase_increment = r_t * epsilon * np.sin(2 * np.pi * omega * t + n * 2 * np.pi / N)
inter_strata_influence = 0.0
for j in range(N):
if j != n:
w_nj = state[n].get('w', [0.0]*N)[j] if len(state[n].get('w', [])) > j else 0.0
phase_diff = state[j].get('phi', 0.0) - phi_signature
inter_strata_influence += 0.1 * w_nj * np.sin(phase_diff)
phi_n_t[n] = phi_signature + spiral_phase_increment + inter_strata_influence
else:
# Mode statique
for n in range(N):
phi_n_t[n] = state[n].get('phi', 0.0)
return phi_n_t
1.6. Latence expressive
1.6.a Latence expressive globale γ(t)
Facteur de latence expressive (qui ralentit ou fluidifie le feedback)
-
Implémentation concrète
Python :
def compute_gamma(t: float, mode: str = "static", T: Optional[float] = None,
k: Optional[float] = None, t0: Optional[float] = None) -> float:
"""
25
Calcule la latence expressive globale.
Args:
t: temps actuel
mode: "static", "dynamic", "sigmoid_up", "sigmoid_down", "sigmoid_adaptive", "sigmoid_oscillating",
"sinusoidal"
T: durée totale (pour modes non statiques)
k: paramètre de pente (optionnel, défaut selon mode) ou fréquence pour sinusoidal
t0: temps de transition (optionnel, défaut = T/2) ou phase initiale pour sinusoidal
Returns:
float: latence entre 0 et 1
Formes:
- static: γ(t) = 1.0
- dynamic: γ(t) = 1/(1 + exp(-k(t - t0)))
- sigmoid_up: activation progressive
- sigmoid_down: désactivation progressive
- sigmoid_adaptive: varie entre 0.3 et 1.0
- sigmoid_oscillating: sigmoïde + oscillation sinusoïdale mise à l'échelle
- sinusoidal: oscillation sinusoïdale pure entre 0.1 et 0.9
"""
if mode == "static":
return 1.0
elif mode == "dynamic" and T is not None:
# Sigmoïde centrée à t0 (par défaut T/2)
k_val = k if k is not None else 2.0
t0_val = t0 if t0 is not None else T / 2
return 1.0 / (1.0 + np.exp(-k_val * (t - t0_val)))
26
elif mode == "sigmoid_up" and T is not None:
# Activation progressive
k_val = k if k is not None else 4.0 / T
t0_val = t0 if t0 is not None else T / 2
return 1.0 / (1.0 + np.exp(-k_val * (t - t0_val)))
elif mode == "sigmoid_down" and T is not None:
# Désactivation progressive
k_val = k if k is not None else 4.0 / T
t0_val = t0 if t0 is not None else T / 2
return 1.0 / (1.0 + np.exp(k_val * (t - t0_val)))
elif mode == "sigmoid_adaptive" and T is not None:
# Varie entre 0.3 et 1.0
k_val = k if k is not None else 4.0 / T
t0_val = t0 if t0 is not None else T / 2
return 0.3 + 0.7 / (1.0 + np.exp(-k_val * (t - t0_val)))
elif mode == "sigmoid_oscillating" and T is not None:
# Sigmoïde avec oscillation sinusoïdale
k_val = k if k is not None else 4.0 / T
t0_val = t0 if t0 is not None else T / 2
# Calcul de la sigmoïde de base (entre 0 et 1)
base_sigmoid = 1.0 / (1.0 + np.exp(-k_val * (t - t0_val)))
# Oscillation avec fréquence adaptée
oscillation_freq = 2.0 # Nombre d'oscillations sur la durée T
oscillation_phase = 2 * np.pi * oscillation_freq / T * t
# Mise à l'échelle pour préserver les oscillations complètes
27
# La sigmoïde varie de 0 à 1, on la transforme pour varier de 0.1 à 0.9
# puis on ajoute une oscillation de ±0.1 autour
sigmoid_scaled = 0.1 + 0.8 * base_sigmoid
oscillation_amplitude = 0.1
# Résultat final : sigmoïde mise à l'échelle + oscillation
# Cela garantit que γ reste dans [0.0, 1.0] sans saturation
gamma = sigmoid_scaled + oscillation_amplitude * np.sin(oscillation_phase)
# Assurer que gamma reste dans les bornes [0.1, 1.0] par sécurité
# mais sans écrêtage brutal
return max(0.1, min(1.0, gamma))
elif mode == "sinusoidal" and T is not None:
# Oscillation sinusoïdale pure sans transition sigmoïde
# k représente le nombre d'oscillations sur la durée T (défaut: 2)
# t0 représente la phase initiale en radians (défaut: 0)
freq = k if k is not None else 2.0 # Nombre d'oscillations sur T
phase_init = t0 if t0 is not None else 0.0 # Phase initiale
# Oscillation entre 0.1 et 0.9 pour rester dans une plage utile
# γ(t) = 0.5 + 0.4 * sin(2π * freq * t/T + phase_init)
oscillation = np.sin(2 * np.pi * freq * t / T + phase_init)
gamma = 0.5 + 0.4 * oscillation
# Assurer que gamma reste dans [0.1, 0.9]
return max(0.1, min(0.9, gamma))
else:
return 1.0
28
1.6.b Latence expressive par strate γ (t)
Facteur de latence expressive de la strate n
-
Implémentation concrète
Python :
def compute_gamma_n(t: float, state: List[Dict], config: Dict, gamma_global: Optional[float] = None,
En_array: Optional[np.ndarray] = None, On_array: Optional[np.ndarray] = None,
An_array: Optional[np.ndarray] = None, fn_array: Optional[np.ndarray] = None,
history: Optional[List[Dict]] = None) -> np.ndarray:
"""
Calcule la latence expressive par strate.
NOUVELLE VERSION : Modulation locale basée sur l'état dynamique de chaque strate.
gamma_n = gamma_global * f(erreur_n, amplitude_n, fréquence_n)
Args:
t: temps actuel
state: état des strates
config: configuration
gamma_global: gamma global pré-calculé (optionnel, pour modes adaptatifs)
En_array: attentes par strate (optionnel)
On_array: observations par strate (optionnel)
An_array: amplitudes par strate (optionnel)
fn_array: fréquences par strate (optionnel)
history: historique de simulation pour récupérer les valeurs précédentes (optionnel)
29
Returns:
np.ndarray: latences par strate modulées localement
"""
N = len(state)
gamma_n_t = np.zeros(N)
# Configuration de latence
latence_config = config.get('latence', {})
gamma_mode = latence_config.get('gamma_mode', 'static')
T = config.get('system', {}).get('T', 100)
# Si gamma_global n'est pas fourni, le calculer
if gamma_global is None:
gamma_dynamic = latence_config.get('gamma_dynamic', {})
k = gamma_dynamic.get('k', None)
t0 = gamma_dynamic.get('t0', None)
gamma_global = compute_gamma(t, gamma_mode, T, k, t0)
# Paramètres de modulation (peuvent être dans config)
modulation_config = latence_config.get('modulation', {})
k_error = modulation_config.get('k_error', 0.1) # Poids de l'erreur
k_amplitude = modulation_config.get('k_amplitude', 0.1) # Poids de l'amplitude
k_frequency = modulation_config.get('k_frequency', 0.05) # Poids de la fréquence
gamma_min = modulation_config.get('gamma_min', 0.5) # Borne inf : gamma_global * 0.5
gamma_max = modulation_config.get('gamma_max', 1.5) # Borne sup : gamma_global * 1.5
# Essayer de récupérer les données depuis l'historique si non fournies
if history and len(history) > 0:
30
last_step = history[-1]
if En_array is None and 'E' in last_step:
En_array = last_step['E'] if isinstance(last_step['E'], np.ndarray) else None
if On_array is None and 'O' in last_step:
On_array = last_step['O'] if isinstance(last_step['O'], np.ndarray) else None
if An_array is None and 'An' in last_step:
An_array = last_step['An'] if isinstance(last_step['An'], np.ndarray) else None
if fn_array is None and 'fn' in last_step:
fn_array = last_step['fn'] if isinstance(last_step['fn'], np.ndarray) else None
# Mode legacy si pas de modulation ou données manquantes
if not modulation_config.get('enabled', True) or any(x is None for x in [En_array, On_array, An_array,
fn_array]):
# Comportement legacy avec décalage temporel optionnel
if latence_config.get('strata_delay', False) and gamma_global is None:
for n in range(N):
t_shifted = t - n * T / (2 * N)
gamma_n_t[n] = compute_gamma(t_shifted, gamma_mode, T,
latence_config.get('gamma_dynamic', {}).get('k', None),
latence_config.get('gamma_dynamic', {}).get('t0', None))
else:
gamma_n_t[:] = gamma_global
return gamma_n_t
# NOUVELLE MODULATION : gamma_n = gamma_global * facteur_modulation_n
if modulation_config.get('verbose', False) and t < 1.0: # Log seulement au début
print(f"[MODULATION] t={t:.2f}: Modulation locale activée (k_err={k_error}, k_amp={k_amplitude},
k_freq={k_frequency})")
31
for n in range(N):
# 1. Erreur normalisée (positive = observation > attente)
error_n = On_array[n] - En_array[n]
# Normaliser par l'amplitude moyenne pour éviter explosion
A_mean = np.mean(np.abs(An_array)) if np.mean(np.abs(An_array)) > 0 else 1.0
error_norm = np.tanh(error_n / A_mean) # Entre -1 et 1
# 2. Amplitude normalisée (activité de la strate)
amplitude_norm = An_array[n] / A_mean if A_mean > 0 else 1.0
amplitude_factor = 1.0 + k_amplitude * (amplitude_norm - 1.0)
# 3. Fréquence normalisée (rapidité du rythme local)
f_mean = np.mean(fn_array) if np.mean(fn_array) > 0 else 1.0
freq_norm = fn_array[n] / f_mean
freq_factor = 1.0 + k_frequency * (freq_norm - 1.0)
# 4. Facteur d'erreur : erreur positive → gamma plus court (réaction plus rapide)
# erreur négative → gamma plus long (attente prudente)
error_factor = 1.0 - k_error * error_norm
# 5. Combiner les facteurs multiplicativement
modulation_factor = error_factor * amplitude_factor * freq_factor
# 6. Appliquer à gamma_global avec protection des bornes
gamma_n_t[n] = gamma_global * modulation_factor
# 7. Bornes adaptatives : rester dans [gamma_min*gamma_global, gamma_max*gamma_global]
gamma_n_t[n] = np.clip(gamma_n_t[n], gamma_min * gamma_global, gamma_max * gamma_global)
32
# 8. Bornes absolues de sécurité
gamma_n_t[n] = np.clip(gamma_n_t[n], 0.1, 1.0)
# Log de vérification de la modulation (seulement si verbose et au début)
if modulation_config.get('verbose', False) and t < 1.0:
gamma_range = np.ptp(gamma_n_t) # peak-to-peak (max - min)
if gamma_range > 0.01:
print(f"[MODULATION] γ_n varie de {gamma_n_t.min():.3f} à {gamma_n_t.max():.3f}
(écart={gamma_range:.3f})")
return gamma_n_t
1.7. Fonctions de régulation
1.7.a Fonction de régulation G(x), G (x)
G(x) est une fonction de transformation non-linéaire et adaptive qui transforme la différence entre l’état
visé (E) et l’état atteint (O)
-
Expressions mathématiques de départ
Plusieurs archétypes :
a) La tanh spiralée :
G(x) = tanh(k·x) (k règle la raideur)
●
●
●
Douce pour les faibles écarts
Saturante pour les grandes dissonances (évite la rupture)
k contrôle la sensibilité (peut être modulé par le contexte)
b) La spirale logarithmique adaptative :
33
G(x) = sign(x) · log(1 + α·|x|) · sin(β·x)
●
●
●
Utilise l’oscillation pour introduire une modulation spiralée.
Le système ne revient pas mécaniquement vers E, mais tourne autour, s’accorde par spirale.
α et β contrôlent la largeur et la fréquence de cette spirale.
c) Une forme purement adaptative :
G(x, t) = η(t) · sin(θ(t) · x)
●
●
η(t) est une enveloppe harmonique contextuelle
θ(t) est une fréquence d’ajustement (qui diminue si le système devient stable)
Cela permet un ralentissement naturel quand l’équilibre est proche.
d) L’onde sinc :
G(x) = sin(x)/x
G(x) = sinc(x) (ou sa version normalisée)
e) La sinusoïde avec enveloppe gaussienne :
G(x) = sin(β·(x − x₀)) · exp(−α·(x − x₀)²)
●
●
●
●
sin(β·(x − x₀)) est la pulsation centrale : la fréquence propre de la strate.
exp(−α·(x − x₀)²) est une enveloppe gaussienne, qui concentre cette onde autour de x₀.
Contrairement à la sinc, elle ne vibre pas à l’infini. Elle s’éteint doucement.
C’est une onde très locale, non invasive, qui vient exprimer une variation temporaire d’un état, puis
s’efface sans perturber le tout.
f) La sinc modifiée en enveloppe adaptative
G(x, t) = A(t) · sinc[ f(t) · (x − μ(t)) ] · env(x, t)
Avec :
●
●
●
●
A(t) = amplitude adaptative
f(t) = fréquence ou densité locale
μ(t) = décalage ou recentrage (la pulsation ne part pas toujours de zéro)
env(x, t) = enveloppe adaptative (ex. : une gaussienne ou une sigmoïde douce)
-
Implémentation concrète
Python :
34
def compute_G(x: Union[float, np.ndarray], archetype: str = "tanh",
params: Optional[Dict[str, float]] = None) -> Union[float, np.ndarray]:
"""
Calcule la fonction de régulation selon l'archétype choisi.
La régulation transforme l'erreur (E - O) en signal de correction.
Chaque archétype a ses propriétés : saturation, oscillation, résonance...
Args:
x: valeur(s) d'entrée (typiquement E - O)
archetype: type de fonction parmi ["tanh", "sinc", "resonance", "spiral_log", "adaptive"]
params: paramètres spécifiques à chaque archétype
Returns:
Valeur(s) de régulation G(x)
Archétypes:
- "tanh": tanh(λx) - Saturation douce, transition continue
- "sinc": sin(x)/x - Oscillations amorties, passage par zéro
- "resonance": sin(βx)·exp(-αx²) - Résonance localisée
- "spiral_log": sign(x)·log(1+α|x|)·sin(βx) - Spirale logarithmique
- "adaptive": Forme à définir selon contexte
"""
if params is None:
params = {}
35
if archetype == "tanh":
# Tangente hyperbolique : saturation douce aux extrêmes
lambda_val = params.get("lambda", 1.0)
return np.tanh(lambda_val * x)
elif archetype == "sinc":
# Sinus cardinal : oscillations qui s'amortissent
# Protection contre division par zéro
with np.errstate(divide='ignore', invalid='ignore'):
result = np.where(x != 0, np.sin(x) / x, 1.0)
return result
elif archetype == "resonance":
# Résonance gaussienne modulée : pic local avec décroissance
alpha = params.get("alpha", 1.0) # Largeur de la gaussienne
beta = params.get("beta", 2.0) # Fréquence d'oscillation
return np.sin(beta * x) * np.exp(-alpha * x**2)
elif archetype == "spiral_log":
# Spirale logarithmique
# G(x) = sign(x) · log(1 + α·|x|) · sin(β·x)
alpha = params.get("alpha", 1.0)
beta = params.get("beta", 2.0)
# Calcul par composants pour gérer les arrays
sign_x = np.sign(x)
abs_x = np.abs(x)
36
log_component = np.log(1 + alpha * abs_x)
sin_component = np.sin(beta * x)
return sign_x * log_component * sin_component
elif archetype == "adaptive":
# Combinaison adaptative de tanh et spiral_log
lambda_val = params.get("lambda", 1.0)
alpha = params.get("alpha", 0.5)
# Calcul des deux composantes
tanh_part = np.tanh(lambda_val * x)
spiral_part = compute_G(x, "spiral_log", params)
# Mélange pondéré
return alpha * tanh_part + (1 - alpha) * spiral_part
elif archetype == "adaptive_aware":
# Mode adaptatif conscient - utilise dynamics.compute_G_adaptive_aware
# Pour éviter une dépendance circulaire, on retourne simplement tanh ici
# La vraie logique adaptive_aware est dans dynamics.py
lambda_val = params.get("lambda", 1.0)
return np.tanh(lambda_val * x)
else:
# Archétype non reconnu - fallback sur tanh
warnings.warn(f"Archétype '{archetype}' non reconnu. Utilisation de 'tanh' par défaut.")
return compute_G(x, "tanh", params)
37
1.7.b. Version temporelle G(x,t)
-
Implémentation concrète
Python :
def compute_G_temporal(x: Union[float, np.ndarray], t: float,
eta_t: float, theta_t: float) -> Union[float, np.ndarray]:
"""
Calcule la version temporelle de la régulation.
G(x,t) = η(t)·sin(θ(t)·x)
Cette forme permet une modulation temporelle de la régulation,
avec amplitude η(t) et fréquence θ(t) variables.
Args:
x: valeur(s) d'entrée
t: temps actuel
eta_t: amplitude contextuelle
theta_t: fréquence adaptative
Returns:
Régulation temporelle G(x,t)
Note:
η(t) et θ(t) sont des paramètres exploratoires
"""
38
return eta_t * np.sin(theta_t * x)
1.7.c. Fonction intégrée pour simulate.py G (error, A (t), t)
-
Implémentation concrète
Python :
def compute_Gn(error: Union[float, np.ndarray], t: float, An_t: Union[float, np.ndarray],
fn_t: Union[float, np.ndarray], config: Dict) -> Union[float, np.ndarray]:
"""
Interface principale pour simulate.py - calcule la régulation complète.
Cette fonction orchestre tous les calculs de régulation en utilisant
la configuration pour déterminer les modes et paramètres.
Args:
error: erreur O(t) - E(t) (peut être scalaire ou array)
t: temps actuel
An_t: amplitude(s) actuelle(s)
fn_t: fréquence(s) actuelle(s)
config: configuration complète
Returns:
Feedback de régulation
"""
# Extraction des paramètres de configuration
regulation_config = config.get('regulation', {})
enveloppe_config = config.get('enveloppe', {})
T = config.get('system', {}).get('T', 100)
# Archétype de régulation
G_arch = regulation_config.get('G_arch', 'tanh')
G_params = {
'lambda': regulation_config.get('lambda', 1.0),
'alpha': regulation_config.get('alpha', 1.0),
'beta': regulation_config.get('beta', 2.0)
}
# Mode enveloppe
env_mode = enveloppe_config.get('env_mode', 'static')
env_type = config.get('to_calibrate', {}).get('env_n', 'gaussienne')
# Si error est un scalaire, on traite une seule strate
if np.isscalar(error):
# Calcul des paramètres d'enveloppe
sigma_n = compute_sigma_n(
t, env_mode, T,
enveloppe_config.get('sigma_n_static', 0.1),
enveloppe_config.get('sigma_n_dynamic')
39
)
mu_n = compute_mu_n(
t, env_mode,
enveloppe_config.get('mu_n', 0.0),
enveloppe_config.get('mu_n_dynamic')
)
# Calcul de l'enveloppe
env_n = compute_env_n(error, t, env_mode, sigma_n, mu_n, T, env_type)
# Régulation de base
if regulation_config.get('dynamic_G', False):
# Mode dynamique avec G complet
return compute_Gn(error, t, An_t, fn_t, mu_n, env_n)
else:
# Mode statique avec archétype simple
return compute_G(error, G_arch, G_params)
# Si error est un array, on traite toutes les strates
else:
N = len(error)
result = np.zeros_like(error)
# Vérifier que An_t et fn_t sont aussi des arrays
if np.isscalar(An_t):
An_t = np.full(N, An_t)
if np.isscalar(fn_t):
fn_t = np.full(N, fn_t)
for n in range(N):
# Calcul par strate
sigma_n = compute_sigma_n(
t, env_mode, T,
enveloppe_config.get('sigma_n_static', 0.1),
enveloppe_config.get('sigma_n_dynamic')
)
mu_n = compute_mu_n(
t, env_mode,
enveloppe_config.get('mu_n', 0.0),
enveloppe_config.get('mu_n_dynamic')
)
env_n = compute_env_n(error[n], t, env_mode, sigma_n, mu_n, T, env_type)
if regulation_config.get('dynamic_G', False):
result[n] = compute_Gn(error[n], t, An_t[n], fn_t[n], mu_n, env_n)
else:
result[n] = compute_G(error[n], G_arch, G_params)
return result
1.7.d. Enveloppes adaptatives σ (t), μ (t), env (x, t)
40
-
Implémentation concrète
Python :
def compute_sigma_n(t: float, mode: str = "static", T: Optional[float] = None,
sigma_n_static: float = 0.1, sigma_n_dynamic: Optional[Dict] = None) -> float:
"""
Calcule l'écart-type de l'enveloppe.
σ(t) contrôle la largeur de l'enveloppe gaussienne ou sigmoïde.
Args:
t: temps actuel
mode: "static" ou "dynamic"
T: période totale (pour mode dynamic)
sigma_n_static: valeur statique par défaut
sigma_n_dynamic: paramètres dynamiques {amp, freq, offset}
Returns:
float: écart-type σ(t)
"""
if mode == "static":
return sigma_n_static
elif mode == "dynamic" and T is not None and sigma_n_dynamic is not None:
# Modulation sinusoïdale de l'écart-type
amp = sigma_n_dynamic.get("amp", 0.05)
41
freq = sigma_n_dynamic.get("freq", 1.0)
offset = sigma_n_dynamic.get("offset", 0.1)
# σ(t) = offset + amp·sin(2π·freq·t/T)
# GARANTIR que σn > 0 avec un minimum absolu
sigma_min = 0.01 # Écart-type minimum pour éviter division par 0
raw_sigma = offset + amp * np.sin(2 * np.pi * freq * t / T)
return max(raw_sigma, sigma_min)
else:
# Fallback sur statique
return sigma_n_static
def compute_mu_n(t: float, mode: str = "static", mu_n_static: float = 0.0,
mu_n_dynamic: Optional[Dict] = None) -> float:
"""
Calcule le centre de l'enveloppe.
μ(t) déplace le centre de régulation, permettant un focus adaptatif.
Args:
t: temps actuel
mode: "static" ou "dynamic"
mu_n_static: valeur statique
mu_n_dynamic: paramètres dynamiques (à définir)
Returns:
42
float: centre μ(t)
"""
if mode == "static":
return mu_n_static
elif mode == "dynamic" and mu_n_dynamic is not None:
# Mode dynamique - À définir
# DÉSACTIVÉ : drift_rate causait des dérives non désirées
# drift_rate = mu_n_dynamic.get("drift_rate", 0.01)
# max_drift = mu_n_dynamic.get("max_drift", 1.0)
#
# # Dérive bornée
# drift = drift_rate * t
# return np.clip(drift, -max_drift, max_drift)
# Pour l'instant, retourner la valeur statique même en mode dynamique
return mu_n_static
else:
return mu_n_static
def compute_env_n(x: Union[float, np.ndarray], t: float, mode: str = "static",
sigma_n: float = 0.1, mu_n: float = 0.0, T: Optional[float] = None,
env_type: str = "gaussienne") -> Union[float, np.ndarray]:
"""
Calcule l'enveloppe adaptative.
43
L'enveloppe localise la régulation autour de μ avec une largeur σ.
Args:
x: valeur(s) d'entrée
t: temps actuel
mode: "static" ou "dynamic"
sigma_n: écart-type
mu_n: centre
T: période totale (pour mode dynamic)
env_type: "gaussienne" ou "sigmoide"
Returns:
Valeur(s) d'enveloppe entre 0 et 1
"""
if env_type == "gaussienne":
# Enveloppe gaussienne : exp(-(x-μ)²/(2σ²))
if sigma_n > 0:
return np.exp(-0.5 * ((x - mu_n) / sigma_n) ** 2)
else:
# Protection contre σ = 0
return np.where(x == mu_n, 1.0, 0.0)
elif env_type == "sigmoide":
# Enveloppe sigmoïde : transition douce
# Utilise σ comme paramètre de pente
k = 1.0 / (sigma_n + 1e-10) # Protection division par zéro
return 1.0 / (1.0 + np.exp(-k * (x - mu_n)))
44
else:
# Type non reconnu - fallback gaussienne
return compute_env_n(x, t, mode, sigma_n, mu_n, T, "gaussienne")
1.8. Fonctions adaptatives de γ(t) et G
1.8.a create
_quantum
_gamma
-
Implémentation concrète
Python :
def create_quantum_gamma(t: float, synergies: Dict) -> float:
"""
Crée une superposition des meilleures synergies.
"""
if not synergies:
return 0.5 + 0.4 * np.sin(0.05 * t)
# Top 3 synergies
top_synergies = sorted(synergies.items(),
key=lambda x: x[1]['score'],
reverse=True)[:3]
gamma = 0
total_weight = 0
for i, ((g_val, _), info) in enumerate(top_synergies):
45
# Poids quantique avec interférences
weight = info['score'] ** 2
phase = i * 2 * np.pi / 3 + 0.1 * t
# Oscillation avec battements
beat_freq = 0.01 * (i + 1)
amplitude = 1 + 0.1 * np.sin(beat_freq * t) * np.cos(phase)
gamma += weight * g_val * amplitude
total_weight += weight
return gamma / total_weight if total_weight > 0 else 0.5
1.8.b. gamma
_
adaptive
aware
_
-
Implémentation concrète
Python :
def compute_gamma_adaptive_aware(t: float, state: List[Dict], history: List[Dict],
config: Dict, discovery_journal: Dict = None) -> Tuple[float, str, Dict]:
"""
Latence adaptative complète ET consciente de G(x).
Combine :
- Surveillance multi-critères (6 métriques)
- Détection du spacing effect
- Conscience de l'archétype G actuel
- Communication bidirectionnelle avec G(x)
46
- Journal enrichi des découvertes couplées
"""
# Initialiser le journal super-enrichi
if discovery_journal is None:
journal = {
# Structure complète du journal
'discovered_regimes': {},
'transitions': [],
'current_regime': 'exploration',
'regime_start_time': 0,
'total_discoveries': 0,
'breakthrough_moments': [],
'score_history': [],
'gamma_peaks': [],
'system_performance': [], # AJOUT de system_performance
'rest_phases': [],
'optimal_gamma_patterns': {},
'spacing_analysis': {
'intervals': [],
'emerging': False,
'maturity_score': 0
},
# NOUVEAU : Conscience de G
'coupled_states': {}, # (γ, G_arch) → performances
'G_transition_impacts': [], # Impacts des changements
'gamma_G_synergies': {}, # Synergies découvertes
'communication_signals': [], # Signaux subtils γ↔G
47
'exploration_log': [] # Log d'exploration
}
else:
journal = discovery_journal.copy()
# Phase initiale
if len(history) < 50:
# Exploration systématique de l'espace gamma
exploration_step = int(t / config['system']['dt'])
# Balayer toutes les valeurs de gamma progressivement
gamma_space = np.linspace(0.1, 1.0, 10)
gamma_index = exploration_step % len(gamma_space)
base_gamma = gamma_space[gamma_index]
# Petite variation aléatoire pour explorer autour
gamma = base_gamma + 0.05 * np.random.randn()
gamma = np.clip(gamma, 0.1, 1.0)
# Enregistrer ce qu'on explore
journal['exploration_log'].append({'t': t, 'gamma': gamma, 'phase': 'systematic'})
return gamma, 'exploration', journal
# 1. OBSERVER L'ÉTAT ACTUEL DE G
current_G_arch = history[-1].get('G_arch_used', 'tanh') if history else 'tanh'
gamma_current = history[-1].get('gamma', 1.0) if history else 1.0
48
# 2. CALCULER LA PERFORMANCE SYSTÈME
recent_history = history[-50:]
scores = metrics.calculate_all_scores(recent_history)
current_scores = scores['current']
system_performance_score = np.mean(list(current_scores.values()))
# 3. ENREGISTRER L'ÉTAT COUPLÉ (γ, G)
state_key = (round(gamma_current, 1), current_G_arch)
if state_key not in journal['coupled_states']:
journal['coupled_states'][state_key] = {
'performances': [],
'first_seen': t,
'synergy_score': 0
}
journal['coupled_states'][state_key]['performances'].append(system_performance_score)
# Calculer le score de synergie
if len(journal['coupled_states'][state_key]['performances']) >= 5:
perfs = journal['coupled_states'][state_key]['performances'][-10:]
mean_perf = np.mean(perfs)
stability = 1 / (1 + np.std(perfs))
growth = np.polyfit(range(len(perfs)), perfs, 1)[0] if len(perfs) > 1 else 0
synergy_score = mean_perf * stability * (1 + growth)
journal['coupled_states'][state_key]['synergy_score'] = synergy_score
49
# Découverte de synergie exceptionnelle ?
if synergy_score > 4.5: # Seuil élevé
if state_key not in journal['gamma_G_synergies']:
journal['gamma_G_synergies'][state_key] = {
'discovered_at': t,
'score': synergy_score,
'note': f'Synergie parfaite découverte : γ={state_key[0]} + G={state_key[1]}'
}
journal['breakthrough_moments'].append({
't': t,
'type': 'perfect_synergy',
'state': state_key,
'score': synergy_score
})
# 4. DÉTECTER LES TRANSITIONS DE G ET LEUR IMPACT
if len(journal['score_history']) >= 2:
prev_G = history[-2].get('G_arch_used', 'tanh') if len(history) >= 2 else 'tanh'
if prev_G != current_G_arch:
# G a changé !
impact = {
't': t,
'gamma': gamma_current,
'G_before': prev_G,
'G_after': current_G_arch,
'performance_before': journal['score_history'][-2]['system_score'] if len(journal['score_history']) >= 2
else 0,
'performance_after': system_performance_score
}
50
impact['delta'] = impact['performance_after'] - impact['performance_before']
journal['G_transition_impacts'].append(impact)
# Ajuster la confiance dans les régimes gamma
# Si le changement de G a dégradé la performance, réduire la confiance
if impact['delta'] < -0.1:
for regime in journal['discovered_regimes'].values():
regime['confidence'] = regime.get('confidence', 1.0) * 0.8
# 5. CALCULER LA MOYENNE GLISSANTE DU SYSTÈME
window_size = 50
if len(journal['score_history']) >= window_size:
recent_system_scores = [
entry['system_score']
for entry in journal['score_history'][-window_size:]
]
rolling_avg = np.mean(recent_system_scores)
# Tendance (augmentation ?)
if len(journal['score_history']) >= window_size * 2:
old_scores = [
entry['system_score']
for entry in journal['score_history'][-window_size*2:-window_size]
]
else:
trend = 'unknown'
old_avg = np.mean(old_scores)
trend = 'increasing' if rolling_avg > old_avg + 0.05 else 'stable'
51
journal['system_performance'].append({
't': t,
'rolling_avg': rolling_avg,
'instant_score': system_performance_score,
'trend': trend
})
# Toujours ajouter à score_history
journal['score_history'].append({
't': t,
'scores': current_scores.copy(),
'gamma': gamma_current,
'G_arch': current_G_arch, # IMPORTANT pour le tracking
'system_score': system_performance_score
})
# 6. DÉTECTER LES PICS DE GAMMA ET LEUR PERFORMANCE
current_gamma = history[-1].get('gamma', 1.0) if history else 1.0
# Détection de pic (gamma élevé après une phase basse)
if len(history) >= 10:
recent_gammas = [h.get('gamma', 1.0) for h in history[-10:]]
avg_recent_gamma = np.mean(recent_gammas)
# Pic si gamma actuel > moyenne + écart-type
if current_gamma > avg_recent_gamma + np.std(recent_gammas):
# C'est un pic !
52
last_peak = journal['gamma_peaks'][-1] if journal['gamma_peaks'] else None
interval = t - last_peak['t'] if last_peak else None
peak_info = {
't': t,
'gamma': current_gamma,
'performance': system_performance_score,
'interval_since_last': interval,
'scores': current_scores.copy(),
'G_arch': current_G_arch # IMPORTANT pour spacing_by_G
}
journal['gamma_peaks'].append(peak_info)
# Enregistrer le pattern gamma → performance
gamma_bucket = round(current_gamma, 1)
if gamma_bucket not in journal['optimal_gamma_patterns']:
journal['optimal_gamma_patterns'][gamma_bucket] = []
journal['optimal_gamma_patterns'][gamma_bucket].append(system_performance_score)
# 7. DÉTECTER LES PHASES DE REPOS
if current_gamma < 0.5 and len(journal['rest_phases']) > 0:
# Potentiellement dans une phase de repos
last_rest = journal['rest_phases'][-1]
if 'end' not in last_rest: # Phase en cours
last_rest['end'] = t
last_rest['duration'] = t - last_rest['start']
last_rest['avg_gamma'] = np.mean([
53
h.get('gamma', 1.0)
for h in history[-(int(last_rest['duration']/config['system']['dt'])):]
])
elif current_gamma < 0.5 and (not journal['rest_phases'] or 'end' in journal['rest_phases'][-1]):
# Nouvelle phase de repos
journal['rest_phases'].append({
'start': t,
'avg_performance': system_performance_score
})
# 8. ANALYSER LE SPACING EFFECT
if len(journal['gamma_peaks']) >= 3:
# Calculer les intervalles entre pics
intervals = []
for peak in journal['gamma_peaks'][-5:]:
if peak.get('interval_since_last') is not None:
intervals.append(peak['interval_since_last'])
if len(intervals) >= 2:
# Vérifier si les intervalles augmentent
increasing_intervals = all(
intervals[i] <= intervals[i+1]
for i in range(len(intervals)-1)
)
# Vérifier si la performance moyenne augmente aussi
peak_performances = [
peak['performance']
54
for peak in journal['gamma_peaks'][-len(intervals)-1:]
]
increasing_performance = np.polyfit(range(len(peak_performances)),
peak_performances, 1)[0] > 0
# Détecter l'émergence du spacing effect
if increasing_intervals and increasing_performance:
journal['spacing_analysis']['emerging'] = True
journal['spacing_analysis']['intervals'] = intervals
# Calculer la maturité (0-1)
interval_growth = (intervals[-1] - intervals[0]) / intervals[0] if intervals[0] > 0 else 0
perf_growth = (peak_performances[-1] - peak_performances[0]) / peak_performances[0] if
peak_performances[0] > 0 else 0
journal['spacing_analysis']['maturity_score'] = min(1.0, (interval_growth + perf_growth) / 2)
# Enregistrer la découverte
if journal['spacing_analysis']['maturity_score'] > 0.5:
journal['breakthrough_moments'].append({
't': t,
'type': 'spacing_effect',
'note': f'Spacing effect émergent ! Intervalles: {intervals}, Maturité:
{journal["spacing_analysis"]["maturity_score"]:.2f}'
})
# 9. TROUVER LE GAMMA OPTIMAL SELON L'HISTORIQUE
best_gamma_for_performance = None
if journal['optimal_gamma_patterns']:
# Moyenner les performances par gamma
55
gamma_avg_perfs = {
gamma: np.mean(perfs)
for gamma, perfs in journal['optimal_gamma_patterns'].items()
if len(perfs) >= 3 # Au moins 3 échantillons
}
if gamma_avg_perfs:
best_gamma_for_performance = max(gamma_avg_perfs.items(),
key=lambda x: x[1])[0]
# 10. DÉCISION DE GAMMA TENANT COMPTE DE G
# Trouver le meilleur couple (γ, G)
best_synergy = None
best_synergy_score = 0
for state_key, state_info in journal['coupled_states'].items():
if state_info['synergy_score'] > best_synergy_score:
best_synergy_score = state_info['synergy_score']
best_synergy = state_key
# Vérifier si on est au plateau parfait
all_scores_5 = all(score >= 5 for score in current_scores.values())
if all_scores_5 and best_synergy_score > 4.5:
# MODE TRANSCENDANCE SYNERGIQUE !
if journal['current_regime'] != 'transcendent_synergy':
56
journal['transitions'].append({
't': t,
'regime': 'transcendent_synergy',
'note': f'Transcendance synergique ! γ={best_synergy[0]}, G={best_synergy[1]}'
})
journal['current_regime'] = 'transcendent_synergy'
# Si on est dans la synergie parfaite, micro-variations
if best_synergy and state_key == best_synergy:
# Parfait ! Juste des micro-ondulations
gamma = best_synergy[0] + 0.02 * np.sin(0.5 * t)
else:
# Converger vers la synergie optimale
target_gamma = best_synergy[0] if best_synergy else 0.8
gamma = gamma_current * 0.9 + target_gamma * 0.1
# Signal subtil pour suggérer le bon G
if current_G_arch != best_synergy[1]:
# Oscillation caractéristique selon le G désiré
if best_synergy[1] == 'resonance':
gamma += 0.05 * np.sin(2 * np.pi * t) # Signal résonant
elif best_synergy[1] == 'spiral_log':
gamma += 0.03 * np.log(1 + np.abs(np.sin(0.1 * t))) # Signal spiral
# Enregistrer le signal
journal['communication_signals'].append({
't': t,
'type': 'gamma_suggests_G',
57
'desired_G': best_synergy[1],
'signal_pattern': 'oscillation'
})
elif journal['spacing_analysis']['emerging']:
# MODE SPACING CONSCIENT DE G
# Le spacing effect peut être différent selon G !
spacing_by_G = defaultdict(list)
for peak in journal['gamma_peaks']:
if 'G_arch' in peak and peak.get('interval_since_last') is not None:
spacing_by_G[peak['G_arch']].append(peak['interval_since_last'])
# Adapter le spacing selon le G actuel
if current_G_arch in spacing_by_G and len(spacing_by_G[current_G_arch]) >= 2:
optimal_interval = np.mean(spacing_by_G[current_G_arch]) * 1.1
else:
optimal_interval = 150 # Défaut
# Logique de spacing adaptée à G
if not journal['gamma_peaks']:
gamma = 0.6
else:
last_peak = journal['gamma_peaks'][-1]
time_since_peak = t - last_peak['t']
base_gamma = best_synergy[0] if best_synergy else 0.8
phase = time_since_peak / optimal_interval
58
if phase < 0.2:
gamma = base_gamma * (1 - phase * 5)
elif phase < 0.8:
gamma = 0.3 + 0.05 * np.sin(4 * np.pi * phase)
elif phase < 1.0:
rise = (phase - 0.8) / 0.2
gamma = 0.3 + (base_gamma - 0.3) * rise
else:
gamma = base_gamma
else:
# EXPLORATION CONSCIENTE
# Explorer les combinaisons (γ, G) non testées
all_gamma_values = set(round(g, 1) for g in np.linspace(0.1, 1.0, 10))
all_G_archs = {'tanh', 'resonance', 'spiral_log', 'adaptive'}
tested_combinations = set(journal['coupled_states'].keys())
untested = [(g, arch) for g in all_gamma_values for arch in all_G_archs
if (g, arch) not in tested_combinations]
if untested:
# Priorité aux γ proches avec G différents
candidates = [
(g, arch) for g, arch in untested
if abs(g - gamma_current) < 0.3 # Proche du γ actuel
]
59
if candidates:
target_gamma, target_G = candidates[0]
# Transition douce vers le nouveau γ
gamma = gamma_current * 0.8 + target_gamma * 0.2
# Signal pour suggérer le nouveau G
if target_G != current_G_arch:
journal['communication_signals'].append({
't': t,
'type': 'exploration_suggestion',
'target_state': (target_gamma, target_G)
})
else:
# Exploration créative
gamma = 0.5 + 0.4 * np.sin(0.05 * t) * np.cos(0.03 * t)
else:
# Tout testé : mode quantique !
gamma = create_quantum_gamma(t, journal['gamma_G_synergies'])
return np.clip(gamma, 0.1, 1.0), journal['current_regime'], journal
1.8.c. G
_
adaptive
_
aware
-
Implémentation concrète
Python :
def compute_G_adaptive_aware(error: float, t: float, gamma_current: float,
60
regulation_state: Dict, history: List[Dict], config: Dict,
allow_soft_preference: bool = True,
score_pair_now: bool = False):
"""
G(x) adaptatif pleinement conscient de γ et de leur danse commune.
Returns:
- G_value: valeur de régulation
- G_arch: archétype utilisé
- G_params: paramètres utilisés (pour logging)
"""
# Récupérer la mémoire de régulation
if 'regulation_state' not in regulation_state:
regulation_state['regulation_state'] = {}
if 'regulation_memory' not in regulation_state['regulation_state']:
regulation_state['regulation_state']['regulation_memory'] = {
'effectiveness_by_context': {}, # (G_arch, γ_range, error_range) → effectiveness
'preferred_G_by_gamma': {},
'G_transition_history': [],
'last_transition_time': 0,
'current_G_arch': 'tanh',
'adaptation_cycles': 0
}
reg_memory = regulation_state['regulation_state']['regulation_memory']
# 1. ANALYSER L'EFFICACITÉ CONTEXTUELLE
61
if len(history) >= 30:
recent = history[-30:]
for i, h in enumerate(recent[:-1]):
if i + 1 < len(recent):
# Contexte
g_arch = h.get('G_arch_used', 'tanh')
gamma = h.get('gamma', 1.0)
error_before = h.get('mean_abs_error', 1.0)
error_after = recent[i+1].get('mean_abs_error', 1.0)
# Efficacité
effectiveness = (error_before - error_after) / (error_before + 0.01)
# Clé contextuelle enrichie
gamma_bucket = round(gamma, 1)
error_bucket = 'low' if abs(error_before) < 0.1 else 'medium' if abs(error_before) < 0.5 else 'high'
context_key = (g_arch, gamma_bucket, error_bucket)
if context_key not in reg_memory['effectiveness_by_context']:
reg_memory['effectiveness_by_context'][context_key] = []
reg_memory['effectiveness_by_context'][context_key].append(effectiveness)
# 2. OBSERVER LES SIGNAUX DE GAMMA
gamma_signals = []
if len(history) >= 5:
recent_gammas = [h.get('gamma', 1.0) for h in history[-5:]]
62
# Détecter oscillations rapides (signal de mécontentement)
if np.std(recent_gammas) > 0.05:
freq_analysis = np.fft.fft(recent_gammas)
high_freq_power = np.sum(np.abs(freq_analysis[2:]))
if high_freq_power > 0.1:
gamma_signals.append('high_freq_oscillation')
# Détecter patterns spécifiques
if len(recent_gammas) >= 3:
diffs = np.diff(recent_gammas)
if np.all(diffs > 0):
gamma_signals.append('rising')
elif np.all(diffs < 0):
gamma_signals.append('falling')
# 3. DÉCIDER DE L'ARCHÉTYPE G
gamma_bucket = round(gamma_current, 1)
error_magnitude = abs(error)
# Initialiser params par défaut pour éviter l'erreur
params = {"lambda": 1.0, "alpha": 1.0, "beta": 2.0}
# Ajouter de la diversité : utiliser le temps pour varier les choix initiaux
exploration_factor = np.sin(0.1 * t) * 0.5 + 0.5 # Oscille entre 0 et 1
# Logique de base enrichie avec exploration
if gamma_bucket < 0.4:
63
# γ faible = repos → Régulation très douce
if error_magnitude < 0.1:
# Alterner entre tanh et adaptive selon le temps
G_arch = "tanh" if exploration_factor < 0.5 else "adaptive"
params = {"lambda": 0.3} if G_arch == "tanh" else {"lambda": 0.5, "alpha": 0.8}
else:
G_arch = "adaptive" # Mix doux
params = {"lambda": 0.5, "alpha": 0.8}
elif gamma_bucket > 0.7:
# γ élevé = actif → Régulation dynamique
if 'high_freq_oscillation' in gamma_signals:
# γ signale un problème → Changer de stratégie
if reg_memory['current_G_arch'] == 'resonance':
G_arch = "spiral_log" # Essayer autre chose
params = {"alpha": 1.0, "beta": 2.0} # Params par défaut pour spiral_log
else:
G_arch = "resonance"
params = {"alpha": 1.0, "beta": 2.0} # Params par défaut pour resonance
else:
# γ stable haute → Alterner entre resonance et spiral_log
G_arch = "resonance" if exploration_factor < 0.6 else "spiral_log"
if G_arch == "resonance":
params = {
"alpha": 1.0 - 0.5 * error_magnitude, # Adaptatif à l'erreur
"beta": 2.0 * gamma_current * (1 + 0.1 * np.sin(0.1 * t))
}
else:
64
params = {"alpha": 1.0, "beta": 2.0}
else:
# Zone intermédiaire → Créativité maximale avec rotation
choices = ["spiral_log", "adaptive", "resonance", "tanh"]
# Utiliser le cycle d'adaptation pour varier
choice_idx = (reg_memory['adaptation_cycles'] + int(exploration_factor * 4)) % 4
G_arch = choices[choice_idx]
if G_arch == "spiral_log":
params = {
"alpha": gamma_current + 0.1 * error_magnitude,
"beta": 3.0 - 2.0 * gamma_current
}
elif G_arch == "adaptive":
params = {
"lambda": gamma_current,
"alpha": 0.5 + 0.5 * (1 - error_magnitude)
}
elif G_arch == "resonance":
params = {
"alpha": 1.0 - 0.3 * error_magnitude,
"beta": 2.0 + gamma_current
}
else: # tanh
params = {"lambda": 0.5 + 0.5 * gamma_current}
# SOFT PREFERENCE: si une préférence douce est définie, orienter G_arch sans l'imposer
65
prefer_arch = reg_memory.get('prefer_next_arch')
prefer_time = reg_memory.get('prefer_hint_time', -1)
if prefer_arch and prefer_arch in ['tanh', 'resonance', 'spiral_log', 'adaptive']:
# Respecter un petit cooldown: si dernière transition très récente, ne pas switcher brutalement
cooldown = 5
if t - reg_memory.get('last_transition_time', -1e9) >= cooldown:
# Blending doux: 60% choix courant, 40% préférence
# Implémenté comme: si different, on bascule vers prefer_arch mais on laissera la transition douce gérer
la continuité
if prefer_arch != G_arch:
G_arch = prefer_arch
params = regulation.adapt_params_for_archetype(G_arch, gamma_current, error_magnitude)
# Consommer la préférence une fois lue (éviter de forcer à chaque pas)
reg_memory.pop('prefer_next_arch', None)
reg_memory.pop('prefer_hint_time', None)
# 4. VÉRIFIER L'EFFICACITÉ HISTORIQUE
error_bucket = 'low' if error_magnitude < 0.1 else 'medium' if error_magnitude < 0.5 else 'high'
context_key = (G_arch, gamma_bucket, error_bucket)
if context_key in reg_memory['effectiveness_by_context']:
effectiveness_history = reg_memory['effectiveness_by_context'][context_key]
if len(effectiveness_history) >= 3: # Réduit de 5 à 3 pour plus de réactivité
avg_effectiveness = np.mean(effectiveness_history[-5:]) # Fenêtre plus petite
# Si inefficace, essayer une alternative
if avg_effectiveness < 0.3: # Augmenté de 0.1 à 0.3 pour plus de changements
alternatives = ['tanh', 'resonance', 'spiral_log', 'adaptive']
alternatives.remove(G_arch)
66
# Chercher la meilleure alternative pour ce contexte
best_alt = None
best_score = avg_effectiveness
for alt in alternatives:
alt_key = (alt, gamma_bucket, error_bucket)
if alt_key in reg_memory['effectiveness_by_context']:
alt_effectiveness = np.mean(reg_memory['effectiveness_by_context'][alt_key][-5:])
if alt_effectiveness > best_score:
best_score = alt_effectiveness
best_alt = alt
if best_alt:
G_arch = best_alt
# Ajuster les paramètres selon l'archétype
params = regulation.adapt_params_for_archetype(G_arch, gamma_current, error_magnitude)
# 5. TRANSITION DOUCE SI CHANGEMENT
if G_arch != reg_memory['current_G_arch']:
# Enregistrer la transition
reg_memory['G_transition_history'].append({
't': t,
'from': reg_memory['current_G_arch'],
'to': G_arch,
'gamma': gamma_current,
'reason': gamma_signals[0] if gamma_signals else 'performance'
})
67
# Transition progressive (pas de changement brutal)
if t - reg_memory['last_transition_time'] < 10: # Réduit de 50 à 10 steps
# Trop tôt pour changer complètement
blend_factor = (t - reg_memory['last_transition_time']) / 10
# Calculer les deux G et mélanger
G_old = regulation.compute_G(error, reg_memory['current_G_arch'],
regulation.adapt_params_for_archetype(reg_memory['current_G_arch'], gamma_current,
error_magnitude))
G_new = regulation.compute_G(error, G_arch, params)
G_value = G_old * (1 - blend_factor) + G_new * blend_factor
# Garder l'ancien archétype pour l'instant
actual_G_arch = reg_memory['current_G_arch']
else:
# Transition complète
G_value = regulation.compute_G(error, G_arch, params)
reg_memory['current_G_arch'] = G_arch
reg_memory['last_transition_time'] = t
actual_G_arch = G_arch
else:
# Pas de changement
G_value = regulation.compute_G(error, G_arch, params)
actual_G_arch = G_arch
# 6. MISE À JOUR DE LA MÉMOIRE
reg_memory['adaptation_cycles'] += 1
68
# Enregistrer la préférence γ → G
if gamma_bucket not in reg_memory['preferred_G_by_gamma']:
reg_memory['preferred_G_by_gamma'][gamma_bucket] = {}
if actual_G_arch not in reg_memory['preferred_G_by_gamma'][gamma_bucket]:
reg_memory['preferred_G_by_gamma'][gamma_bucket][actual_G_arch] = 0
# Incrémenter le compteur d'utilisation
reg_memory['preferred_G_by_gamma'][gamma_bucket][actual_G_arch] += 1
# Après avoir décidé de G_value et G_arch_used
# --- Nouveau: scoring de la paire (gamma, G_arch_used) + oubli spiralé ---
try:
mem = regulation_state.get('regulation_memory', {}) if isinstance(regulation_state, dict) else {}
# 1) Oubli spiralé/spacing: décroissance continue de la confiance
phi = config.get('spiral', {}).get('phi', 1.618)
base_tau = float(config.get('exploration', {}).get('spacing_effect', {}).get('base_tau', 20.0))
spacing_level = int(mem.get('spacing_level', 0))
tau = base_tau * (phi ** spacing_level)
last_decay_update = float(mem.get('last_decay_update', t))
dt = max(0.0, float(t) - last_decay_update)
if dt > 0:
decay = float(np.exp(-dt / max(1e-6, tau)))
mem['best_pair_confidence'] = float(mem.get('best_pair_confidence', 0.0)) * decay
mem['last_decay_update'] = float(t)
# 2) Évaluer et renforcer uniquement lors des pics planifiés
if score_pair_now and history:
69
best = mem.get('best_pair', None)
# Mémoïsation par t pour éviter N recalculs par step
if mem.get('last_scores_t', None) == float(t) and 'last_adaptive_scores' in mem:
adaptive_scores = mem['last_adaptive_scores']
else:
adaptive_scores = metrics.calculate_all_scores(history, config).get('current', {})
mem['last_scores_t'] = float(t)
mem['last_adaptive_scores'] = adaptive_scores
criteria = ['stability', 'regulation', 'fluidity', 'resilience', 'innovation', 'cpu_cost', 'effort']
gaps = []
for c in criteria:
s = float(adaptive_scores.get(c, 3.0))
gaps.append(max(0.0, 5.0 - s))
mean_gap = float(np.mean(gaps)) if gaps else 2.0
score = 5.0 - mean_gap
current_pair = {'gamma': float(gamma_current), 'G_arch': G_arch, 'score': float(score), 't': float(t)}
improved = (not best) or (score > best.get('score', -1e-9))
close_enough = best and (score >= best.get('score', 0.0) - 0.02)
if improved:
mem['best_pair'] = current_pair
# Renforcement (spacing): confiance augmente doucement, et spacing_level progresse
if improved or close_enough or mem.get('best_pair_confidence', 0.0) < 0.2:
conf = float(mem.get('best_pair_confidence', 0.0))
mem['best_pair_confidence'] = min(1.0, 0.7 * conf + 0.3)
mem['last_reinforce_time'] = float(t)
mem['spacing_level'] = spacing_level + 1
# Journal léger
pairs = mem.get('pairs_log', [])
70
if len(pairs) < 1000:
pairs.append(current_pair)
else:
pairs.pop(0); pairs.append(current_pair)
mem['pairs_log'] = pairs
regulation_state['regulation_memory'] = mem
except Exception:
pass
return G_value, G_arch, params
1.8.d. adapt
_params
for
_
_
archetype
-
Implémentation concrète
Python :
def adapt_params_for_archetype(G_arch: str, gamma: float, error_magnitude: float) -> Dict[str, float]:
"""
Adapte les paramètres selon l'archétype, γ et l'erreur.
Utilisé par compute_G_adaptive_aware pour ajuster dynamiquement les paramètres.
Args:
G_arch: archétype de régulation
gamma: valeur actuelle de gamma
error_magnitude: magnitude de l'erreur
Returns:
dict avec paramètres adaptés pour l'archétype
"""
71
if G_arch == "tanh":
return {"lambda": 0.5 + gamma * 0.5}
elif G_arch == "resonance":
return {
"alpha": 1.0 - 0.3 * error_magnitude,
"beta": 1.5 * gamma + 0.5
}
elif G_arch == "spiral_log":
return {
"alpha": gamma + 0.2 * error_magnitude,
"beta": 3.0 - gamma
}
elif G_arch == "adaptive":
return {
"lambda": gamma,
"alpha": 0.5 + 0.5 * (1 - error_magnitude)
}
else:
return {"lambda": 1.0}
1.9. Sorties observées et attendues
1.9.a. Sortie observée pour chaque strate O(t)
72
-
Implémentation concrète
Python :
def compute_On(t: float, state: List[Dict], An_t: np.ndarray, fn_t: np.ndarray,
phi_n_t: np.ndarray, gamma_n_t: np.ndarray) -> np.ndarray:
"""
Calcule la sortie observée pour chaque strate.
O(t) = A(t) · sin(2π·f(t)·t + φ(t)) · γ(t)
Args:
t: temps actuel
state: état des strates
An_t: amplitudes
fn_t: fréquences
phi_n_t: phases
gamma_n_t: latences
Returns:
np.ndarray: sorties observées
Formule exploratoire
"""
N = len(state)
On_t = np.zeros(N)
for n in range(N):
# Contribution de la strate n au signal global
73
On_t[n] = An_t[n] * np.sin(2 * np.pi * fn_t[n] * t + phi_n_t[n]) * gamma_n_t[n]
return On_t
1.9.b. Sortie attendue (harmonique cible) pour chaque strate E(t)
-
Implémentation concrète
Python :
def compute_En(t: float, state: List[Dict], history: List[Dict], config: Dict,
history_align: List[float] = None) -> np.ndarray:
"""
Calcule la sortie attendue (harmonique cible) pour chaque strate.
NOUVEAU S1: Attracteur inertiel avec lambda_E adaptatif
E(t) = (1-λ) * E(t-dt) + λ * φ * O(t-τ)
où λ peut être modulé par k_spacing selon le nombre d'alignements
Args:
t: temps actuel
state: état des strates
history: historique
config: configuration
history_align: historique des alignements En≈On (nouveau S1)
Returns:
np.ndarray: sorties attendues
74
"""
N = len(state)
En_t = np.zeros(N)
# Paramètres de l'attracteur inertiel
lambda_E = config.get('regulation', {}).get('lambda_E', 0.05)
k_spacing = config.get('regulation', {}).get('k_spacing', 0.0)
phi = config.get('spiral', {}).get('phi', 1.618)
dt = config.get('system', {}).get('dt', 0.1)
# Adapter lambda selon le nombre d'alignements (spacing effect)
if history_align is not None and k_spacing > 0:
n_alignments = len(history_align)
lambda_dyn = lambda_E / (1 + k_spacing * n_alignments)
else:
lambda_dyn = lambda_E
if len(history) > 0:
# Récupérer les valeurs précédentes
last_En = history[-1].get('E', np.zeros(N))
last_On = history[-1].get('O', np.zeros(N))
# S'assurer que les arrays ont la bonne taille
if not isinstance(last_En, np.ndarray) or len(last_En) != N:
last_En = np.zeros(N)
for n in range(N):
last_En[n] = state[n]['A0']
75
if not isinstance(last_On, np.ndarray) or len(last_On) != N:
last_On = np.zeros(N)
# Attracteur inertiel : E(t) = (1-λ)*E(t-dt) + λ*φ*O(t-τ)
# τ = dt pour l'instant (peut être ajusté)
for n in range(N):
En_t[n] = (1 - lambda_dyn) * last_En[n] + lambda_dyn * phi * last_On[n]
else:
# Valeur initiale = amplitude de base
for n in range(N):
En_t[n] = state[n]['A0']
return En_t
1.10. Spiralisation
1.10.a. Ratio spiralé r(t)
r(t) est un ratio non constant mais pulsé — une fonction spiralante.
-
Expression mathématique de départ
On la note :
r(t) = φ + ε · sin(2π·ω·t + θ)
Où :
●
●
●
●
φ : le nombre d’or (≈ 1.618) comme attracteur stable
ε : petite variation harmonique (ex : 0.05 ou moins)
ω : fréquence de modulation (faible, lente respiration)
θ : phase de départ
76
Ce rapport rappelle le nombre d’or φ dans les spirales naturelles, mais ici, il varie subtilement
-
Implémentation concrète
Python :
def compute_r(t: float, phi: float, epsilon: float, omega: float, theta: float) -> float:
"""
Calcule le ratio spiralé.
r(t) = φ + ε · sin(2π·ω·t + θ)
Args:
t: temps actuel
phi: nombre d'or
epsilon: amplitude de variation
omega: fréquence de modulation
theta: phase initiale
Returns:
float: ratio spiralé
"""
return phi + epsilon * np.sin(2 * np.pi * omega * t + theta)
1.10.b. Coefficient d’accord spiralé C(t)
La coefficient C(t) module la synchronisation des strates, leur cohérence rythmique
-
Expression mathématique de départ
77
On la note :
C(t) = (1 / N) · ∑₌₁ⁿ⁻¹ cos(φ ₊₁(t) − φ(t))
●
●
●
Si C(t) tend vers 1, les strates sont alignées
Si C(t) varie en spiralant, on a une stabilité dynamique
Si C(t) chute, il y a dissonance ou rupture
-
Implémentation concrète
Python :
def compute_C(t: float, phi_n_array: np.ndarray) -> float:
"""
Calcule le coefficient d'accord spiralé.
C(t) = (1/N) · Σ cos(φ ₊₁ - φ)
Pour phases spiralantes (sans modulo), normalise les différences
pour mesurer la cohérence locale plutôt que l'alignement absolu.
Args:
t: temps actuel
phi_n_array: phases de toutes les strates (peuvent dépasser 2π)
Returns:
float: coefficient d'accord entre -1 et 1
"""
N = len(phi_n_array)
if N <= 1:
78
return 1.0
# Somme des cosinus entre phases adjacentes
cos_sum = 0.0
for n in range(N - 1):
# Différence de phase brute (peut être > 2π)
phase_diff = phi_n_array[n + 1] - phi_n_array[n]
# Pour phases spiralantes : mesurer cohérence locale
# en normalisant la différence modulo 2π
normalized_diff = ((phase_diff + np.pi) % (2 * np.pi)) - np.pi
cos_sum += np.cos(normalized_diff)
return cos_sum / (N - 1)
1.10.c. Modulation moyenne A(t)
-
Expression mathématique de départ
On la note :
A(t) = (1/N) · Σ Δf(t)
-
Implémentation concrète
Python :
def compute_A(t: float, delta_fn_array: np.ndarray) -> float:
79
"""
Calcule la modulation moyenne.
A(t) = (1/N) · Σ Δf(t)
Args:
t: temps actuel
delta_fn_array: modulations de fréquence
Returns:
float: modulation moyenne
"""
if len(delta_fn_array) == 0:
return 0.0
return np.mean(delta_fn_array)
1.10.d. Amplitude harmonisée A_spiral(t)
-
Expression mathématique de départ
On la note :
A_spiral(t) = C(t) · A(t)
-
Implémentation concrète
Python :
def compute_A_spiral(t: float, C_t: float, A_t: float) -> float:
80
"""
Calcule l'amplitude harmonisée.
A_spiral(t) = C(t) · A(t)
Args:
t: temps actuel
C_t: coefficient d'accord
A_t: modulation moyenne
Returns:
float: amplitude spiralée
"""
return C_t * A_t
1.11. Feedback pour une strate F(t)
Le feedback n’est plus une boucle rigide. Il devient une onde spiralée de réajustement.
-
Expression mathématique de départ
On la note :
F(t) = β · (O(t) - E(t)) · γ(t)
Où :
●
●
●
●
O(t) : sortie actuelle de la strate
E(t) : sortie attendue / espérée / harmonique
β : facteur d’amplification / plasticité de la strate
γ(t) : facteur de latence expressive (qui ralentit ou fluidifie le feedback)
81
-
Implémentation concrète
Python :
def compute_Fn(t: float, beta_n: float, On_t: float, En_t: float, gamma_t: float,
An_t: float, fn_t: float, config: dict) -> float:
"""
Calcule le feedback pour une strate.
F(t) = β · G(O(t) - E(t)) · γ(t)
où G peut être :
- Identité (pas de régulation)
- Archétype simple (tanh, sinc, resonance, adaptive)
- Gn complet avec sinc et enveloppe
Args:
t: temps actuel
beta_n: plasticité de la strate
On_t: sortie observée
En_t: sortie attendue
gamma_t: latence globale
An_t: amplitude actuelle
fn_t: fréquence actuelle
config: configuration
Returns:
float: valeur de feedback
82
"""
error = On_t - En_t
# Récupérer le mode de feedback depuis config
feedback_mode = config.get('regulation', {}).get('feedback_mode', 'simple')
if feedback_mode == 'simple':
# Formule de base sans régulation G
return beta_n * error * gamma_t
elif feedback_mode == 'archetype':
# Utiliser un archétype G simple
G_arch = config.get('regulation', {}).get('G_arch', 'tanh')
G_params = {
'lambda': config.get('regulation', {}).get('lambda', 1.0),
'alpha': config.get('regulation', {}).get('alpha', 1.0),
'beta': config.get('regulation', {}).get('beta', 2.0)
}
G_feedback = regulation.compute_G(error, G_arch, G_params)
return beta_n * G_feedback * gamma_t
elif feedback_mode == 'gn_full':
# Utiliser Gn complet avec sinc et enveloppe
env_config = config.get('enveloppe', {})
T = config['system']['T']
# Centre et largeur de l'enveloppe
mu_n_t = regulation.compute_mu_n(t, env_config.get('env_mode', 'static'),
83
env_config.get('mu_n', 0.0))
sigma_n_t = regulation.compute_sigma_n(t, env_config.get('env_mode', 'static'), T,
env_config.get('sigma_n_static', 0.1))
# Type d'enveloppe (gaussienne ou sigmoïde)
env_type = env_config.get('env_type', 'gaussienne')
# Calcul de l'enveloppe
env_n = regulation.compute_env_n(error, t, env_config.get('env_mode', 'static'),
sigma_n_t, mu_n_t, T, env_type)
# Régulation complète avec Gn
G_feedback = regulation.compute_Gn(error, t, An_t, fn_t, mu_n_t, env_n)
return beta_n * G_feedback * gamma_t
else:
# Mode non reconnu, fallback sur simple
print(f"⚠ Mode de feedback '{feedback_mode}' non reconnu, utilisation du mode simple")
return beta_n * error * gamma_t
1.12. Inputs contextuels I(t)
1.12.a. Fonction
-
Implémentation concrète
Python :
def compute_In(t: float, perturbation_config: Dict[str, Any], N: Optional[int] = None) -> Union[float, np.ndarray]:
84
"""
Calcule l'input contextuel pour toutes les strates.
Args:
t: temps actuel
perturbation_config: configuration de perturbation depuis config.json
N: nombre de strates (optionnel, pour retourner un array)
Returns:
float ou np.ndarray: valeur(s) d'input contextuel
Modes supportés:
- "constant": valeur fixe
- "choc": impulsion à t0
- "rampe": augmentation linéaire
- "sinus": oscillation périodique
- "uniform": U[0,1] aléatoire
- "none": pas de perturbation (0.0)
"""
mode = perturbation_config.get('type', 'none')
amplitude = perturbation_config.get('amplitude', 1.0)
t0 = perturbation_config.get('t0', 0.0)
# Calcul de la valeur de base selon le mode
if mode == "constant":
value = amplitude
elif mode == "choc":
85
# Impulsion brève à t0
dt = perturbation_config.get('dt', 0.05) # durée du pic
if abs(t - t0) < dt:
value = amplitude
else:
value = 0.0
elif mode == "rampe":
# Augmentation linéaire de 0 à amplitude
duration = perturbation_config.get('duration', 10.0)
if t < t0:
value = 0.0
elif t < t0 + duration:
value = amplitude * (t - t0) / duration
else:
value = amplitude
elif mode == "sinus":
# Oscillation périodique
freq = perturbation_config.get('freq', 0.1)
if t >= t0:
value = amplitude * np.sin(2 * np.pi * freq * (t - t0))
else:
value = 0.0
elif mode == "uniform":
# Bruit uniforme U[0,1] * amplitude
value = amplitude * np.random.uniform(0, 1)
86
else: # "none" ou mode inconnu
value = 0.0
# Retourner un array si N est spécifié
if N is not None:
return np.full(N, value)
return value
1.12.b. Module
-
Implémentation concrète
Python :
import numpy as np
from typing import Dict, List, Union, Optional, Any, Tuple
import json
import warnings
from dataclasses import dataclass
from enum import Enum
from utils import deep_convert
# ============== TYPES DE PERTURBATIONS ==============
class PerturbationType(Enum):
"""Énumération des types de perturbations disponibles."""
NONE = "none"
87
CHOC = "choc"
RAMPE = "rampe"
SINUS = "sinus"
BRUIT = "bruit"
COMPOSITE = "composite" # Pour les séquences complexes
@dataclass
class PerturbationConfig:
"""Configuration structurée d'une perturbation."""
type: str
t0: float = 0.0
amplitude: float = 1.0
duration: Optional[float] = None
freq: Optional[float] = None
phase: Optional[float] = 0.0
seed: Optional[int] = None
@classmethod
def from_dict(cls, config: Dict) -> 'PerturbationConfig':
"""Crée une configuration depuis un dictionnaire."""
return cls(**{k: v for k, v in config.items() if k in cls.__annotations__})
# ============== GÉNÉRATEURS DE PERTURBATIONS ==============
def generate_perturbation(t: float, config: Union[Dict, PerturbationConfig]) -> float:
"""
88
Génère une perturbation selon la configuration.
Args:
t: temps actuel
config: configuration de la perturbation
Returns:
float: valeur de la perturbation à l'instant t
Types supportés:
- "choc": impulsion à t0
- "rampe": croissance linéaire
- "sinus": oscillation périodique
- "bruit": variation aléatoire
- "none": pas de perturbation
"""
# Gestion des configurations vides ou invalides
if isinstance(config, dict):
if not config or 'type' not in config:
return 0.0 # Pas de perturbation si configuration invalide
config = PerturbationConfig.from_dict(config)
pert_type = config.type.lower()
if pert_type == PerturbationType.NONE.value:
return 0.0
elif pert_type == PerturbationType.CHOC.value:
89
return generate_choc(t, config)
elif pert_type == PerturbationType.RAMPE.value:
return generate_rampe(t, config)
elif pert_type == PerturbationType.SINUS.value:
return generate_sinus(t, config)
elif pert_type == PerturbationType.BRUIT.value:
return generate_bruit(t, config)
else:
warnings.warn(f"Type de perturbation '{pert_type}' non reconnu. Retour à 0.")
return 0.0
def generate_choc(t: float, config: PerturbationConfig) -> float:
"""
Génère une perturbation de type choc (impulsion).
Le choc peut avoir une durée configurable pour modéliser
des impulsions brèves mais non instantanées.
Args:
t: temps actuel
config: configuration
Returns:
90
float: amplitude si dans la fenêtre du choc, 0 sinon
"""
# Durée du choc (par défaut très brève)
duration = config.duration if config.duration else 0.1
# Vérifier si on est dans la fenêtre du choc
if config.t0 <= t < config.t0 + duration:
# Option : profil du choc (rectangulaire par défaut)
# On pourrait ajouter un profil gaussien ou triangulaire
return config.amplitude
else:
return 0.0
def generate_rampe(t: float, config: PerturbationConfig) -> float:
"""
Génère une perturbation de type rampe (croissance linéaire).
La rampe peut être bornée ou non selon la durée spécifiée.
Args:
t: temps actuel
config: configuration
Returns:
float: valeur de la rampe
"""
if t < config.t0:
91
return 0.0
# Temps écoulé depuis le début
elapsed = t - config.t0
if config.duration is not None:
# Rampe bornée
if elapsed >= config.duration:
return config.amplitude
else:
# Croissance linéaire
return config.amplitude * (elapsed / config.duration)
else:
# Rampe non bornée (croissance infinie)
# On peut ajouter un taux de croissance
growth_rate = config.amplitude / 10.0 # Par défaut : amplitude/10 par unité de temps
return growth_rate * elapsed
def generate_sinus(t: float, config: PerturbationConfig) -> float:
"""
Génère une perturbation sinusoïdale.
Permet de modéliser des environnements cycliques
ou des influences périodiques.
Args:
t: temps actuel
92
config: configuration
Returns:
float: valeur sinusoïdale
"""
if t < config.t0:
return 0.0
# Fréquence par défaut
freq = config.freq if config.freq is not None else 0.1
# Phase initiale
phase = config.phase if config.phase is not None else 0.0
# Sinusoïde
return config.amplitude * np.sin(2 * np.pi * freq * (t - config.t0) + phase)
def generate_bruit(t: float, config: PerturbationConfig) -> float:
"""
Génère une perturbation de type bruit.
Plusieurs types de bruit sont disponibles :
- Uniforme : distribution uniforme
- Gaussien : distribution normale
- Rose/Brown : bruit coloré (à implémenter)
Args:
93
t: temps actuel
config: configuration
Returns:
float: valeur aléatoire
"""
# Seed pour reproductibilité si spécifiée
if config.seed is not None:
# Seed basée sur le temps pour variation mais reproductible
np.random.seed(int(config.seed + t * 1000) % 2**32)
# Type de bruit (uniforme par défaut)
# On pourrait étendre avec un paramètre noise_type
return config.amplitude * np.random.uniform(-1, 1)
# ============== APPLICATION DES PERTURBATIONS ==============
def apply_perturbation_to_In(In_array: np.ndarray,
perturbation_value: float) -> np.ndarray:
"""
Applique une perturbation aux inputs contextuels.
Args:
In_array: array des inputs pour chaque strate
perturbation_value: valeur de la perturbation
Returns:
94
np.ndarray: inputs perturbés
"""
# Application simple : addition
# On pourrait avoir des modes plus sophistiqués
return In_array + perturbation_value
def apply_perturbation_selective(In_array: np.ndarray,
perturbation_value: float,
strata_mask: Optional[np.ndarray] = None) -> np.ndarray:
"""
Applique une perturbation sélectivement à certaines strates.
Args:
In_array: array des inputs
perturbation_value: valeur de la perturbation
strata_mask: masque booléen pour sélectionner les strates
Returns:
np.ndarray: inputs perturbés sélectivement
"""
result = In_array.copy()
if strata_mask is not None:
# Appliquer seulement aux strates sélectionnées
result[strata_mask] += perturbation_value
else:
# Appliquer à toutes
95
result += perturbation_value
return result
# ============== SÉQUENCES DE PERTURBATIONS ==============
def generate_perturbation_sequence(T: float, dt: float,
perturbation_configs: List[Dict]) -> np.ndarray:
"""
Génère une séquence complète de perturbations combinées.
Permet de créer des scénarios complexes avec plusieurs
perturbations qui se chevauchent ou se succèdent.
Args:
T: durée totale
dt: pas de temps
perturbation_configs: liste de configurations
Returns:
np.ndarray: séquence temporelle des perturbations
"""
# Array temporel
t_array = np.arange(0, T, dt)
n_steps = len(t_array)
# Initialiser la séquence
96
sequence = np.zeros(n_steps)
# Appliquer chaque perturbation
for config in perturbation_configs:
for i, t in enumerate(t_array):
sequence[i] += generate_perturbation(t, config)
return sequence
def create_scenario(scenario_name: str, T: float, base_amplitude: float = 1.0) -> List[Dict]:
"""
Crée des scénarios de perturbations prédéfinis.
Args:
scenario_name: nom du scénario
T: durée totale
base_amplitude: amplitude de base
Returns:
List[Dict]: liste de configurations de perturbations
Scénarios disponibles:
- "stress_test": chocs répétés
- "environnement_variable": sinus + bruit
- "crise_recovery": choc fort puis rampe douce
- "chaos": bruit intense avec pics aléatoires
"""
97
scenarios = {
"stress_test": [
{"type": "choc", "t0": T*0.2, "amplitude": base_amplitude*2, "duration": 0.5},
{"type": "choc", "t0": T*0.4, "amplitude": base_amplitude*1.5, "duration": 0.5},
{"type": "choc", "t0": T*0.6, "amplitude": base_amplitude*2.5, "duration": 0.5},
{"type": "choc", "t0": T*0.8, "amplitude": base_amplitude*3, "duration": 0.5}
],
"environnement_variable": [
{"type": "sinus", "t0": 0, "amplitude": base_amplitude*0.5, "freq": 0.1},
{"type": "sinus", "t0": T*0.3, "amplitude": base_amplitude*0.3, "freq": 0.3, "phase": np.pi/4},
{"type": "bruit", "t0": T*0.5, "amplitude": base_amplitude*0.2}
],
"crise_recovery": [
{"type": "choc", "t0": T*0.3, "amplitude": base_amplitude*5, "duration": 1.0},
{"type": "rampe", "t0": T*0.5, "amplitude": -base_amplitude*2, "duration": T*0.3}
],
"chaos": [
{"type": "bruit", "t0": 0, "amplitude": base_amplitude*0.5},
{"type": "choc", "t0": T*0.15, "amplitude": base_amplitude*3, "duration": 0.2},
{"type": "choc", "t0": T*0.35, "amplitude": -base_amplitude*2, "duration": 0.3},
{"type": "sinus", "t0": T*0.5, "amplitude": base_amplitude, "freq": 0.5},
{"type": "choc", "t0": T*0.75, "amplitude": base_amplitude*4, "duration": 0.1}
]
}
98
if scenario_name in scenarios:
return scenarios[scenario_name]
else:
warnings.warn(f"Scénario '{scenario_name}' non reconnu. Retour à une perturbation simple.")
return [{"type": "choc", "t0": T/2, "amplitude": base_amplitude}]
# ============== ANALYSE DES PERTURBATIONS ==============
def analyze_perturbation_impact(S_history: np.ndarray,
perturbation_sequence: np.ndarray,
dt: float) -> Dict[str, float]:
"""
Analyse l'impact des perturbations sur le signal.
Args:
S_history: historique du signal S(t)
perturbation_sequence: séquence des perturbations
dt: pas de temps
Returns:
Dict avec métriques d'impact
"""
# Corrélation perturbation-signal
if len(S_history) == len(perturbation_sequence):
correlation = np.corrcoef(S_history, perturbation_sequence)[0, 1]
else:
correlation = 0.0
99
# Délai de réponse (cross-corrélation)
if len(S_history) > 10 and len(perturbation_sequence) > 10:
xcorr = np.correlate(S_history, perturbation_sequence, mode='same')
delay_idx = np.argmax(np.abs(xcorr)) - len(xcorr)//2
response_delay = delay_idx * dt
else:
response_delay = 0.0
# Amplification/Atténuation
pert_energy = np.sum(perturbation_sequence**2)
signal_energy = np.sum(S_history**2)
if pert_energy > 0:
amplification = signal_energy / pert_energy
else:
amplification = 1.0
# Persistance de l'effet
# (combien de temps le signal reste perturbé après la fin de la perturbation)
pert_end_idx = np.where(perturbation_sequence != 0)[0]
if len(pert_end_idx) > 0:
last_pert_idx = pert_end_idx[-1]
if last_pert_idx < len(S_history) - 10:
post_pert_std = np.std(S_history[last_pert_idx:])
baseline_std = np.std(S_history[:min(100, last_pert_idx)])
persistence = post_pert_std / (baseline_std + 1e-10)
else:
100
persistence = 1.0
else:
persistence = 0.0
return deep_convert({
'correlation': correlation,
'response_delay': response_delay,
'amplification': amplification,
'persistence': persistence
})
# ============== VISUALISATION DES PERTURBATIONS ==============
def plot_perturbation_profile(config: Union[Dict, PerturbationConfig],
T: float, dt: float = 0.1) -> Tuple[np.ndarray, np.ndarray]:
"""
Génère le profil temporel d'une perturbation pour visualisation.
Args:
config: configuration de la perturbation
T: durée totale
dt: pas de temps
Returns:
Tuple[np.ndarray, np.ndarray]: (temps, valeurs)
"""
t_array = np.arange(0, T, dt)
101
values = np.array([generate_perturbation(t, config) for t in t_array])
return t_array, values
# ============== EXPORT/IMPORT DE SCÉNARIOS ==============
def save_scenario(scenario_configs: List[Dict], filename: str) -> None:
"""
Sauvegarde un scénario de perturbations dans un fichier JSON.
Args:
scenario_configs: liste de configurations
filename: nom du fichier
"""
with open(filename, 'w') as f:
json.dump(deep_convert({
'version': '1.0',
'timestamp': np.datetime64('now').astype(str),
'perturbations': scenario_configs
}), f, indent=2)
print(f"Scénario sauvegardé : {filename}")
def load_scenario(filename: str) -> List[Dict]:
"""
Charge un scénario depuis un fichier JSON.
102
Args:
filename: nom du fichier
Returns:
List[Dict]: liste de configurations
"""
with open(filename, 'r') as f:
data = json.load(f)
return data.get('perturbations', [])
# ============== NOUVELLE ARCHITECTURE In(t) ==============
def compute_In(t: float, input_cfg: Dict, state: Optional[Dict] = None,
history: Optional[List[Dict]] = None, dt: float = 0.05) -> float:
"""
Calcule In(t) selon la nouvelle architecture modulaire.
In(t) = Offset(t) + Gain(t) · tanh(In_raw(t) / scale)
où In_raw(t) = Σᵢ wᵢ · Pertᵢ(t)
Args:
t: temps actuel
input_cfg: configuration de l'input contextuel
state: état du système (pour modes adaptatifs)
history: historique pour calculs adaptatifs
103
dt: pas de temps
Returns:
float: valeur de In(t) garantie > 0
Structure de input_cfg:
{
"baseline": {
"offset_mode": "static" | "adaptive",
"offset": 0.1, # pour mode static
"offset_adaptive": {...}, # pour mode adaptive
"gain_mode": "static" | "adaptive",
"gain": 1.0, # pour mode static
"gain_adaptive": {...} # pour mode adaptive
},
"scale": 1.0,
"perturbations": [
{"type": "sinus", "weight": 1.0, ...},
{"type": "choc", "weight": 0.5, ...}
]
}
"""
# Configuration par défaut
baseline = input_cfg.get('baseline', {})
scale = input_cfg.get('scale', 1.0)
perturbations = input_cfg.get('perturbations', [])
# ----- 1. Calcul de l'Offset -----
104
offset_mode = baseline.get('offset_mode', 'static')
if offset_mode == 'static':
offset = baseline.get('offset', 0.1)
else: # adaptive
offset = compute_adaptive_offset(t, baseline.get('offset_adaptive', {}),
state, history, dt)
# ----- 2. Calcul du Gain -----
gain_mode = baseline.get('gain_mode', 'static')
if gain_mode == 'static':
gain = baseline.get('gain', 1.0)
else: # adaptive
gain = compute_adaptive_gain(t, baseline.get('gain_adaptive', {}),
state, history, dt)
# ----- 3. Superposition des perturbations -----
In_raw = 0.0
for pert_cfg in perturbations:
weight = pert_cfg.get('weight', 1.0)
pert_value = generate_perturbation(t, pert_cfg)
In_raw += weight * pert_value
# ----- 4. Normalisation douce avec tanh -----
# tanh borne In_raw dans [-1, 1], donc In_final reste dans [offset-gain, offset+gain]
In_normalized = np.tanh(In_raw / scale)
In_final = offset + gain * In_normalized
105
# Garantir In > 0
epsilon = 0.001 # Valeur minimale absolue
In_final = max(In_final, epsilon)
return In_final
def compute_adaptive_offset(t: float, offset_cfg: Dict, state: Optional[Dict],
history: Optional[List[Dict]], dt: float) -> float:
"""
Calcule un offset adaptatif pour maintenir σ(In) dans une plage cible.
Args:
offset_cfg: configuration de l'offset adaptatif
- "target_sigma": valeur cible pour σ(In) (défaut: 0.5)
- "k_A": gain de l'intégrateur (défaut: 0.01)
- "min": offset minimum (défaut: 0.05)
- "max": offset maximum (défaut: 0.5)
- "window": taille de fenêtre pour moyenner (défaut: 20)
"""
# Paramètres
target_sigma = offset_cfg.get('target_sigma', 0.5)
k_A = offset_cfg.get('k_A', 0.01)
offset_min = offset_cfg.get('min', 0.05)
offset_max = offset_cfg.get('max', 0.5)
window_size = offset_cfg.get('window', 20)
106
# Récupérer l'offset actuel depuis l'état
if state is None:
state = {}
if 'adaptive_offset' not in state:
state['adaptive_offset'] = offset_cfg.get('initial', 0.1)
current_offset = state['adaptive_offset']
# Calculer σ moyen sur la fenêtre récente
if history and len(history) >= window_size:
# Récupérer les In récents et calculer leurs sigmoïdes
recent_In = [h.get('In', 0.1) for h in history[-window_size:]]
# Sigmoïde simple : 1/(1+exp(-x))
recent_sigma = [1.0 / (1.0 + np.exp(-In)) for In in recent_In]
mean_sigma = np.mean(recent_sigma)
# Intégrateur : dA/dt = k_A * (target - actual)
error = target_sigma - mean_sigma
dA = k_A * error * dt
# Mise à jour avec saturation
new_offset = np.clip(current_offset + dA, offset_min, offset_max)
state['adaptive_offset'] = new_offset
return new_offset
else:
# Pas assez d'historique, retourner l'offset actuel
107
return current_offset
def compute_adaptive_gain(t: float, gain_cfg: Dict, state: Optional[Dict],
history: Optional[List[Dict]], dt: float) -> float:
"""
Calcule un gain adaptatif pour éviter la saturation.
Args:
gain_cfg: configuration du gain adaptatif
- "target_range": amplitude cible pour In_raw (défaut: 2.0)
- "k_G": gain du contrôleur (défaut: 0.05)
- "min": gain minimum (défaut: 0.1)
- "max": gain maximum (défaut: 2.0)
- "window": taille de fenêtre (défaut: 10)
"""
# Paramètres
target_range = gain_cfg.get('target_range', 2.0)
k_G = gain_cfg.get('k_G', 0.05)
gain_min = gain_cfg.get('min', 0.1)
gain_max = gain_cfg.get('max', 2.0)
window_size = gain_cfg.get('window', 10)
# Récupérer le gain actuel depuis l'état
if state is None:
state = {}
if 'adaptive_gain' not in state:
108
state['adaptive_gain'] = gain_cfg.get('initial', 1.0)
current_gain = state['adaptive_gain']
# Calculer |In_raw| moyen récent
if history and len(history) >= window_size:
# Note: il faudrait stocker In_raw dans l'historique
# Pour l'instant, on estime depuis les variations de In
recent_In = [h.get('In', 0.1) for h in history[-window_size:]]
In_variations = np.std(recent_In)
# Si les variations sont trop grandes, réduire le gain
error = In_variations - target_range
dG = -k_G * error * dt
# Mise à jour avec saturation
new_gain = np.clip(current_gain + dG, gain_min, gain_max)
state['adaptive_gain'] = new_gain
else:
return new_gain
return current_gain
1.13. Métriques globales
1.13.a. Énergie totale du système E(t)
-
Expression mathématique de départ
109
On la note :
E(t) = max |A(t)|
-
Implémentation concrète
Python :
def compute_E(t: float, signal_array: Union[np.ndarray, List[float]]) -> float:
"""
Calcule l'énergie totale du système.
E(t) = sqrt(Σ A²(t)) / sqrt(N)
Représente l'énergie totale distribuée dans le système, utilisée pour évaluer la capacité du système à
maintenir une activité cohérente.
Args:
t: temps actuel
signal_array: amplitudes A(t) de chaque strate
Returns:
float: énergie totale normalisée
"""
if len(signal_array) == 0:
return 0.0
# Convertir en array numpy si nécessaire
110
amplitudes = np.asarray(signal_array)
# Énergie comme norme L2 des amplitudes
energy = np.sqrt(np.sum(np.square(amplitudes)))
# Normaliser par sqrt(N) pour avoir une mesure comparable
# indépendamment du nombre de strates
N = len(amplitudes)
if N > 0:
energy = energy / np.sqrt(N)
return energy
1.13.b. Indice de la strate avec la variation d’amplitude maximale L(t)
-
Expression mathématique de départ
On la note :
L(t) = argmax |dA(t)/dt|
-
Implémentation concrète
Python :
def compute_L(t: float, An_history: List[np.ndarray], dt: float = 0.1) -> int:
"""
Calcule L(t) : argmax |dA(t)/dt|
111
Retourne l'indice de la strate avec la variation d'amplitude maximale.
"Latence maximale de variation d'une strate" - quelle strate change le plus vite.
Args:
t: temps actuel (pour compatibilité, pas utilisé dans cette version)
An_history: historique des amplitudes [An(t-dt), An(t), ...]
dt: pas de temps pour calcul dérivée
Returns:
int: indice de la strate avec |dA/dt| maximal
"""
if len(An_history) < 2:
# Pas assez d'historique pour dérivée
return 0
# Derniers états pour calcul dérivée
An_current = np.asarray(An_history[-1]) # An(t)
An_previous = np.asarray(An_history[-2]) # An(t-dt)
if len(An_current) == 0 or len(An_previous) == 0:
return 0
# Calcul dérivées : dA/dt ≈ (An(t) - An(t-dt)) / dt
derivatives = np.abs((An_current - An_previous) / dt)
# Retourner indice de variation maximale
return int(np.argmax(derivatives))
112
1.13.c. Version legacy de compute
_
L (pour compatibilité si besoin) L
_
legacy
-
Implémentation concrète
Python :
def compute_L_legacy(t: float, signal_array: Union[np.ndarray, List[float]]) -> int:
"""
Version legacy de compute_L (pour compatibilité si besoin).
Args:
t: temps actuel
signal_array: signaux ou amplitudes
Returns:
int: indice de la strate avec amplitude max
"""
if len(signal_array) == 0:
return 0
# Indice de la strate dominante
return int(np.argmax(np.abs(signal_array)))
1.14. Régimes d’exploration/exploitation des synergies optimales de combinaisons γ(t)
et G par rapport au type d’input et à l’état du système (transcendant
_
synergy,
spacing_
schedule, spacing_
bascule, silence), en fonction des états émergents du
système mesurés par sentiment.py
1.14.a. transcendant
_
synergy
Cf gamma_adaptive aware - en cours d’implémentation
113
1.14.b. spacing_
schedule
-
Implémentation concrète (module)
Python :
from typing import Dict, List
def build_spacing_schedule(total_time: float,
start_interval: float = 2.0,
growth: float = 1.5,
num_blocks: int = 8,
order: List[str] = None) -> Dict[str, List[float]]:
"""
Build an exponentially spaced sequence of trigger times alternating between
gamma-peaks and G-switches (or any provided order), until total_time.
Args:
total_time: total simulation time (seconds)
start_interval: first gap (seconds)
growth: multiplicative growth per gap (>1)
num_blocks: maximum number of blocks to schedule (<=0 means fill until total_time)
order: sequence of labels to repeat (default: ['gamma', 'G'])
Returns:
dict with 'gamma_peaks': [times...], 'G_switches': [times...]
"""
114
if order is None or len(order) == 0:
order = ['gamma', 'G']
gamma_peaks: List[float] = []
G_switches: List[float] = []
t = 0.0
gap = max(1e-3, float(start_interval))
block = 0
infinite = num_blocks is None or int(num_blocks) <= 0
while (infinite or block < int(num_blocks)) and t < total_time:
t += gap
label = order[block % len(order)]
if t >= total_time:
break
if label.lower().startswith('g') and label.lower() != 'gamma':
G_switches.append(t)
else:
gamma_peaks.append(t)
gap *= max(1.0, float(growth))
block += 1
return {
'gamma_peaks': gamma_peaks,
'G_switches': G_switches
}
Module servant de carte temporelle pour les fonctions adaptive_aware
1.14.c. spacing_
bascule
115
En cours d’implémentation
1.14.d. silence
En cours d’implémentation
1.15. Fonctions utilitaires
1.15.a. update
_
state
-
Implémentation concrète
Python :
def update_state(state: List[Dict], An_t: np.ndarray, fn_t: np.ndarray,
phi_n_t: np.ndarray, gamma_n_t: np.ndarray, F_n_t: np.ndarray) -> List[Dict]:
"""
Met à jour l'état complet du système.
Args:
state: état actuel
An_t: amplitudes calculées
fn_t: fréquences calculées
phi_n_t: phases calculées
gamma_n_t: latences calculées
F_n_t: feedbacks calculés
Returns:
État mis à jour
"""
N = len(state)
116
for n in range(N):
# Mise à jour des valeurs courantes
state[n]['current_An'] = An_t[n] if n < len(An_t) else state[n].get('A0', 1.0)
state[n]['current_fn'] = fn_t[n] if n < len(fn_t) else state[n].get('f0', 1.0)
state[n]['current_phi'] = phi_n_t[n] if n < len(phi_n_t) else state[n].get('phi', 0.0)
state[n]['current_gamma'] = gamma_n_t[n] if n < len(gamma_n_t) else 1.0
state[n]['current_Fn'] = F_n_t[n] if n < len(F_n_t) else 0.0
# NOUVEAU : Mise à jour des valeurs de base pour la prochaine itération
# Ceci permet l'évolution temporelle du système
# if 'current_An' in state[n] and state[n]['current_An'] != 0:
# Evolution progressive de A0 vers la valeur courante
# Taux réduit pour éviter l'extinction du signal
# adaptation_rate = 0.01 # Réduit de 0.1 à 0.01
# Conserver une amplitude minimale pour éviter l'extinction
# min_amplitude = 0.1
# new_A0 = state[n]['A0'] * (1 - adaptation_rate) + state[n]['current_An'] * adaptation_rate
# state[n]['A0'] = max(min_amplitude, new_A0)
# if 'current_fn' in state[n]:
# Evolution progressive de f0 vers la valeur courante
# adaptation_rate = 0.005 # Réduit de 0.05 à 0.005
# state[n]['f0'] = state[n]['f0'] * (1 - adaptation_rate) + state[n]['current_fn'] * adaptation_rate
# NOUVEAU : Mise à jour des phases si mode dynamique
# if 'current_phi' in state[n]:
# state[n]['phi'] = state[n]['current_phi']
117
return state
1.15.b. Paramètres config.json
-
Implémentation concrète (exemple de paramétrage avec une des configurations de test)
JSON :
{
"system": {
"N": 5,
"T": 100,
"dt": 0.1,
"seed": 12345,
"mode": "FPS",
"signal_mode": "extended",
"logging": {
"level": "INFO",
"output": "csv",
"log_metrics": [
"t",
"S(t)",
"C(t)",
"A_spiral(t)",
"E(t)",
"L(t)",
"cpu_step(t)",
"effort(t)",
"A_mean(t)",
118
"f_mean(t)",
"variance_d2S",
"fluidity",
"entropy_S",
"mean_abs_error",
"mean_high_effort",
"d_effort_dt",
"t_retour",
"max_median_ratio",
"continuous_resilience",
"adaptive_resilience",
"effort_status",
"En_mean(t)",
"On_mean(t)",
"gamma",
"gamma_mean(t)",
"G_arch_used",
"In_mean(t)",
"An_mean(t)",
"fn_mean(t)",
"spacing_gamma_bias",
"spacing_G_hint"
]
},
"input": {
"baseline": {
"offset_mode": "static",
"offset": 0.1,
119
"offset_adaptive": {
"target_sigma": 0.5,
"k_A": 0.01,
"min": 0.05,
"max": 0.5,
"window": 20,
"initial": 0.1
},
"gain_mode": "static",
"gain": 1.0,
"gain_adaptive": {
"target_range": 2.0,
"k_G": 0.05,
"min": 0.1,
"max": 2.0,
"window": 10,
"initial": 1.0
}
},
"scale": 1.2,
"perturbations": [
{
"type": "sinus",
"amplitude": 0.4,
"t0": 0.0,
"freq": 0.1,
"weight": 1.0
}
120
]
}
},
"adaptive_windows": {
"exploration": {
"target_percent": 0.10,
"min_absolute": 30,
"max_percent": 0.25
},
"gamma_adaptation": {
"target_percent": 0.10,
"min_absolute": 50,
"max_percent": 0.20
},
"G_effectiveness": {
"target_percent": 0.06,
"min_absolute": 30,
"max_percent": 0.15
},
"scoring": {
"immediate": {
"target_percent": 0.02,
"min_absolute": 10
},
"recent": {
"target_percent": 0.10,
"min_absolute": 30
},
121
"medium": {
"target_percent": 0.40,
"min_absolute": 100
}
},
"transition_smoothing": {
"target_percent": 0.02,
"min_absolute": 10
},
"pattern_detection": {
"target_percent": 0.15,
"min_absolute": 50
}
},
"coupling": {
"type": "spiral",
"c": 0.1,
"closed": false,
"mirror": true
},
"strates": [
{
"id": 0,
"A0": 0.268941,
"f0": 0.7,
"phi": 0.0,
"alpha": 0.5,
"beta": 0.22,
122
"k": 2.0,
"x0": 0.5,
"w": [
0.0,
0.8,
0.3,
-0.5,
-0.6
]
},
{
"id": 1,
"A0": 0.5,
"f0": 1.3,
"phi": 0.0,
"alpha": 0.6,
"beta": 0.32,
"k": 2.5,
"x0": 0.4,
"w": [
0.8,
0.0,
0.7,
-0.6,
-0.9
]
},
{
"id": 2,
"A0": 0.7,
"f0": 0.4,
"phi": 0.0,
"alpha": 0.6,
"beta": 0.35,
"k": 1.8,
"x0": 0.6,
"w": [
0.3,
0.7,
0.0,
0.8,
-1.8
]
},
{
"id": 3,
"A0": 0.4,
"f0": 1.8,
"phi": 0.0,
"alpha": 0.7,
"beta": 0.24,
"k": 2.2,
"x0": 0.45,
"w": [
-0.5,
-0.6,
123
124
0.8,
0.0,
0.3
]
},
{
"id": 4,
"A0": 0.6,
"f0": 2.4,
"phi": 0.0,
"alpha": 0.45,
"beta": 0.38,
"k": 1.9,
"x0": 0.55,
"w": [
-0.6,
-0.9,
1.5,
0.0,
0.0
]
}
],
"dynamic_parameters": {
"dynamic_phi": true,
"dynamic_beta": true,
"dynamic_alpha": false,
"dynamic_gamma": true,
125
"dynamic_G": true
},
"spiral": {
"phi": 1.618,
"epsilon": 0.1,
"omega": 0.05,
"theta": 0.0,
"signature_mode": "individual"
},
"regulation": {
"feedback_mode": "archetype",
"G_arch": "adaptive_aware",
"lambda": 1.7,
"alpha": 0.6,
"beta": 0.6,
"dynamic_G": true,
"lambda_E": 0.10,
"k_spacing": 0.0,
"epsilon_E": 0.01
},
"latence": {
"gamma_mode": "adaptive_aware",
"gamma_static_value": 1.0,
"gamma_dynamic": {
"k": 1.0,
"t0": 25
},
"strata_delay": false
126
},
"enveloppe": {
"env_type": "gaussienne",
"env_mode": "dynamic",
"mu_n": 0.0,
"sigma_n_static": 0.2,
"sigma_n_dynamic": {
"amp": 0.10,
"freq": 0.3,
"offset": 0.1,
"T": 50
}
},
"temporal_regulation": {
"use_temporal": true,
"eta_mode": "adaptive",
"eta_value": 1.0,
"theta_mode": "resonant",
"theta_value": 1.0
},
"modulation": {
"enabled": true,
"k_error": 0.1,
"k_amplitude": 0.1,
"k_frequency": 0.05,
"gamma_min": 0.5,
"gamma_max": 1.5,
"verbose": true
127
},
"exploration": {
"metrics": [
"S(t)",
"C(t)",
"A_mean(t)",
"f_mean(t)",
"entropy_S",
"effort(t)",
"mean_high_effort",
"d_effort_dt",
"mean_abs_error"
],
"window_sizes": [
1,
10,
100
],
"fractal_threshold": 0.8,
"detect_fractal_patterns": true,
"detect_anomalies": true,
"detect_harmonics": true,
"recurrence_window": [
1,
10,
100
],
"anomaly_threshold": 3.0,
128
"min_duration": 3,
"spacing_effect": {
"enabled": true,
"start_interval": 2.0,
"growth": 1.5,
"num_blocks": 0,
"order": ["gamma", "G", "gamma", "G"]
}
},
"to_calibrate": {
"variance_d2S": 0.01,
"fluidity_threshold": 0.3,
"stability_ratio": 10,
"resilience": 2,
"entropy_S": 0.5,
"mean_high_effort": 2,
"d_effort_dt": 5,
"t_retour": 1,
"effort_transitoire_threshold": 2.0,
"effort_chronique_threshold": 1.5,
"gamma_n": 1.0,
"env_n": "gaussienne",
"sigma_n": 0.1,
"cpu_step_ctrl": 2,
"max_chaos_events": 5
},
"validation": {
"criteria": [
129
"fluidity",
"stability",
"resilience",
"innovation",
"regulation",
"cpu_cost",
"effort_internal",
"effort_transient"
],
"alert_sigma": 3,
"batch_size": 5,
"refine_after_runs": true,
"auto_log_refinement": true
},
"debug": {
"log_detailed": true,
"log_interval": 10
},
"analysis": {
"compare_kuramoto": true,
"save_indiv_files": true,
"export_html_report": true,
"visualize_grid": true
}
}
130
PARTIE II - VALIDATION
2. Pipeline de tests, hypothèses et résultats
2.1. Pipeline de tests
2.1.a. Architecture pipeline
1) Vue globale via main.py
●
●
●
●
●
●
●
●
Validation complète via validate
_
config.py
Exécution via simulate.py (FPS/Kuramoto/Neutral) : soit via run(), soit via complete()
Exploration via explore.py
Analyse batch via analyse.py
Visualisation via visualise.py
Rapports
Compare les modes via compare
_
modes.py
Gestion complète des erreurs et données
2) Détail
●
validate_config.py
Le module orchestre une validation en 3 temps :
1. 2. 3. Vérification structurelle → Les 11 blocs essentiels, les 16 métriques et 8 critères sont-ils là
?
Validation locale → Chaque bloc respecte-t-il ses contraintes internes ?
Cohérence globale → Les blocs dialoguent-ils correctement entre eux ?
Les 11 Acteurs Principaux
Bloc Rôle Validation Clé
131
system Le chef d'orchestre N, T, dt, mode, perturbation
strates Les oscillateurs Matrices de couplage w[i][j], paramètres
A0, f0, phi
spiral Le nombre d'or phi ≈ 1.618 (±0.001)
regulation L'architecture de contrôle G_arch et ses paramètres spécifiques
latence Les délais adaptatifs gamma modes (static/dynamic)
enveloppe L'environnement stochastique mu_n, sigma_n
exploration Le détecteur de patterns Métriques, fenêtres, seuils fractals
to_calibrate Les cibles d'optimisation Seuils de stabilité, résilience, chaos
validation Les critères de succès Critères valides, batch_size
analysis Les sorties visuelles Booléens pour exports
dynamic_pa
rameters
L'évolution temporelle Flags pour phi, alpha, beta dynamiques
Les 16 métriques
METRIQUES_VALIDES = {
"t", "S(t)", "A_mean(t)", "f_mean(t)", "effort(t)", "cpu_step(t)",
"C(t)", "E(t)", "L(t)", "variance_d2S", "entropy_S", "effort_status",
"mean_abs_error", "mean_high_effort", "d_effort_dt", "t_retour"
}
132
Les 8 critères
CRITERES
_
VALIDES = {
"fluidity"
,
"stability"
"resilience"
,
,
"regulation"
,
"cpu
cost"
"effort
,
_
"innovation"
,
internal"
"effort
,
_
transient"
_
}
Le Flux d'Exécution
1. [ENTRÉE] → JSON ou Dict Python
2. [COLLECTEUR] → ValidationErrorCollector initialisé
3. [PHASE 1] → validate
main
_
_
└─ Vérifie présence des 11 blocs
blocks()
4. [PHASE 2] → Pour chaque bloc présent :
└─ Appel de validate
_[bloc]()
└─ Accumulation erreurs/warnings dans le collecteur
5. [PHASE 3] → validate
cross
_
_
checks()
└─ Cohérences inter-blocs
└─ Recommandations (N>10, T>1000)
6. [SORTIE] → (liste
erreurs, liste
_
_
warnings)
Types de vérification :
Validation de Bornes
Ex :
valeur > 0 (amplitudes, fréquences, k, omega...)
valeur ∈ ]0,1[ (entropy_
S, fractal
_
threshold...)
valeur ≥ 0 (t0, alpha, beta...)
Validation de Type & Enum
Le module vérifie l'appartenance à des ensembles fermés
133
-
-
-
Modes : static/dynamic, FPS/Kuramoto/neutral
Types : choc/rampe/sinus/bruit/none
Architectures : tanh/sinc/resonance/adaptive
Dépendances Conditionnelles
"Si A alors B doit exister"
-
-
-
Si mode="dynamic"
→ les paramètres dynamiques requis
Si type="sinus"
→ freq doit être défini
Si G
arch="resonance"
_
→ alpha et beta requis
Cohérence Dimensionnelle
Tout s'aligne sur N (nombre de strates)
-
-
-
len(strates) == N
Chaque w[i] a N éléments
Diagonale nulle dans les matrices de couplage
●
Setup environnement via utils.py
Le module utils prépare l'infrastructure avant la simulation
Rôle à ce stade : Architecte de l'environnement
Après validation de la configuration, main.py fait appel à 3 fonctions de utils.py pour préparer le
terrain :
Les 3 fonctions appelées
1. setup_
directories(base
_
dir)
Crée l'arborescence complète horodatée :
fps
_
output/
└── run
20250109
143022/
_
_
├── logs/ # Fichiers CSV de métriques
├── checkpoints/ # États sauvegardés
├── figures/ # Visualisations
├── reports/ # Rapports d'analyse
└── configs/ # Configurations utilisées
134
Retourne : Dict avec tous les chemins créés
2. generate
run
_
_
id(prefix="pipeline")
Génère : pipeline
20250109
143022
a7f3b2
_
_
_
└─prefix─┘└──timestamp──┘└hash┘
●
●
●
Timestamp pour l'ordre chronologique
Hash MD5 court (6 chars) pour l'unicité absolue
Retourne : String identifiant unique
3. log_
config_
and
_
meta(config, run
_
id, output
_
dir)
Sauvegarde deux fichiers essentiels :
config_{run
_
id}.json :
●
●
Configuration complète utilisée
Conversion automatique des types numpy via deep_
convert()
meta
_{run
_
id}.json :
json
{
"run
_
id": "pipeline
20250109
143022
_
_
"timestamp": "2025-01-09T14:30:22"
,
"fps
version": "1.3"
,
_
"system": {
"N": 5,
"T": 1000,
"mode": "FPS"
,
"seed": 42
a7f3b2"
,
_
}
}
Le flux d'appel dans main.py
python
# Après validation...
dirs = setup_
environment(config)
# Ce qui déclenche :
def setup_
environment(config):
dirs = utils.setup_
directories("fps
_pipeline
_
run
_
id = utils.generate
run
_
_
id("pipeline")
utils.log_
config_
and
_
meta(config, run
_
dirs['pipeline
run
_
_
id'] = run
id
_
output")
id, dirs['configs'])
135
return dirs
Points clés
✅ Traçabilité totale : Chaque run a son ID unique et sa config archivée
✅ Organisation claire : Structure de dossiers standardisée
✅ Horodatage précis : Timestamp dans les noms pour l'ordre chronologique
✅ Conversion automatique : deep_
convert() gère les types numpy→JSON
Note : utils.py contient beaucoup d'autres fonctions (parallélisation, archivage, recovery...) qui seront
utilisées plus tard dans le pipeline ou documentées séparément.
●
Exécution via simulate.py
Ce que fait execute
_
simulations() dans main.py :
1. Prépare la liste des modes : FPS + Kuramoto (si configuré) + neutral
2. Pour chaque mode :
○
Crée un fichier config temporaire avec deep_
convert() (pour cohérence
numpy→JSON)
○
Appelle simulate.run
_
simulation(temp_
config_path, mode)
○
Copie le log résultant dans le dossier pipeline
○
Gère les erreurs et warnings
Ce que fait simulate.run
_
simulation() directement :
Le chef d'orchestre qui dirige la dynamique FPS
Rôle : Orchestrateur de simulation
run
_
simulation(config_path, mode="FPS") est le point d'entrée qui :
1. Valide la configuration
2. Initialise le système
3. Branche vers le bon mode (FPS/Kuramoto/Neutral)
4. Gère le post-traitement
Phase 1 : Validation & Setup
Actions directes dans simulate.py :
136
# Charge et valide la config
config = init.load
_
config(config_path)
errors, warnings = validate
_
config.validate
_
config(config_path)
# Initialise la seed pour reproductibilité
np.random.seed(SEED)
random.seed(SEED)
Modules appelés :
●
●
●
init.load
_
config() → Charge le JSON
validate
_
config.validate
_
config() → Vérifie l'intégrité (déjà documenté)
utils.log_
seed() → Archive la seed dans seeds.txt
Phase 2 : Initialisation du système (init.py)
Modules appelés :
●
●
init.init
_
strates(config) → Crée l'état initial des N oscillateurs
○
Retourne un tableau de dictionnaires avec A0, f0, phi, alpha, beta, k, x0, w
init.setup_
logging(config, mode
_
suffix) → Prépare les fichiers CSV
○
Crée les writers CSV avec les bonnes colonnes
○
Retourne dict avec run
id, csv
_
_
writer, output
dir
_
Phase 3 : Branchement par mode (kuramoto.py)
Le routeur :
if mode == "kuramoto":
result = run
kuramoto
_
_
simulation(config, loggers)
elif mode == "neutral":
result = run
neutral
_
_
simulation(config, loggers)
elif mode == "fps":
result = run
_
fps
_
simulation(config, state, loggers)
Phase 4 : Boucle FPS (si mode="FPS")
Structure de run
_
fps
_
simulation() :
Setup initial :
●
●
●
Extraction des paramètres (T, dt, N)
Création des historiques (S
_
history, effort
_
Setup des fichiers individuels si N > 10
history, etc.)
Boucle temporelle principale :
Pour chaque pas de temps t :
137
1. INPUTS → perturbations.compute
_
In()
2. DYNAMIQUE → dynamics.compute
_
*()
3. RÉGULATION → regulation.compute
_
G()
4. MÉTRIQUES → metrics.compute
_
*()
5. LOGGING → CSV writer
6. BACKUP → utils.save
simulation
_
_
state()
Modules appelés dans la boucle :
perturbations :
●
compute
_
In(t, input
_
config, state, history, dt) → Input contextuel
dynamics (cœur de la FPS) :
●
●
●
●
●
●
compute
_
An() → Amplitudes avec enveloppe
compute
_
fn() → Fréquences avec modulation
compute
_phi
_
n() → Phases (spiral ou fixe)
compute
_gamma() → Latence globale
compute
_gamma
_
n() → Latence par strate
compute
_gamma
_
adaptive
_
aware() → Mode adaptatif : gamma apprend les meilleurs
régimes
- Explore différentes valeurs de gamma
- Maintient un journal des découvertes
- S'adapte selon l'historique du système
●
compute
G
_
_
adaptive
_
aware() → Régulation adaptative : teste les archétypes
- Alterne entre tanh, resonance, spiral
_
log, adaptive
- Mémorise les meilleures paires (gamma, G)
- Transitions douces entre archétypes
●
●
●
●
●
compute
compute
compute
compute
update
_
On() → Sortie observée
_
En() → Sortie attendue
_
S() → Signal global
_
C() → Cohérence
_
state() → Mise à jour de l'état
regulation :
●
compute
_
G(error
n, G
arch, G
_
_
_params) → Fonction de feedback
metrics :
●
●
compute
compute
_
effort() → Effort instantané
variance
_
_
d2S() → Fluidité
138
●
●
●
●
compute
compute
compute
compute
_
entropy_
S() → Innovation
mean
abs
_
_
_
error() → Régulation
t
_
_
retour() → Résilience
_
cpu
_
step() → Performance
utils (périodiquement) :
●
●
save
save
simulation
_
_
state() → Checkpoint tous les 100 pas
_
coupled
_
discoveries() → Sauvegarde des découvertes adaptatives
- Archive le journal gamma et l'état de régulation
- Divise en parties si > 15MB
- Crée un index pour navigation
●
handle
crash
_
_
recovery() → Si exception
spacing_
schedule :
●
build
_
spacing_
schedule() → Planification d'exploration
- Crée des "pics" d'exploration espacés exponentiellement
-Alterne entre exploration gamma et G
- Suit le principe de répétition espacée pour l'apprentissage
Phase 5 : Post-traitement
Modules appelés après la boucle :
●
●
●
●
analyze.analyze
criteria
and
_
_
_
refine() → Analyse des critères
explore.run
_
exploration() → Détection d'émergences
visualize.plot
metrics
_
_
dashboard() → Visualisation (si disponible)
utils.log_
end
of
_
_
run() → Finalisation
Retour vers main.py
simulate.py retourne un dictionnaire avec :
{
'history': [...], # Historique complet
'logs': 'path/to/csv'
, # Chemin du fichier log
'metrics': {...}, # Métriques finales
'cpu
_
steps': [...], # Performance
'S
_
history': [...], # Signal temporel
'run
_
id': 'fps
20250109...
_
' # Identifiant unique
}
139
Arbre des appels pour execute
_
simulations()
main.execute
_
simulations()
└── simulate.run
_
simulation()
├── init.load
_
config()
├── validate
_
config.validate
_
config()
├── utils.log_
seed()
├── init.init
_
strates()
├── init.setup_
logging()
├── run
kuramoto
_
_
simulation()
└── run
_
fps
_
simulation()
├── [SETUP] spacing_
schedule.build
_
spacing_
schedule() # Si enabled
├── [BOUCLE] perturbations.compute
_
In()
├── [BOUCLE] dynamics.compute
_
*()
├── [BOUCLE] regulation.compute
_
G()
├── [BOUCLE] metrics.compute
_
*()
├── [BOUCLE] utils.save
simulation
_
_
state()
●
Spacing effect des pics d’exploration et plateaux d’exploitation pour
synergies gamma et G via spacing_schedule.py (planification
d’exploration intelligente)
L'effet d'espacement appliqué à l'apprentissage adaptatif FPS
Rôle : Planificateur temporel d'exploration
Ce module applique le principe de répétition espacée (spacing effect) pour optimiser l'exploration
des paramètres gamma et G. Au lieu d'explorer constamment ou aléatoirement, il crée des "fenêtres
d'exploration" espacées exponentiellement.
Principe de base
L'effet d'espacement :
Temps: |--2--|----3----|--------4.5--------|...
Exploration: ↑γ ↑G ↑γ
Pic1 Pic2 Pic3
Les intervalles augmentent exponentiellement (×1.5 par défaut)
Fonction principale
build
_
spacing_
schedule(total
_
time, start
_
interval, growth, num
_
blocks, order)
140
Paramètres :
●
●
●
●
●
total
_
time : Durée totale de simulation (T)
start
_
interval : Premier intervalle (ex: 2.0)
growth : Facteur de croissance (ex: 1.5)
num
_
blocks : Nombre de fenêtres d'exploration
order : Alternance ['gamma'
,
'G'] ou ['G'
,
'gamma']
Retourne :
python
{
'gamma
_peaks': [2.0, 6.5, 14.75, ...], # Moments d'exploration gamma
'G
_
switches': [3.5, 9.25, 20.125, ...] # Moments de switch G
}
Intégration dans simulate.py (simplifiée, illustrative)
Python
# Optional spacing-effect schedule for exploration of gamma and G
spacing_
cfg = config.get('exploration'
, {}).get('spacing_
effect'
, {})
use
_
spacing = bool(spacing_
cfg.get('enabled'
, False)) and spacing_
schedule is not None
if use
_
spacing:
# Récupération des paramètres depuis config
schedule = spacing_
schedule.build
_
spacing_
schedule(
total
_
time=float(T),
start
_
interval=float(spacing_
cfg.get('start
interval'
_
, 2.0)),
growth=float(spacing_
cfg.get('growth'
, 1.5)),
num
_
blocks=int(spacing_
cfg.get('num
blocks'
_
, 8)),
order=spacing_
cfg.get('order'
, ['gamma'
,
'G'])
)
print(f"⏱ Spacing schedule: {schedule}")
# Conversion des temps en indices de steps pour la boucle
141
_
steps['gamma
_peaks'] = set(int(max(0, round(ti / dt))) for ti in schedule.get('gamma
_peaks'
,
schedule
[]))
schedule
_
steps['G
_
switches'] = set(int(max(0, round(ti / dt))) for ti in schedule.get('G
switches'
_
, []))
# -- DANS LA BOUCLE PRINCIPALE --
for step, t in enumerate(t
_
array):
# Détection des moments planifiés (avec fenêtre de tolérance ±1)
if use
_
spacing and step in schedule
_
steps['gamma
_peaks']:
spacing_planning_
hint
_
flag = 1
gamma
_
t += 0.25 # Pic d'exploration (impulse
_
magnitude par défaut)
spacing_gamma
bias
_
_
flag = 1
La vraie intelligence est dans dynamics.py
C'est compute
_gamma
_
adaptive
_
aware() et compute
G
_
_
adaptive
_
aware() qui utilisent ces temps
pour orchestrer l'exploration :
Dans simulate.py (setup) :
python
schedule = spacing_
schedule.build
_
spacing_
schedule(...)
schedule
_
steps['gamma
_peaks'] = set(int(max(0, round(ti / dt))) for ti in schedule.get('gamma
_peaks'
,
[]))
Dans simulate.py (boucle simplifiée) :
python
if step in schedule
_
steps['gamma
_peaks']:
# Signal à dynamics.py qu'il faut explorer
spacing_planning_
hint
_
flag = 1
gamma
_
t += impulse
_
magnitude # Pic d'exploration
Dans dynamics.py :
●
●
compute
_gamma
_
adaptive
_
aware() : Utilise le signal pour sortir des minima locaux
compute
G
_
_
adaptive
_
aware() : Relaxe les contraintes pour tester de nouveaux archétypes
142
Configuration actuelle (config.json)
json
"spacing_
effect": {
"enabled": true,
"start
_
interval": 2.0, # Premier gap
"growth": 1.5, # Croissance exponentielle
"num
_
blocks": 0, # 0 = illimité jusqu'à T
"order": ["gamma"
,
"G"
,
"gamma"
,
"G"] # Alternance
}
Avec T=100 et ces paramètres, génère environ :
●
●
Gamma peaks : t=2, 5, 9.5, 16.25, 26.87, 42.8, 66.2, 99.8
G switches : t=3.5, 7.25, 12.87, 21.8, 35.2, 55.3, 85.45
Synergie avec les modes adaptatifs
Le module spacing_
schedule est un déclencheur temporel. Son rôle est de dire "maintenant !" aux
fonctions adaptatives de dynamics.py :
1. 2. 3. Gamma adaptatif reçoit le signal → augmente temporairement gamma pour explorer
G adaptatif reçoit le signal → relaxe cool
down
_
_
time pour changer d'archétype
Les deux communiquent via journal['communication
_
signals']
Le génie est dans la simplicité : un générateur de temps minimaliste qui permet une exploration
sophistiquée quand couplé aux fonctions adaptatives.
Note : Le module spacing_
schedule.py fait 50 lignes. La vraie complexité est dans les 1000+ lignes
de compute
_gamma
_
adaptive
_
aware() et compute
G
_
_
adaptive
_
aware() dans dynamics.py.
2.1.b. Métriques
metrics.py : Le miroir empirique du système - 7 catégories, 25+ métriques
Rôle : Quantificateur multi-échelles
Ce module mesure TOUS les aspects du système FPS pour permettre la falsification et le raffinement
continu. Chaque métrique est un capteur spécifique de l'état du système.
1. MÉTRIQUES DE PERFORMANCE
compute
_
cpu
_
step(start, end, N)
143
cpu
_
step = (end
time - start
_
_
time) / N
●
●
Temps CPU normalisé par strate
Permet de détecter les goulots d'étranglement
2. MÉTRIQUES D'EFFORT
L'énergie interne du système
compute
_
effort(delta
_
An, delta
_
fn, delta
_gamma
_
n, An
_
ref, fn
_
ref, gamma
_
ref)
effort = Σ [|ΔA |/(|A |+ε) + |Δf |/(|f |+ε) + |Δγ |/(|γ |+ε)]
●
●
Mesure le changement RELATIF (évite l'explosion si ref→0)
Saturation à MAX
_
EFFORT=100 pour stabilité
compute
effort
_
_
status(effort
_
t, effort
_
history, config)
●
●
Returns : "stable" | "transitoire" | "chronique"
Logique adaptative :
○
Transitoire : pic > mean + 2σ
○
Chronique : effort élevé persistant (70% du temps récent)
○
Stable : dans la norme
compute
mean
_
_
high
_
effort(effort
_
history, percentile=80)
●
●
Moyenne des efforts au-dessus du percentile
Mesure l'effort chronique (tension persistante)
compute
d
effort
_
_
_
dt(effort
_
history, dt)
●
●
Dérivée temporelle de l'effort
Détecte les variations brusques (transitoires)
3. MÉTRIQUES DE QUALITÉ DYNAMIQUE
La fluidité et richesse du signal
compute
variance
_
_
d2S(S
_
history, dt)
●
●
●
Variance de d²S/dt² (accélération du signal)
Utilise IQR pour robustesse contre outliers
Faible variance = transitions douces
compute
_
fluidity(variance
_
d2S, reference
_
variance=175.0)
fluidity = 1 / (1 + exp(5*(variance/ref - 1)))
144
●
●
Sigmoïde inversée pour sensibilité optimale
Score 0→1 : 0=saccadé, 1=très fluide
compute
_
entropy_
S(S
_
t, sampling_
rate)
●
●
●
Entropie spectrale de Shannon
Haute entropie = riche en fréquences
Normalisation 0→1
compute
max
median
_
_
_
ratio(S
_
history)
ratio = max(|S|) / median(|S|)
●
●
Mesure la stabilité
Ratio élevé = pics extrêmes
4. MÉTRIQUES DE RÉGULATION
compute
mean
abs
_
_
_
error(En
_
array, On
_
array)
error = mean(|E (t) - O (t)|)
●
●
Qualité de convergence attendu/observé
Faible = bonne régulation
5. MÉTRIQUES DE RÉSILIENCE
Capacité de récupération
compute
t
_
_
retour(S
_
history, t
_
choc, dt, threshold=0.95)
●
Temps pour revenir à 95% de l'état pré-choc
●
Pour perturbations ponctuelles
compute
continuous
_
_
resilience(C
_
history, S
_
history, perturbation
_
active)
●
●
●
Pour perturbations continues (sinus, bruit)
Combine :
○
Stabilité de C(t) : cohérence maintenue
○
Stabilité de S(t) : signal expressif
Score 0→1 : capacité à maintenir l'harmonie
compute
_
adaptive
_
resilience(config, metrics, ...)
●
●
Sélecteur intelligent qui choisit automatiquement :
○
t
_
retour pour chocs ponctuels
○
continuous
_
resilience pour perturbations continues
Retourne un score unifié 1→5
145
6. MÉTRIQUES DE COHÉRENCE TEMPORELLE
La mémoire douce du système
compute
_
temporal
_
coherence(S
_
history, dt, tau
_
target=1.0)
●
●
●
Mesure la "mémoire" du système
Compare le temps de décorrélation à une cible optimale
Score 0→1 : 1=mémoire optimale (ni rigide ni chaotique)
compute
autocorr
_
_
tau(S
_
history, dt, method='one
over
_
_
e')
●
●
Calcule le temps de décorrélation τ
Méthodes :
○
'one
over
e' : croisement à 1/e
○
○
_
_
'first
_
zero' : premier passage à 0
'integrated' : intégrale de l'autocorrélation
compute
_
multiple
_
tau(signals
_
dict, dt)
●
●
●
Calcule τ pour plusieurs signaux simultanément
Révèle les échelles temporelles (surface vs profondeur)
Ex: τ
_
S=0.1s (surface fluide) vs τ
_gamma=2.4s (structure profonde)
7. ANALYSE GLOBALE ET SCORES
Intelligence adaptative pour évaluation multi-échelles
compute
_
adaptive
_
window(total
_
steps, target
_percentage, min
_
absolute, max
_percentage)
●
●
Calcule des fenêtres de taille optimale
Respecte : % cible, minimum statistique, maximum d'inertie
calculate
all
_
_
scores(recent
_
history, config)
●
●
●
Système complet d'évaluation avec fenêtres adaptatives
Utilise 3 fenêtres temporelles :
○
Immédiate (2%, min 10 pas) : réactivité
○
Récente (10%, min 30 pas) : tendance
○
Moyenne (40%, min 100 pas) : stabilité
Pondération adaptative selon maturité :
○
Début (< 20%) : 70% immédiat, 30% récent
○
Mi-parcours : équilibré
○
Mature (> 50%) : inclut fenêtre globale
compute
_
scores(history_
slice)
●
Calcule 7 scores normalisés (1→5) :
1. 2. Stabilité : std(S) et variations C(t)
Régulation : erreur moyenne E-O
146
3. Fluidité : directement depuis métrique
4. Résilience : adaptive
_
resilience prioritaire
5. Innovation : entropie spectrale
6. Coût CPU : performance computationnelle
7. Effort : tension interne moyenne
FONCTIONS SPÉCIALISÉES
detect
chaos
_
_
events(S
_
history, threshold
_
sigma=3.0)
●
●
Détecte déviations > 3σ
Retourne liste d'événements avec z-score
compute
correlation
effort
_
_
_
cpu(effort
_
history, cpu
_
history)
●
●
Corrélation de Pearson effort↔CPU
Détecte si tension interne → charge computationnelle
check
●
●
_
thresholds(metrics
_
dict, thresholds
_
dict)
Vérifie franchissement des seuils configurés
Retourne dict métrique→dépassement True/False
SEUILS CRITIQUES (config.json)
Json :
"to
_
calibrate": {
"variance
d2S": 0.01,
_
"fluidity_
threshold": 0.3,
"stability_
ratio": 10,
"resilience": 2,
"entropy_
S": 0.5,
"mean
_
high
effort": 2,
_
"d
effort
dt": 5,
_
_
"t
retour": 1,
_
"effort
transitoire
_
_
"effort
_
chronique
_
"cpu
_
step_
ctrl": 2,
threshold": 2.0,
threshold": 1.5,
},
PHILOSOPHIE DES MÉTRIQUES
Les métriques FPS forment un système hiérarchique :
1. 2. 3. Niveau instantané : effort, CPU, erreur
Niveau local : fluidité, entropie, cohérence
Niveau global : résilience, scores adaptatifs
147
4. Niveau méta : corrélations, tendances
Chaque niveau révèle un aspect différent de l'harmonie systémique, permettant une compréhension
multi-échelles de la dynamique FPS.
Note : Les métriques sont conçues pour être robustes (protection NaN/Inf, saturation, fallbacks) et
adaptatives (fenêtres variables, seuils dynamiques).
2.1.c. Scores empiriques
-
Implémentation concrète
Python :
def calculate
int]:
"""
_
empirical
_
scores(metrics: Dict, config: Dict = None, history: List[Dict] = None) -> Dict[str,
Calcule les scores 1-5 pour la grille empirique en utilisant les fenêtres adaptatives.
Args:
metrics: métriques calculées (pour fallback)
config: configuration (contient adaptive
_
windows)
history: historique complet pour fenêtres adaptatives
Returns:
dict avec scores 1-5 pour chaque critère
"""
# Si on a l'historique, utiliser le nouveau système adaptatif
if history and len(history) >= 20:
try:
# Utiliser calculate
all
_
_
scores avec fenêtres adaptatives
import metrics as metrics
module
_
adaptive
scores = metrics
module.calculate
all
_
_
_
_
scores(history, config)
current
_
scores = adaptive
_
scores.get('current'
, {})
if current
scores:
_
# Mapper vers la grille empirique
return {
'Stabilité': int(round(current
_
scores.get('stability'
, 3))),
'Régulation': int(round(current
_
scores.get('regulation'
, 3))),
'Fluidité': int(round(current
_
scores.get('fluidity'
, 3))),
148
'Résilience': int(round(current
_
scores.get('resilience'
, 3))),
'Innovation': int(round(current
_
scores.get('innovation'
, 3))),
'Coût CPU': int(round(current
_
scores.get('cpu
cost'
_
, 3))),
'Effort interne': int(round(current
_
scores.get('effort'
, 3)))
}
except Exception as e:
print(f" ⚠ Erreur système adaptatif, fallback : {e}")
# Sinon, fallback sur l'ancien système (garde la compatibilité)
scores = {}
# Stabilité (basée sur std
S et max
median
_
_
_
std
_
s = metrics.get('std
S'
_
, float('inf'))
if std
s < 0.5:
_
scores['Stabilité'] = 5
elif std
s < 1.0:
_
scores['Stabilité'] = 4
elif std
s < 2.0:
_
scores['Stabilité'] = 3
else:
ratio)
scores['Stabilité'] = 2
# Régulation (basée sur final
_
error = metrics.get('final
mean
mean
abs
abs
_
_
error)
error'
_
_
_
, float('inf'))
if error < 0.1:
scores['Régulation'] = 5
elif error < 0.5:
scores['Régulation'] = 4
elif error < 1.0:
scores['Régulation'] = 3
else:
scores['Régulation'] = 2
# Fluidité (basée sur la nouvelle métrique de fluidité)
fluidity = metrics.get('final
_
fluidity'
, None)
if fluidity is None:
# Fallback : calculer depuis variance
_
d2S si fluidity n'est pas disponible
var
_
d2s = metrics.get('final
variance
d2S'
_
_
, 175.0)
x = var
d2s / 175.0 # Reference variance
_
fluidity = 1 / (1 + np.exp(5.0 * (x - 1)))
if fluidity >= 0.9:
scores['Fluidité'] = 5
elif fluidity >= 0.7:
scores['Fluidité'] = 4
elif fluidity >= 0.5:
scores['Fluidité'] = 3
elif fluidity >= 0.3:
scores['Fluidité'] = 2
else:
149
scores['Fluidité'] = 1
# Résilience - Utilise métrique adaptative unifiée
adaptive
resilience
_
_
score = metrics.get('adaptive
resilience
score'
_
_
, None)
if adaptive
resilience
score is not None:
_
_
scores['Résilience'] = adaptive
resilience
_
_
else:
# Fallback : ancienne logique
has
continuous
_
_perturbation = False
score
# Nouvelle structure avec input.perturbations
input
_
cfg = config.get('system'
, {}).get('input'
, {}) if config else {}
perturbations = input
_
cfg.get('perturbations'
, [])
for pert in perturbations:
if pert.get('type') in ['sinus'
'bruit'
,
,
'rampe']:
has
continuous
_
_perturbation = True
break
# Si pas trouvé dans la nouvelle structure, vérifier l'ancienne (pour compatibilité)
if not has
continuous
_
_perturbation and config:
old
_pert = config.get('system'
, {}).get('perturbation'
, {})
if old
_pert.get('type') in ['sinus'
'bruit'
,
,
'rampe']:
has
continuous
_
_perturbation = True
if has
continuous
_
_perturbation:
cont
_
resilience = metrics.get('continuous
resilience
mean'
,
_
_
metrics.get('continuous
resilience'
_
, 0))
if cont
resilience >= 0.90:
_
scores['Résilience'] = 5
elif cont
resilience >= 0.75:
_
scores['Résilience'] = 4
elif cont
resilience >= 0.60:
_
scores['Résilience'] = 3
elif cont
resilience >= 0.40:
_
scores['Résilience'] = 2
else:
scores['Résilience'] = 1
else:
t
_
retour = metrics.get('resilience
t
retour'
_
_
, float('inf'))
if t
retour < 1.0:
_
scores['Résilience'] = 5
elif t
retour < 2.0:
_
scores['Résilience'] = 4
elif t
retour < 5.0:
_
scores['Résilience'] = 3
elif t
retour < 10.0:
_
scores['Résilience'] = 2
else:
150
scores['Résilience'] = 1
# Innovation (basée sur entropy_
S moyen pour cohérence avec système adaptatif)
# CORRECTION: utiliser la moyenne plutôt que la valeur finale pour éviter les artefacts
entropy = metrics.get('entropy_
S'
, metrics.get('final
_
entropy_
S'
, 0)) # Priorité à la moyenne
if entropy > 0.8:
scores['Innovation'] = 5
elif entropy > 0.6:
scores['Innovation'] = 4
elif entropy > 0.4:
scores['Innovation'] = 3
else:
scores['Innovation'] = 2
# Coût CPU (basé sur mean
_
cpu
_
step)
cpu = metrics.get('mean
_
cpu
_
step'
, float('inf'))
if cpu < 0.001:
scores['Coût CPU'] = 5
elif cpu < 0.01:
scores['Coût CPU'] = 4
elif cpu < 0.1:
scores['Coût CPU'] = 3
else:
scores['Coût CPU'] = 2
# Effort interne (basé sur mean
_
effort)
effort = metrics.get('mean
effort'
_
, float('inf'))
if effort < 0.5:
scores['Effort interne'] = 5
elif effort < 1.0:
scores['Effort interne'] = 4
elif effort < 2.0:
scores['Effort interne'] = 3
else:
scores['Effort interne'] = 2
return scores
2.1.d. Groupes contrôles
Architecture et mécanismes de comparaison dans le pipeline FPS
TROIS MODES DE SIMULATION
Sélection du mode
python
151
# simulate.py - run
_
simulation()
if mode.lower() == "kuramoto":
result = run
kuramoto
_
_
simulation(config, loggers)
elif mode.lower() == "neutral":
result = run
neutral
_
_
simulation(config, loggers)
elif mode.lower() == "fps":
result = run
_
fps
_
simulation(config, state, loggers, strict)
Mode FPS
●
●
●
Fonction : run
_
fps
_
simulation()
Calculs : An(t), fn(t), φn(t), γn(t), En(t), On(t), Fn(t), G(x), S(t)
Adaptation : Paramètres dynamiques selon config
Mode Kuramoto
●
●
●
Module : kuramoto.py
Équation : dφᵢ/dt = ωᵢ + (K/N) · Σⱼ sin(φⱼ - φᵢ)
Paramètres : K=0.5, ωᵢ~U[0,1], phases initiales aléatoires
Calculs :
python
def kuramoto
_
step(phases, frequencies, K, N, dt):
dphases
_
dt = frequencies.copy()
for i in range(N):
coupling_
sum = 0.0
for j in range(N):
if i != j:
coupling_
sum += np.sin(phases[j] - phases[i])
dphases
_
dt[i] += (K / N) * coupling_
sum
return dphases
dt
_
Mode Neutral
●
●
●
Fonction : run
neutral
_
_
simulation()
Signal : S(t) = Σ amplitudes * sin(2π * frequencies * t + phases)
Paramètres fixes : amplitudes=1, frequencies linéaires [0.8, 1.2], phases=0
MÉTRIQUES COMMUNES
Tous les modes enregistrent dans le CSV :
python
metrics
't': t,
_
dict = {
152
'S(t)': S
t,
_
'C(t)': C
t,
_
'E(t)': E
t, # order
_
_param pour Kuramoto
'L(t)': L
t,
_
'cpu
_
step(t)': cpu
_
step,
'effort(t)': effort
_
t, # 0 pour Kuramoto/Neutral
'A
_
mean(t)': A
mean
t,
_
_
'f
_
mean(t)': f
mean
t,
_
_
'variance
d2S': variance
d2S,
'entropy_
'mean
_
abs
S': entropy_
S,
error': mean
_
_
'continuous
abs
error,
_
_
_
resilience': continuous
_
_
resilience
}
ORCHESTRATION (main.py)
Exécution séquentielle
python
def execute
_
simulations(config_path, config, dirs):
results = {}
modes = ['FPS']
if config.get('analysis'
, {}).get('compare
kuramoto'
_
, True):
modes.append('Kuramoto')
modes.append('neutral')
for mode in modes:
# Créer config temporaire avec deep_
convert
temp_
config_path = os.path.join(dirs['configs'],
f'config_{mode.lower()}_
main.json')
with open(temp_
config_path,
'w') as f:
json.dump(deep_
convert(config), f, indent=2)
# Lancer simulation
result = FPS
_
MODULES['simulate'].run
_
simulation(temp_
config_path, mode)
results[mode.lower()] = result
Structure des résultats
python
result = {
'logs': log_
file
_path,
'metrics': {
'mean
S': float,
_
153
'std
S': float,
_
'mean
C': float,
_
'mean
_
cpu
_
step': float,
'entropy_
S': float,
'continuous
resilience': float,
_
'mode': str
},
'history': list,
'run
id': str,
_
'S
_
history': array,
'C
_
history': array
}
MODULE COMPARE
_
MODES.PY
Fonction principale
python
def calculate
_
efficiency_
metrics(fps
result, kuramoto
result, neutral
_
_
_
metrics = {}
result):
# Pour chaque métrique, calcul des valeurs et efficiences
for metric in ['synchronization'
,
'stability'
'resilience'
,
,
'innovation'
,
'fluidity'
,
'cpu
_
efficiency']:
metrics[metric] = {
'fps
value': extract
_
_
value(fps
_
result, metric),
'kuramoto
value': extract
_
_
value(kuramoto
_
result, metric),
'neutral
value': extract
_
_
value(neutral
_
result, metric),
'fps
vs
kuramoto
_
_
_
efficiency': percentage
_
diff(),
'fps
vs
neutral
_
_
_
efficiency': percentage
_
diff()
}
Métriques calculées
Synchronisation
python
fps
_
sync = fps
_
result.get('metrics'
, {}).get('mean
C'
_
, 0)
kura
_
sync = kuramoto
_
result.get('metrics'
, {}).get('mean
C'
_
, 0)
fps
vs
kura
_
_
_
eff = (fps
_
sync - kura
_
sync) / (abs(kura
_
sync) + 1e-10) * 100
Stabilité
python
154
fps
_
stability = 1.0 / (fps
std
_
_
S + 1e-3)
kura
_
stability = 1.0 / (kura
std
_
_
S + 1e-3)
Résilience
python
# Si résilience adaptative disponible
fps
_
adaptive
_
resil = fps
_
result.get('metrics'
, {}).get('adaptive
resilience'
_
, None)
if fps
_
adaptive
resil is not None:
_
fps
_
resilience = fps
_
adaptive
resil
_
else:
# Sinon utiliser t
_
retour
fps
_
resilience = 1.0 / (fps
t
_
_
retour + 1.0)
Fluidité
python
# Si métrique fluidity non disponible, calculer depuis variance
d2S
_
if fps
fluid is None:
_
fps
_
var = fps
_
result.get('metrics'
, {}).get('final
variance
d2S'
_
_
, 175.0)
x = fps
var / 175.0
_
fps
_
fluid = 1 / (1 + np.exp(5.0 * (x - 1)))
Score global normalisé
python
def normalize
_
metric(value, min
val=0, max
_
_
val=1):
return max(0, min(1, (value - min
_
val) / (max
val - min
_
_
val)))
# Normaliser chaque métrique entre les 3 modes
sync
_
values = [metrics['synchronization'][mode]
for mode in ['fps
value'
'kuramoto
value'
,
,
_
_
sync
_
min, sync
_
max = min(sync
_
values), max(sync
_
'neutral
_
values)
value']]
fps
_
score = np.mean([
normalize
_
metric(metrics['synchronization']['fps
_
value'], sync
_
min, sync
_
normalize
_
metric(metrics['stability']['fps
_
value'], stab
min, stab
_
_
max),
# ... autres métriques
max),
])
Gestion des cas limites
python
155
# Détection de ratios suspects
if abs(fps
_
sync) > 0 and abs(kura
_
sync) > 0:
ratio = abs(fps
_
sync / kura
_
sync)
if ratio < 0.2: # FPS 5x plus faible
# Formule adaptative pour éviter valeurs extrêmes
fps
vs
kura
_
_
_
eff = -50 - (1 - ratio) * 40
EXPORT DES COMPARAISONS
Génération du rapport
python
def export
_
comparison
_
report(fps
result, kuramoto
result, neutral
_
_
_
result, output
_path):
report = {
'metadata': {
'timestamp': datetime.now().isoformat(),
'fps
run
_
_
id': fps
_
result.get('run
id'
,
_
'unknown'),
'kuramoto
run
id': kuramoto
result.get('run
id'
,
'unknown'),
'neutral
_
run
_
_
_
id': neutral
_
_
_
result.get('run
id'
,
_
'unknown')
},
'detailed
metrics': metrics,
_
'summary': {
'fps
_
advantages': [],
'fps
_
disadvantages': [],
'overall
verdict': ''
_
}
}
# Export JSON
with open(output
_path,
'w') as f:
json.dump(deep_
convert(report), f, indent=2)
# Export TXT
txt
_path = output
_path.replace('
.json'
,
# ... génération du rapport texte
'
.txt')
Structure du rapport
comparison
_
fps
vs
_
_
├── metadata
│ ├── timestamp
│ └── run
ids
_
├── detailed
metrics
_
│ ├── synchronization
controls.json
156
│ ├── stability
│ ├── resilience
│ ├── innovation
│ ├── fluidity
│ └── cpu
_
efficiency
└── summary
├── fps
_
advantages[]
├── fps
_
disadvantages[]
└── overall
verdict
_
SPÉCIFICITÉS KURAMOTO
Paramètre d'ordre
python
def compute
kuramoto
_
_
order(phases):
complex
_
sum = np.mean(np.exp(1j * phases))
r = np.abs(complex
_
sum) # Amplitude [0,1]
psi = np.angle(complex
_
sum) # Phase moyenne
return r, psi
Métriques additionnelles
python
# Calculs post-simulation pour Kuramoto
if len(S
_
history) >= 3:
dS
_
dt = np.gradient(S
_
history, dt)
d2S
_
dt2 = np.gradient(dS
_
dt, dt)
variance
_
d2S = np.var(d2S
_
dt2)
if len(S
_
history) >= 10:
freqs, psd = scipy.signal.periodogram(S
_
psd
_
norm = psd / np.sum(psd) + 1e-15
history, 1/dt)
entropy_
S = -np.sum(psd
_
norm * np.log(psd
_
norm)) / np.log(len(psd
_
norm))
WORKFLOW D'ANALYSE BATCH
Configuration batch
python
# config.json
"validation": {
"batch
size": 5,
_
"compare
_
kuramoto": true
157
}
Exécution batch avec comparaison
python
# main.py - run
batch
_
_
analysis()
for i in range(batch
_
size):
config_
copy = json.loads(json.dumps(config))
config_
copy['system']['seed'] = config['system']['seed'] + i
temp_
config_path = os.path.join(dirs['configs'], f'config_
batch
_{i}.json')
with open(temp_
config_path,
'w') as f:
json.dump(deep_
convert(config_
copy), f, indent=2)
result = FPS
_
MODULES['simulate'].run
_
simulation(temp_
config_path,
'FPS')
batch
_
logs.append(result['logs'])
Génération rapport comparatif
python
if all(mode in results for mode in ['fps'
,
import compare
modes
_
'kuramoto'
,
'neutral']):
comparison
_path = os.path.join(dirs['reports'],
'comparison
_
fps
vs
_
_
controls.json')
comparison
_
report = compare
_
modes.export
_
comparison
_
results['fps'],
results['kuramoto'],
results['neutral'],
comparison
_path
report(
)
STRUCTURE DES FICHIERS GÉNÉRÉS
fps
_pipeline
_
output/
├── logs/
│ ├── run
FPS
*
.csv
_
_
│ ├── run
KURAMOTO
*
.csv
_
_
│ └── run
NEUTRAL
*
.csv
_
_
├── configs/
│ ├── config_
fps
_
main.json
│ ├── config_
kuramoto
_
main.json
│ └── config_
neutral
_
main.json
└── reports/
├── comparison
_
fps
vs
_
_
controls.json
158
└── comparison
_
fps
vs
_
_
controls.txt
2.1.e. Exploration des émergences via explore.py
Le révélateur de l'invisible dans la dynamique FPS
ORCHESTRATION DE L'EXPLORATION
Fonction principale
python
def run
_
exploration(run
data
_
_path: str, output
_
dir: str,
config: Optional[Dict] = None) -> Dict[str, Any]
Workflow :
1. Charge les données (CSV ou HDF5)
2. Diversifie les paramètres selon la seed
3. Lance tous les détecteurs
4. Agrège les événements
5. Génère rapport et logs
Diversification par seed
python
# Extraction de la seed du nom de fichier
seed
from
filename = extract
seed
from
_
_
_
_
_
filename(run
data
_
_path)
if seed
from
filename:
_
_
# Modifier les seuils pour chaque seed
seed
_
factor = (seed
from
_
_
filename % 1000) / 1000.0 # [0.0, 0.999]
anomaly_
threshold = config_
threshold * (0.8 + 0.4 * seed
_
factor)
fractal
_
threshold = config_
threshold * (0.7 + 0.3 * seed
_
factor)
Objectif : Éviter explorations identiques sur runs similaires
DÉTECTEURS D'ÉMERGENCE
Détection d'anomalies
python
159
def detect
_
anomalies(data, metrics, threshold=3.0, min
_
duration=3)
Méthode : Fenêtre glissante avec z-score
●
●
Calcul : z
_
score = |value - mean
_
window| / std
_
Anomalie si : z
_
score > threshold pendant min
_
window
duration pas
Événement généré :
python
{
'event
type': 'anomaly'
,
't
_
start': i,
_
't
end': i + duration - 1,
_
'metric': metric,
'value': max
z
score,
_
_
'severity': 'low'|'medium'|'high'
}
Détection de bifurcations spiralées
python
def detect
_
spiral
_
bifurcations(data, phase
_
metric='C(t)'
, threshold=np.pi)
Deux types détectés :
1. 2. Événements :
Changement de signe de dérivée : phase
_
derivative[i-1] * phase
_
Saut de phase : |phase
_
diff| > threshold
derivative[i+1] < 0
●
●
'bifurcation' : Changement graduel
'phase
_jump' : Saut brutal
Émergences harmoniques
python
def detect
harmonic
_
_
emergence(data, signal
_
metric='S(t)'
,
n
_
harmonics=5, window=100, step=10)
Méthode : FFT glissante
1. FFT sur fenêtre de taille window
160
2. Extraction des n
_
harmonics pics principaux
3. Comparaison avec fenêtre précédente
4. Détection nouvelles harmoniques
Événement :
python
{
'event
_
type': 'harmonic
_
emergence'
,
't
start': i,
_
't
end': i + window,
_
'value': nombre
nouvelles
_
_
harmoniques
}
Exploration espace de phase
python
def explore
_phase
_
space(data, metric='S(t)'
, window=50,
min
_
diagonal
_
length=5)
Méthode : Recurrence plot avec embedding de Takens
python
# Embedding 3D avec delay=5
for i in range(len(values) - (embedding_
dim-1)*delay):
point = [values[i + j*delay] for j in range(embedding_
dim)]
embedded.append(point)
# Matrice de récurrence
distances = spatial.distance
_
matrix(embedded, embedded)
threshold = np.percentile(distances.flatten(), 10)
recurrence
matrix = distances < threshold
_
Détection : Diagonales dans la matrice = cycles attracteurs
Motifs fractals
python
def detect
_
fractal
_patterns(data, metrics, window
_
sizes=[1, 10, 100],
threshold=0.8)
Deux analyses :
161
Auto-similarité multi-échelles
python
# Comparer motifs à différentes échelles
small
_pattern = values[t:t+small
_
window]
large
_pattern = values[t:t+large
_
window]
downsampled = signal.resample(large
_pattern, len(small
_pattern))
correlation = np.corrcoef(small
_pattern, downsampled)[0, 1]
Dimension fractale (box-counting)
python
def estimate
fractal
_
_
dimension(data, max
box
_
_
size=100):
# Compter boîtes occupées à différentes échelles
# Régression log-log : slope = -dimension
_
fractale
Événements :
●
●
'fractal
_pattern' : Corrélation multi-échelles > threshold
'fractal
_
dimension' : Dimension ∈ [1.2, 1.8]
STRUCTURE DES DONNÉES
Chargement flexible
python
def load
run
_
_
data(data
_path: str) -> Dict[str, np.ndarray]:
if data
_path.endswith('
.csv'):
return load
csv
_
_
data(data
_path)
elif data
_path.endswith(('
.h5'
'
,
.hdf5')):
return load
hdf5
_
_
data(data
_path) # Si h5py disponible
Format événement standard
python
event = {
'event
_
type': str, # Type d'émergence
't
_
start': int, # Début temporel
't
_
end': int, # Fin temporelle
'metric': str, # Métrique concernée
'value': float, # Valeur/amplitude
'severity': str, # 'low'|'medium'|'high'
# Champs optionnels selon type
162
'scale': str, # Pour fractals
'n
_
harmonics': int # Pour harmoniques
}
EXPORTS ET RAPPORTS
Fichiers générés
output
dir/
_
├── emergence
events
_
_{run
_
id}.csv # Tous les événements
├── fractal
events
_
_{run
_
id}.csv # Événements fractals seuls
└── exploration
_
report
_{run
_
id}.md # Rapport détaillé
Structure CSV événements
csv
event
_
type,t
start,t
_
_
end,metric,value,severity
anomaly,200,219,S(t),5.234567,high
bifurcation,400,402,C(t),3.141593,medium
fractal
_pattern,100,200,effort(t),0.892345,medium
Rapport Markdown
markdown
# Rapport d'exploration FPS
**Run ID :** run
20250622
_
_
**Total événements :** 42
seed12345
## Résumé par type d'événement
-
**anomaly** : 8 événements
-
**bifurcation** : 5 événements
-
**fractal
_pattern** : 12 événements
...
## [Type] (top 5 par valeur)
### 1. t=200-219
-
**Métrique :** S(t)
-
**Valeur :** 5.2346
-
**Sévérité :** high
CONFIGURATION
163
Paramètres exploration (config.json)
json
"exploration": {
"metrics": ["S(t)"
,
"C(t)"
,
"effort(t)"],
"window
_
sizes": [1, 10, 100],
"fractal
threshold": 0.8,
_
"detect
fractal
_
_patterns": true,
"detect
anomalies": true,
_
"detect
harmonics": true,
_
"anomaly_
threshold": 3.0,
"min
duration": 3,
_
"recurrence
_
window": [1, 10, 100]
}
Activation sélective
●
●
●
detect
detect
detect
anomalies : Active détection anomalies
_
_
harmonics : Active FFT harmoniques
fractal
_
_patterns : Active analyse fractale
ALGORITHMES CLÉS
Classification sévérité
python
def classify_
severity(value: float, threshold: float) -> str:
if value < threshold * 1.5:
return 'low'
elif value < threshold * 3:
return 'medium'
else:
return 'high'
Diversification exploration
python
def add
_
exploration
_
diversity(data, seed):
# Ajoute bruit < 0.01% pour différencier patterns similaires
noise
_
amplitude = np.std(values) * 0.0001
noise = np.random.normal(0, noise
_
amplitude, len(values))
diversified
_
data[key] = values + noise
But : Éviter détections identiques sur runs proches
164
STATISTIQUES ET AGRÉGATION
Comptage par type
python
def count
events
_
_
by_
type(events) -> Dict[str, int]:
counts = defaultdict(int)
for event in events:
counts[event['event
_
type']] += 1
return dict(counts)
Résultat final
python
results = {
'status': 'success'
,
'run
id': str,
_
'seed
used': int,
_
'total
events': int,
_
'events
_
by_
type': Dict[str, int],
'events': List[Dict],
'diversified
_params': {
'anomaly_
threshold': float,
'fractal
threshold': float,
_
'phase
threshold': float
_
},
'paths': {
'events': str,
'report': str,
'fractal
events': str
_
}
}
EXTENSIBILITÉ
Ajout nouveau détecteur
python
def detect
custom
_
_pattern(data,
events = []
# Logique de détection
events.append({
'event
_
type': 'custom
_pattern'
,
't
start': t,
**params) -> List[Dict]:
_
165
't
end': t + duration,
_
'metric': metric,
'value': amplitude,
'severity': severity
})
return events
# Dans run
_
exploration()
if config.get('detect
custom'
_
, False):
custom
events = detect
custom
_
_
_pattern(data,
all
_
events.extend(custom
_
events)
**params)
Métriques personnalisées
Toute colonne du CSV peut être analysée en l'ajoutant dans :
json
"metrics": ["S(t)"
,
"C(t)"
,
"nouvelle
_
metrique"]
SEUILS ET INTERPRÉTATION
Seuils par défaut
●
●
●
●
●
Anomalie : z-score > 3σ pendant 3+ pas
Bifurcation : changement phase > π
Fractal : corrélation multi-échelles > 0.8
Harmonique : pic FFT > 10% du max
Récurrence : distance < 10e percentile
Diversification automatique
●
●
●
Seed 12345 → seuils × [0.8, 1.2]
Seed 12346 → seuils × [0.85, 1.15]
Évite faux positifs systématiques
OPTIMISATIONS
Gestion mémoire
●
●
●
Fenêtres glissantes limitées
Downsampling pour grandes échelles
HDF5 pour N > 10 ou T > 10000
Performance
●
●
FFT uniquement si len(data) >= window
Skip métrique si std < 1e-10
166
●
Parallélisation possible par métrique
Note : Ce module reste ouvert et extensible - chaque contributeur peut ajouter ses propres détecteurs
d'émergence.
2.1.f. Analyse via analyze.py
Une boucle d'apprentissage et de fine-tunning (l’impact du fine-tunning reste léger)
PHILOSOPHIE DU MODULE
Principe central
Analyse par batch → Détection seuils → Raffinement → Traçabilité
Le système analyse typiquement 5 runs, détecte les dépassements de seuils, et raffine
automatiquement les paramètres si >50% des runs sont problématiques.
Plasticité méthodologique
Conformément à la feuille de route FPS V1.3 :
●
●
●
Raffinements itératifs et réversibles
Traçabilité complète via changelog
Apprentissage continu
ANALYSE DE BATCH
Fonction principale
python
def analyze
criteria
and
_
_
_
refine(logs
_
batch: List[str], config: Dict)
Workflow :
1. Charge logs CSV de chaque run
2. Calcule statistiques par critère
3. Vérifie franchissements de seuils
4. Déclenche raffinements si >50% dépassent
5. Met à jour config
6. Log les changements
167
Structure de retour
python
{
'refinements': [
{
'criterion': str,
'changes': dict,
'stats': dict,
'timestamp': ISO-8601
}
],
'statistics': dict,
'updated
_
config': dict
}
CRITÈRES ET MÉTRIQUES
Mapping critères → métriques
python
criteria
_
metrics = {
'fluidity': {
'metric': 'fluidity'
,
'threshold
_
key': 'fluidity_
threshold'
,
'condition': lambda x, t: x < t, # Inversé
'threshold
_percent': 0.7 # 70% du run
},
'stability': {
'metric': 'max
median
ratio'
,
_
_
'threshold
_
key': 'stability_
ratio'
,
'condition': lambda x, t: x > t,
'threshold
_percent': 0.05
},
'resilience': {
'metric': 't
retour'
,
_
'threshold
_
key': 'resilience'
,
'condition': lambda x, t: x > t,
'threshold
_percent': None # Valeur unique
},
'innovation': {
'metric': 'entropy_
S'
,
'threshold
_
key': 'entropy_
S'
,
'condition': lambda x, t: x < t,
'threshold
_percent': 0.7
},
168
'regulation': {
'metric': 'mean
abs
error'
,
_
_
'threshold
_
key': 'mean
_
high
effort'
,
_
'condition': lambda x, t: x > 2 * t,
'threshold
_percent': 0.5
},
'cpu
_
cost': {
'metric': 'cpu
_
step(t)'
,
'threshold
_
key': 'cpu
_
step_
ctrl'
,
'condition': lambda x, t: x > t,
'threshold
_percent': 0.8
}
}
Critères d'effort spéciaux
python
def analyze
effort
_
_
criteria(batch
_
data, config)
Effort chronique :
●
Détecté si mean
_
high
effort > 2×médiane sur >80% du run
_
Effort transitoire :
●
Détecté si >10 pics dépassent 5σ dans d
effort
_
_
dt
RÈGLE DE DÉCLENCHEMENT
python
def determine
_
refinements(criteria
_
stats, config)
Règle 50% : Un raffinement est déclenché si le critère dépasse son seuil sur >50% des runs du
batch.
Exemple : Si 3 runs sur 5 ont une fluidity < threshold pendant 70% du temps, le raffinement fluidity est
déclenché.
FONCTIONS DE RAFFINEMENT
Raffinement Fluidité
python
169
def refine
_
fluidity(config, stats)
Actions :
●
●
●
gamma
n
_
_
mode: static → dynamic
env
_
mode: static → dynamic
sigma
_
n: ×1.3 (max 0.3)
Objectif : Adoucir les transitions
Raffinement Stabilité
python
def refine
_
stability(config, stats)
Actions :
●
●
k (sensibilité): ×0.8 (min 1.0)
alpha (souplesse): ×0.7 (min 0.1)
Objectif : Réduire oscillations
Raffinement Résilience
python
def refine
_
resilience(config, stats)
Actions :
●
●
alpha: ×1.3 (max 1.0) - Plus réactif
beta: ×1.2 (max 2.0) - Feedback plus fort
Objectif : Récupération plus rapide
Raffinement Innovation
python
def refine
_
innovation(config, stats)
Actions :
●
●
epsilon (variation spirale): ×1.5 (max 0.15)
dynamic
_
G: False → True
170
Note : θ(t) et η(t) seront raffinés en phase 2
Raffinement Régulation
python
def refine
_
regulation(config, stats)
Actions :
●
●
beta: ×1.3 (max 2.5)
G
_
arch: tanh → resonance (si erreur moyenne > 1.0)
Raffinement CPU
python
def refine
_
cpu(config, stats)
Actions :
●
Réduire log_
metrics aux essentielles : ['t'
,
'S(t)'
,
'C(t)'
,
'effort(t)'
,
'cpu
_
step(t)'
,
'entropy_
S']
Raffinement Effort Chronique
python
def refine
chronic
_
_
effort(config, stats)
Actions :
●
●
alpha: ×0.6 (min 0.1) - Moins d'adaptation constante
sigma
_
n: ×1.3 (max 0.3) - Transitions plus douces
Raffinement Effort Transitoire
python
def refine
transient
_
_
effort(config, stats)
Actions :
●
●
Poids w: ×0.8 avec conservation Σw=0
Diagonale forcée à 0
FACTEURS DE RAFFINEMENT
171
Depuis config['refinement
_
factors'] :
python
{
'k
reduction': 0.8,
_
'alpha
reduction': 0.7,
_
'sigma
increase': 1.3,
_
'epsilon
increase': 1.5,
_
'weight
reduction': 0.8
_
}
7. TRAÇABILITÉ
Changelog
python
def log_
refinement(changelog_path, date, run
_
id,
criterion, old
value, new
_
_
value, reason)
Format :
[2025-01-15 14:30:00] | batch
_
5 | alpha: 0.5 → 0.7 | Dépassement sur 60.0% des runs
Journal des seuils
python
def export
threshold
_
_journal(threshold
_
history, output
_path)
Structure JSON :
json
{
"timestamp": "2025-01-15T14:30:00"
,
"thresholds": {
"fluidity": {
"threshold": 0.5,
"runs
_
triggered": 3,
"trigger
rate": 0.6,
_
"mean
_
values": [0.3, 0.35, 0.4]
},
}
"version": "1.0"
172
}
ANALYSES CROISÉES
Corrélation effort/CPU
python
def compute
correlation
effort
_
_
_
cpu(effort
_
history, cpu
_
history)
Utilise metrics.py ou calcul fallback si non disponible
Corrélations inter-métriques
python
def analyze
cross
_
_
metrics(run
_
data)
Paires analysées :
●
●
●
●
effort(t) vs cpu
_
step(t)
entropy_
S vs variance
d2S
_
mean
abs
_
_
error vs effort(t)
C(t) vs S(t)
ROBUSTESSE
Gestion erreurs
●
●
●
●
Vérification existence fichiers
Validation types (list, dict)
Fallback si modules manquants
Try/except sur chargement CSV
Conservation contraintes
●
●
●
Poids : Σw=0 maintenu après raffinement
Diagonale : w[i][i]=0 forcé
Bornes : respect min/max sur tous paramètres
WORKFLOW TYPE
Calibration initiale (5 runs)
python
173
# 1. Lancer 5 runs avec config initiale
runs = ['run1.csv'
'run2.csv'
'run3.csv'
,
,
,
'run4.csv'
,
# 2. Analyser et raffiner
result = analyze
criteria
and
_
_
_
refine(runs, config)
# 3. Si raffinements appliqués
if result['refinements']:
# Sauvegarder nouvelle config
with open('config_
refined.json'
,
json.dump(result['updated
_
'w') as f:
config'], f)
# Consulter changelog
print("Voir logs/changelog.txt pour détails")
'run5.csv']
Cycle itératif
Run batch → Analyse → Raffinement → Nouveau batch → ...
Convergence typique en 2-3 itérations.
EXEMPLES DE DÉCLENCHEMENT
Cas 1 : Fluidity faible
Batch : 5 runs
Fluidity < 0.5 pendant 70% du temps sur 3 runs
→ Trigger rate = 60% > 50%
→ Raffinement déclenché
→ gamma
_
n passe en dynamique, sigma
_
n augmenté
Cas 2 : Effort chronique
Batch : 5 runs
mean
_
high
_
effort > 2×médiane sur 80% du temps dans 4 runs
→ Trigger rate = 80% > 50%
→ Raffinement déclenché
→ alpha réduit à 60%, sigma
_
n augmenté
EXTENSIBILITÉ
Ajout nouveau critère
python
# 1. Ajouter dans criteria
_
metrics
174
'nouveau
_
critere': {
'metric': 'nouvelle
_
metrique'
,
'threshold
_
key': 'nouveau
seuil'
,
_
'condition': lambda x, t: x > t,
'threshold
_percent': 0.5
}
# 2. Créer fonction raffinement
def refine
_
nouveau(config, stats):
# Logique de raffinement
return changes
# 3. Enregistrer dans REFINEMENT
_
FUNCTIONS
REFINEMENT
_
FUNCTIONS['nouveau
_
critere'] = refine
_
nouveau
Note : Analyze.py est le module logistique de finetunning du système FPS, permettant une évolution
continue vers l'harmonie optimale.
2.1.g. Visualisations via visualize.py
Les yeux qui révèlent la danse invisible des oscillateurs
ORGANISATION DU MODULE
Configuration visuelle
python
# Style et dimensions par défaut
plt.style.use('seaborn-v0
_
8-darkgrid')
plt.rcParams['figure.figsize'] = (12, 8)
# Palette FPS thématique
FPS
_
COLORS = {
'primary': '#2E86AB'
, # Bleu profond
'secondary': '#A23B72'
, # Magenta
'accent': '#F18F01'
, # Orange
'success': '#87BE3F'
, # Vert
'warning': '#FFC43D'
, # Jaune
'danger': '#C73E1D'
, # Rouge
'spiral': '#6A4C93' # Violet spirale
}
175
Catégories de visualisations
1. Évolution temporelle - Signaux dans le temps
2. Comparaisons - Entre strates ou modes
3. Espace de phase - Diagrammes polaires
4. Tableaux de bord - Vue d'ensemble
5. Grilles empiriques - Notation visuelle
6. Animations - Évolution dynamique
7. Matrices - Corrélations et liens
8. Rapports - Export HTML
ÉVOLUTION TEMPORELLE
Signal global S(t)
python
def plot
_
signal
_
evolution(t
_
array, S
_
array, title="
...
")
Caractéristiques :
●
●
●
Signal principal avec enveloppe ±1σ glissante
Fenêtre glissante : min(50, len(S)/10)
Zone d'incertitude en transparence
COMPARAISON DES STRATES
python
def plot
strata
_
_
comparison(t
_
array, An
_
arrays, fn
_
arrays)
Layout : 2 subplots verticaux
●
●
●
Haut : Amplitudes An(t) par strate
Bas : Fréquences fn(t) par strate
Couleurs : viridis colormap (20 couleurs)
DIAGRAMME DE PHASE
python
def plot
_phase
_
diagram(phi
n
_
_
arrays)
Représentation polaire :
●
Chaque strate à rayon différent : r = 0.5 + n * 0.5/N
176
●
●
Points colorés par strate
Cercle unitaire en référence
TABLEAU DE BORD COMPLET
Structure GridSpec 5×3
python
def plot
metrics
_
_
dashboard(metrics
_
history)
Disposition :
[ S(t) (2 cols) ] [ C(t) ]
[ Effort ] [ Innovation ] [ Régulation ]
[ Histogramme ] [ Pie Status ] [ Stats ]
[ Dynamique complète En/On/In etc ]
Bloc dynamique complète (nouveau)
python
# Ligne 4 : Alignement En/On avec gamma
ax.plot(En
'g--
'
mean,
_
, label='En (attendu)')
ax.plot(On
mean,
'b-
'
_
, label='On (observé)')
ax.plot(In
mean,
'r:'
_
, label='In (input)')
ax.fill
_
between(t, y_
min, gamma
scaled,
_
color='orange'
, alpha=0.3, label='γ')
Gestion fluidity
python
# Affichage direct si disponible
if 'fluidity' in history_
dict:
ax.plot(fluidity)
# Sinon calcul depuis variance
_
else:
d2S
x = variance
data / 175.0
_
fluidity = 1 / (1 + np.exp(5.0 * (x - 1)))
GRILLE EMPIRIQUE
177
Notation visuelle
python
def create
_
empirical
_grid(scores
_
dict: Dict[str, int])
Échelle de notation :
Score Icône Couleur État
1 ✖ Rouge Rupture/Chaotique
2 ▲ Orange Instable
3 ● Jaune Fonctionnel
4 ✔ Vert Harmonieux
5 ∞ Bleu FPS-idéal
Critères évalués
●
●
●
●
●
●
●
Stabilité
Régulation
Fluidité
Résilience
Innovation
Coût CPU
Effort interne
Affichage :
●
Nom critère + icône + barre progression + note/5 + description
ANIMATION SPIRALE
python
def animate
_
spiral
_
evolution(data, output
_path)
Double vue animée :
●
●
Gauche : Signal S(t) temporel
Droite : Spirale polaire avec r = 0.5 + 0.5*C(t)
Export : GIF via pillow writer (20 fps)
COMPARAISON FPS VS KURAMOTO
178
python
def plot
_
fps
vs
_
_
kuramoto(fps
data, kuramoto
_
_
data)
Layout 2×2
1. 2. 3. 4. Signal S(t) : Superposition FPS (trait plein) vs Kuramoto (tirets)
Coefficient C(t) : Synchronisation comparative
Effort/CPU : Double axe Y (effort gauche, CPU droite)
Métriques bar chart : Mean S, Std S, CPU (μs), Final C
Gestion robuste
python
# Vérification longueurs et contenus
if 'S(t)' in fps
_
data and len(fps
_
ax.plot(fps
_
data['S(t)'])
data['S(t)']) > 0:
# Conversion CPU en microsecondes
fps
_
cpu
_
mean = np.mean(fps
_
data['cpu
_
step(t)']) * 1e6
MATRICE DE CORRÉLATION
python
def generate
correlation
_
_
matrix(criteria
terms
_
_
mapping)
Input : {critère: [termes]}
Process :
1. Extraction termes uniques
2. Matrice binaire critères × termes
3. Heatmap avec colormap YlOrRd
4. Grille mineure pour séparation
RAPPORT HTML
python
def export
html
_
_
report(all
_
data, output
_path)
Structure HTML
179
html
<!DOCTYPE html>
<style>
.metric-box {
display: inline-block;
padding: 15px;
border-left: 4px solid #2E86AB;
}
.grid-container {
display: grid;
grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
}
</style>
Sections
1. 2. 4. Métriques principales - Boîtes avec valeurs
Détection émergences - Liste événements
3. Configuration - JSON formaté
Footer - Timestamp et crédits
Formatage intelligent
python
if abs(value) < 0.001:
formatted = f"{value:.2e}" # Scientifique
elif int(value) == value:
formatted = f"{int(value)}" # Entier
else:
formatted = f"{value:.3f}" # Décimal
RÉSILIENCE ADAPTATIVE
python
def plot
_
adaptive
_
resilience(metrics
_
history, perturbation
_
type='none')
Sélection automatique :
●
●
adaptive
_
resilience si disponible (métrique unifiée)
Sinon selon perturbation :
○
'choc'
→ t
retour normalisé
_
○
'sinus'/'bruit'/'rampe'
→ continuous
resilience_
180
Visualisation :
●
●
●
Lignes de référence (0.5 acceptable, 0.8 excellent)
Statistiques en encadré
Interprétation colorée
UTILITAIRES
Sauvegarde batch
python
def save
all
_
_
figures(figures: Dict[str, plt.Figure], output
_
dir: str)
●
●
Sauvegarde PNG à 150 DPI
bbox
_
inches='tight' pour marges optimales
PATTERNS D'UTILISATION
Workflow type visualisation
python
# 1. Générer les données
t = np.linspace(0, T, n
_points)
S = simulate
_
fps(...)
# 2. Créer les figures
fig1 = plot
_
signal
_
evolution(t, S)
fig2 = plot
metrics
_
_
dashboard(history)
fig3 = create
_
empirical
_grid(scores)
# 3. Sauvegarder
save
all
_
_
figures({
'signal': fig1,
'dashboard': fig2,
'grid': fig3
}, output
_
dir)
# 4. Rapport HTML
export
html
_
_
report(all
_
data,
'report.html')
Comparaison modes
python
# Lancer FPS et Kuramoto
181
fps
_
kura
result = run
_
result = run
simulation('FPS')
_
_
simulation('Kuramoto')
# Visualiser comparaison
fig = plot
_
fps
vs
_
_
kuramoto(fps
result, kura
_
_
result)
ROBUSTESSE
Gestion données manquantes
●
●
●
Vérification if key in data and len(data[key]) > 0
Fallback calculs (fluidity depuis variance)
Axes conditionnels (twin Y si données CPU)
Performance
●
●
●
Fenêtres glissantes limitées
Downsampling pour grandes données
Vectorisation NumPy
Flexibilité
●
●
●
Conversion dict/list automatique
Couleurs paramétrables
Titres personnalisables
Note : Visualize.py transforme les données brutes en insights visuels
2.1.h. Utilitaires (initialisations, structures d’outputs, conversions, protection contre
valeurs aberrantes, etc..)
a) Init.py
Le chef d'orchestre qui prépare la scène avant la danse
FONCTIONS PRINCIPALES
Configuration et seed
python
def load
_
config(config_path='config.json')
182
# Charge le JSON de configuration
def set
_
seed(seed)
# Évite double initialisation
# Fixe seed pour numpy et random
Fonction orchestratrice
python
def initialize
_
system(config, post
init
_
_
callback=None)
Workflow :
1. Fixe la seed
2. Initialise les strates
3. Estime l'usage mémoire
4. Prépare l'historique
5. Construit l'état système
6. Appelle callback si fourni
INITIALISATION DES STRATES
Structure d'une strate
python
strate = {
# Identité
'id': int,
# Paramètres initiaux
'A0': float, # Amplitude initiale
'f0': float, # Fréquence initiale
'phi': float, # Phase initiale
'alpha': float, # Souplesse adaptation
'beta': float, # Force feedback
'k': float, # Sensibilité
'x0': float, # Position équilibre
'w': list, # Poids connexions
# États dynamiques
'An': float, # Amplitude courante
'fn': float, # Fréquence courante
'gamma
n': float, # Latence courante
_
'mu
_
n': float, # Centre enveloppe
'sigma
_
n': float, # Largeur enveloppe
183
# États internes
'En': float, # Sortie attendue
'On': float, # Sortie observée
'history': list, # Historique local
# Flags dynamiques
'dynamic
_phi': bool,
'dynamic
_
alpha': bool,
'dynamic
beta': bool,
_
# Paramètres dynamiques
'gamma
n
_
_params': dict,
'sigma
n
_
_params': dict
}
Génération automatique des poids spiralés
python
coupling_
cfg = config.get("coupling"
, {})
if coupling_
cfg.get("type") in {"spiral"
,
"ring"}:
W
_
spiral = generate
_
spiral
_
weights(
N
_
total=len(config["strates"]),
c=coupling_
cfg.get("c"
, 0.25),
closed=coupling_
cfg.get("closed"
, False),
mirror=coupling_
cfg.get("mirror"
, False)
)
Modes de couplage :
●
●
●
"spiral" : Couplage en spirale (peut être ouvert/fermé)
"ring" : Force closed=True (anneau fermé)
"custom" : Utilise les poids du config.json
Validation des poids
python
def verify_
weight
_
matrix(w, strate
_
id, epsilon=1e-8, enforce
zero
_
_
sum=True)
Règles strictes :
1. 2. Diagonale nulle : w[i][i] = 0 (pas d'auto-connexion)
Conservation : Σw[i] = 0 (sauf extrémités spirale ouverte)
Tolérance extrémités :
python
184
# Pour spirale ouverte, extrémités peuvent violer Σw=0
spiral
_
open = spiral
mode and not closed
val
_
_
skip_
edges = spiral
_
open and not mirror
_
mode and (i == 0 or i == N
_
enforce
_
sum = not skip_
edges
total - 1)
Auto-correction :
python
if erreurs
detectees:
_
# Force diagonale à 0
w[strate['id']] = 0.0
# Redistribue pour Σw = 0
correction = -sum(w) / (len(w) - 1)
for j in range(len(w)):
if j != strate['id']:
w[j] += correction
PARAMÈTRES DYNAMIQUES
Initialisation gamma
_
n (latence)
python
if latence
_
config.get("gamma
n
_
_
mode") == "dynamic":
gamma
n
_
_
init = 0.5 # Valeur initiale dynamique
gamma
n
_
_params = {
"k
n": 2.0,
_
"t0
n": 50
_
}
else:
gamma
n
_
_
init = 1.0 # Mode statique
Initialisation sigma
_
n (enveloppe)
python
if enveloppe
_
config.get("env
_
mode") == "dynamic":
sigma
n
init = 0.1
_
_
sigma
n
_
_params = {
"amp": 0.05,
"freq": 1,
"offset": 0.1,
"T": 100
}
185
else:
sigma
n
_
_
init = enveloppe
_
config.get("sigma
n
_
_
static"
, 0.1)
SYSTÈME DE LOGGING
Configuration
python
def setup_
logging(config, log_
dir="logs"
, mode
_
suffix=None)
Naming convention :
run
_{YYYYMMDD-HHMMSS}_{MODE}_
seed{SEED}.csv
Structure retournée :
python
{
'csv
writer': csv.writer,
_
'csv
file': file
_
_
object,
'run
id': str,
_
'output
dir': str,
_
'log_
file': str
}
En-têtes CSV
python
# Depuis config['system']['logging']['log_
metrics']
['t'
,
'S(t)'
,
'C(t)'
,
'effort(t)'
,
'cpu
_
step(t)'
, ...]
Traçabilité seed
logs/seeds.txt:
20250622-143025 | FPS | SEED = 12345
20250622-143026 | Kuramoto | SEED = 12345
GESTION MÉMOIRE
Estimation usage
186
python
n
_
metrics = len(config['system']['logging']['log_
metrics'])
estimated
_points = N * T * n
metrics
_
estimated
_
mb = (estimated
_points * 8) / (1024 * 1024) # 8 bytes/float64
safe
limit = 2
000
000
_
_
_
Avertissement
Exemple :
⚠ Attention : L'historique va contenir ~5,000,000 points.
Estimation mémoire : ~38.1 MB
Paramètres : N=50, T=1000, 100 métriques
Options : réduire N/T/log_
metrics ou activer compression
Confirmation utilisateur si dépassement
ÉTAT SYSTÈME COMPLET
Structure retournée
python
system
_
state = {
# Core
'strates': List[Dict],
't': 0,
'config': Dict,
'mode': 'FPS'|'Kuramoto'|'neutral'
,
# Historique
'history': {metric: [] for metric in log_
metrics},
# Configurations
'perturbation': Dict,
'exploration': Dict,
'dynamic
_parameters': Dict,
'regulation': Dict,
'latence': Dict,
'enveloppe': Dict,
'validation': Dict,
'analysis': Dict,
# Métadonnées
'init
_
timestamp': ISO-8601,
'fps
version': '1.3'
,_
187
'weight
validation
_
_passed': bool,
# Extensibilité
'logs': {},
'run
id': None
_
}
EXTENSIBILITÉ PAR CALLBACK
Mécanisme
python
def initialize
_
system(config, post
init
_
_
# ... initialisation normale ...
callback=None):
if post
init
_
_
callback:
post
init
_
_
callback(system
_
state)
Exemple d'usage
python
def patch
for
_
_
kuramoto(system
_
state):
"""Adapte l'état pour mode Kuramoto"""
system
_
state['mode'] = 'Kuramoto'
system
_
state['kuramoto
_
specific'] = {
'K': 0.5,
'order
_param': 0.0
}
system = initialize
_
system(config, post
init
_
_
callback=patch
for
_
_
kuramoto)
Cas d'usage :
●
●
●
●
Ajout paramètres spécifiques mode
Monitoring/logging custom
Patches expérimentaux
Extensions tierces
VALIDATION PRÉ-INITIALISATION
Workflow complet
python
188
# 1. Validation config
errors, warnings = validate
_
config(config_path)
if errors:
exit(1)
# 2. Chargement après validation
config = load
_
config(config_path)
# 3. Setup logging
loggers = setup_
logging(config)
# 4. Initialisation système
system
state = initialize
_
_
system(config)
FICHIERS GÉNÉRÉS
logs/
├── seeds.txt # Historique des seeds
├── run
*
FPS
seed*
_
_
_
.csv # Logs simulation
├── warnings.txt # Avertissements système
└── weight
_
validation.txt # Erreurs matrices poids
POINTS CLÉS
Robustesse
●
●
●
Validation stricte des poids avec auto-correction
Évite double init de seed
Gestion mémoire avec estimation et avertissement
Flexibilité
●
●
●
Traçabilité
●
●
●
Poids automatiques ou custom
Callbacks pour extensions
Mode suffix pour différencier logs
Seeds loguées systématiquement
Warnings persistants dans fichiers
Métadonnées complètes dans system
_
state
b) utils.py
189
L'orchestre invisible qui fait tourner la machine FPS et le pipeline de tests
ORGANISATION FONCTIONNELLE
Le module utils.py regroupe 7 catégories de fonctions :
1. 3. 4. 5. 6. 7. Gestion des logs - Fusion, traçabilité
2. Sauvegarde/Restauration - Checkpoints, recovery
Replay/Analyse - Rejeu depuis logs
Exécution parallèle - Batch runner
Export données - HDF5, archives
Génération IDs - Identifiants uniques
Topologies - Génération poids spiralés
GESTION DES LOGS
Fusion de logs multiples
python
def merge
_
logs(log_
files: List[str], output
_path: str, format='csv')
●
●
●
●
Charge plusieurs CSV
Ajoute colonne source
file
_
Trie par temps si colonne t existe
Export CSV ou Parquet
Traçabilité
python
def log_
seed(seed: int, seed
_
file="seeds.txt")
# Format: "2025-01-15 14:30:25 | SEED = 12345"
def log_
config_
and
_
meta(config, run
_
id, output
_
dir="logs")
# Génère: config_{run
_
id}.json et meta
_{run
_
id}.json
def log_
end
of
_
_
run(run
_
id, summary=None)
# Enregistre dans runs
_
completed.txt
SAUVEGARDE ET RESTAURATION
Checkpoints
python
190
def save
simulation
_
_
state(state: Dict, checkpoint
_path: str)
# Pickle pour types numpy
# Génère aussi {checkpoint}_
info.txt
def load
simulation
_
_
state(checkpoint
_path: str) -> Dict
# Restaure depuis pickle
Crash recovery
python
def handle
crash
_
_
recovery(state, loggers, exception)
Workflow :
1. Créé dossier crash
_
recovery/
2. Sauvegarde état avec pickle
3. Génère crash
_{timestamp}_
info.json :
json
{
"timestamp": "
"
...
,
"exception
_
type": "ValueError"
,
"exception
_
message": "
"
...
,
"traceback": "
...
"
,
"t
current": 50.5,
_
"n
strates": 10
_
}
REPLAY ET ANALYSE
Rejeu depuis logs
python
def replay_
from
_
logs(csv
_path, start
t=0, end
_
_
t=None)
●
●
●
Charge CSV avec pandas
Filtre par intervalle temporel
Retourne Dict[metric, np.ndarray]
Comparaison de runs
python
191
def compare
_
runs(run1
_path, run2
_path, metrics) -> Dict
Calcule pour chaque métrique :
●
●
●
Moyennes et écarts-types
Différence des moyennes
Corrélation entre runs
EXÉCUTION PARALLÈLE
Worker pour simulation
python
def run
_
single
_
simulation(args: Tuple[config_path, override
_params])
●
●
●
●
Charge config
Applique overrides
Lance simulate.run
_
simulation()
Retourne status + métriques
Batch runner
python
def batch
_
runner(configs
_
list, parallel=True, n
_
workers=None)
Modes :
●
●
Parallèle : Pool multiprocessing (défaut: tous CPUs)
Séquentiel : Pour debug
Input flexible :
python
configs = [
"config1.json"
, # Simple
("config2.json"
, {"system.N": 20}) # Avec overrides
]
EXPORT DE DONNÉES
Export HDF5
python
192
def export
to
_
_
hdf5(data
_
dict: Dict[str, np.ndarray], hdf5
_path: str)
●
●
●
Compression gzip automatique
Métadonnées dans attributs
Fallback si h5py non disponible
Archivage
python
def archive
_
run(run
dir, archive
_
_
# Crée ZIP du dossier complet
name=None) -> str
def setup_
directories(base
_
dir="fps
_
# Structure créée:
# fps
_
output/run
20250115
143025/
_
_
# ├── logs/
# ├── checkpoints/
# ├── figures/
# ├── reports/
# └── configs/
output") -> Dict
GÉNÉRATION D'IDENTIFIANTS
python
def generate
run
_
_
id(prefix="run") -> str
Format : {prefix}_{YYYYMMDD
_
Exemple : run
20250115
143025
_
_
_
HHMMSS}_{hash6}
a3f2b1
INTÉGRITÉ DES DONNÉES
Checksum
python
def compute
_
checksum(file
_path, algorithm='sha256') -> str
Vérification
python
193
def verify_
data
_
integrity(data
_
dir, checksum
_
file="checksums.txt")
Format checksums.txt :
a3f2b1c4d5e6... data.csv
b2c3d4e5f6a7... config.json
CONVERSION DE TYPES
Deep convert standard
python
def deep_
convert(obj)
# np.ndarray → list
# np.integer → int
# np.floating → float
# Récursif sur dict/list
Deep convert pour JSON
python
def deep_
convert
for
_
_json(obj)
# Gère en plus:
# tuple (clés) → string "(a,b)"
# objets custom →
dict
__
__
GÉNÉRATION POIDS SPIRALÉS
Fonction principale
python
def generate
_
spiral
_
weights(N, c=0.25, closed=False, mirror=False)
Logique de couplage
python
# Couplage antisymétrique
for i in range(N - 1):
W[i, i + 1] = +c W[i + 1, i] = -c # i → i+1
# i+1 → i (feedback)
194
# Si fermé (ring)
if closed and N > 2:
W[N - 1, 0] = +c
W[0, N - 1] = -c
# Si mirror (conservation aux bords)
elif mirror and N > 2:
W[0, 1] -= c # Première ligne Σ=0
W[N-1, N-2] += c # Dernière ligne Σ=0
Propriétés :
●
●
Antisymétrique : W[i,j] = -W[j,i]
Conservation : Σ W[i] = 0 (sauf bords si spiral ouvert)
DÉCOUVERTES ADAPTATIVES
Sauvegarde découvertes couplées
python
def save
_
coupled
_
discoveries(gamma
_journal, regulation
_
state, output
_path)
Gestion volumétrie :
●
●
Si < 15MB : fichier unique
Si > 15MB : division en chunks
Structure chunks :
discoveries
_
coupled
_parts/
├── index.json
├── part
_
000.json # États 0-999
├── part
_
001.json # États 1000-1999
└──
...
UTILITAIRES DIVERS
Formatage durée
python
def format
_
duration(seconds: float) -> str
# 3665.2 →
"1h 1m 5.2s"
195
Informations système
python
def get
_
system
_
info() -> Dict
Retourne :
●
●
●
●
Platform, Python version
CPU count
Memory (total/available GB)
Disk usage %
PATTERNS D'UTILISATION
Workflow type simulation
python
# 1. Setup
dirs = setup_
directories()
run
_
id = generate
run
_
_
id()
# 2. Logging
log_
seed(seed)
log_
config_
and
_
meta(config, run
_
id, dirs['configs'])
# 3. Simulation avec checkpoints
try:
# ... simulation ...
if step % 100 == 0:
save
simulation
_
_
state(state, checkpoint
_path)
except Exception as e:
handle
crash
_
_
recovery(state, loggers, e)
# 4. Fin
log_
end
archive
of
_
_
run(run
_
id, summary)
_
run(dirs['base'])
Batch parallèle
python
configs = [
("config.json"
, {"system.seed": i})
for i in range(100)
]
196
results = batch
_
runner(configs, parallel=True)
Analyse post-hoc
python
# Rejeu partiel
data = replay_
from
_
logs("run.csv"
, start
t=50, end
_
_
t=100)
# Comparaison
comparison = compare
_
runs("run1.csv"
,
["S(t)"
,
"effort(t)"])
"run2.csv"
,
ROBUSTESSE
Gestion erreurs
●
●
●
●
Checksums pour intégrité
Pickle + info.txt pour checkpoints
Crash recovery automatique
Deep convert avant JSON
Performance
●
●
●
Multiprocessing pour batch
HDF5 + gzip pour gros volumes
Chunks pour découvertes > 15MB
Traçabilité
●
●
●
●
seeds.txt : Toutes les seeds
runs
_
completed.txt : Tous les runs
checksums.txt : Intégrité fichiers
meta
_{run
_
id}.json : Métadonnées
c) test
_
fps.py
Ces tests garantissent que chaque pièce de la mécanique FPS fonctionne parfaitement, seule et en
harmonie avec les autres.
ARCHITECTURE DES TESTS
Organisation en classes
197
python
TestDynamics # Fonctions de dynamics.py
TestRegulation # Module regulation.py
TestMetrics # Calculs de métriques
TestValidateConfig # Validation configuration
TestPerturbations # Génération perturbations
TestAnalyze # Analyse et raffinement
TestExplore # Détection émergences
TestKuramoto # Mode Kuramoto
TestUtils # Fonctions utilitaires
TestIntegration # Système complet
TestPerformance # Limites et performance
Structure commune
python
class TestModule(unittest.TestCase):
def setUp(self):
# Configuration initiale
def tearDown(self):
# Nettoyage
def test
_
fonction(self):
# Tests unitaires
TESTS DYNAMICS
Fonction sigmoïde
python
def test
_
compute
_
sigma(self):
# Points critiques
σ(0, k=2, x0=0) = 0.5 σ(-10, k=2, x0=0) ≈ 0.0 σ(10, k=2, x0=0) ≈ 1.0 # Centre
# Saturation basse
# Saturation haute
Modes de latence gamma
python
def test
_
compute
_gamma
_[mode](self):
198
Modes testés :
●
●
●
●
●
●
static : γ = 1.0 constant
dynamic : γ(T/2) = 0.5 (point médian)
sigmoid
_
up : Croissance monotone
sigmoid
down : Décroissance monotone
_
sigmoid
_
adaptive : Borné [0.3, 1.0]
sigmoid
_
oscillating : Variations détectées
Signal inter-strates
python
def test
_
compute
S
_
_
i(self):
# Vérification formule FPS
S
_
i = Σ(j≠n) O
_j * w
_
nj
Conformité formules
python
def test
formule
S
i
_
_
_
_
conforme(self):
# Pour chaque strate n
expected = Σ(j≠n) history['O'][j] * w
_
n[j]
def test
formule
Fn
_
_
_
# Feedback
Fn = βn * (On - En) * γ
conforme(self):
TESTS REGULATION
Archétypes G(x)
python
def test
_
compute
G
_
_
archetypes(self):
Vérifications :
●
●
●
tanh : G(0) = 0 (centré)
sinc : G(0) = 1 (maximum)
resonance : G(0) = 0
Enveloppes
python
199
def test
_
compute
env
n
_
_
_
static(self):
# Gaussienne : env(0, μ=0) = 1
# Sigmoïde : env(0, μ=0) = 0.5
Modes feedback
python
def test
feedback
_
_
modes(self):
Modes :
●
●
●
simple : βn(On-En)γ
archetype : Avec G(x)
gn
_
full : Enveloppe complète
TESTS METRICS
Métriques de base
python
def test
_
compute
_
cpu
_
step(self):
# Mesure temps < 1s/strate
def test
_
compute
_
effort(self):
# effort = Σ|ΔAn| + Σ|Δfn| + Σ|Δγn|
Fluidity
python
def test
_
compute
_
fluidity(self):
Tests critiques :
●
●
●
●
variance=0 → fluidity=1.0
variance=175 → fluidity≈0.5
variance=350 → fluidity<0.1
Monotonie : ↑variance → ↓fluidity
Résilience adaptative
python
200
def test
_
_
_
compute
adaptive
resilience(self):
Sélection automatique :
●
●
●
Perturbation continue → continuous
Perturbation ponctuelle → t
_
Sans perturbation → défaut t
retour
retour
_
resilience
_
Scoring :
●
●
Score 1-5 selon valeur métrique
Exemple : resilience=0.85 → score=4
Statut effort
python
def test
_
compute
effort
_
_
status(self):
Classification :
●
●
●
stable : Sous seuils
transitoire : Pic isolé
chronique : Élevé persistant
TESTS VALIDATE
_
CONFIG
Configuration valide
python
def test
validate
valid
_
_
_
config(self):
config = generate
default
_
_
config(N=5, T=100)
errors, warnings = validate
_
config(config)
assert len(errors) == 0
Détection erreurs
python
def test
validate
invalid
_
_
_
config(self):
Cas testés :
●
Config sans system
201
●
●
N négatif
Matrices poids invalides
Mise à jour seuils
python
def test
_
update
_
config_
threshold(self):
# Modification + changelog
TESTS PERTURBATIONS
Types de perturbations
python
def test
_perturbation
_
types(self):
Vérifications :
●
●
●
choc : 0 avant t0, amplitude pendant, 0 après
rampe : Croissance linéaire
sinus : sin(2πft)
Séquences
python
def test
_perturbation
_
sequence(self):
# Combinaison de perturbations
# Vérification longueur et valeurs
Scénarios
python
def test
_
scenarios(self):
Scénarios : stress
test, environnement
_
_
variable, chaos
TESTS ANALYZE
Chargement données
202
python
def test
load
run
_
_
_
data(self):
# CSV → Dict[metric, array]
Raffinements
python
def test
refinement
_
_
functions(self):
Fonctions testées :
●
●
●
refine
_
fluidity : σn augmenté
refine
_
stability : k et α réduits
Autres raffinements
TESTS EXPLORE
Détection anomalies
python
def test
detect
_
_
anomalies(self):
# Injection anomalie : S[200:220] += 5.0
# Vérification détection
Motifs fractals
python
def test
detect
fractal
_
_
_patterns(self):
# Ajout auto-similarité multi-échelles
# Détection corrélations
TESTS KURAMOTO
Pas Kuramoto
python
def test
kuramoto
_
_
step(self):
# dφ/dt = ω + (K/N)Σsin(φj - φi)
# Vérification couplage
203
Paramètre d'ordre
python
def test
_
compute
kuramoto
_
_
order(self):
Cas limites :
●
●
Phases synchronisées : r ≈ 1.0
Phases uniformes : r < 0.2
TESTS UTILS
Génération IDs
python
def test
_generate
run
_
_
# Unicité
id(self):
# Format : run
_{timestamp}_{hash}
Sauvegarde/Restauration
python
def test
save
load
_
_
_
state(self):
# Pickle → fichier → restauration
# Vérification intégrité
Formatage durée
python
def test
_
45.3 →
125 →
format
_
"45.3s"
"2m 5.0s"
duration(self):
3665 →
"1h 1m 5.0s"
TESTS INTÉGRATION
Flux complet
python
204
@patch('simulate.init.setup_
logging')
@patch('simulate.init.init
_
strates')
def test
simulation
_
_
flow(self, mocks...):
Vérifications :
●
●
●
Imports modules
Initialisation
Pipeline complet
TESTS PERFORMANCE
Grande échelle
python
def test
_
large
n
_
_
# N=50 strates
strates(self):
# Vérification calculs
Stabilité numérique
python
def test
numerical
_
_
stability(self):
# Valeurs extrêmes : [1e-10, 1e10,
# σ reste dans [0,1]
# G reste borné
-1e10]
RUNNER PRINCIPAL
Fonction orchestratrice
python
def run
all
_
_
tests():
# Charge toutes les classes
# Execute avec verbosité
# Génère rapport
Rapport final
RÉSUMÉ DES TESTS FPS
====================
Tests exécutés : 87
205
Succès : 85
Échecs : 2
Erreurs : 0
✅ TOUS LES TESTS SONT PASSÉS !
La FPS est prête à danser ! 🌀
COUVERTURE DES TESTS
Modules testés
Module Tests Couverture
dynamics 11 Fonctions principales
regulation 5 Archétypes, enveloppes
metrics 7 Toutes métriques
validate
_
config 3 Validation, mise à jour
perturbations 3 Types, séquences
analyze 2 Chargement, raffinement
explore 2 Anomalies, fractals
kuramoto 2 Step, ordre
utils 3 IDs, état, durée
integration 1 Flux complet
performance 2 Échelle, stabilité
Points critiques validés
●
●
●
●
●
Formules FPS : S
i, Fn conformes
_
Bornes : σ∈[0,1], G borné
Monotonie : Modes gamma cohérents
Unicité : IDs uniques
Robustesse : Valeurs extrêmes
UTILISATION
Lancement simple
bash
python test
_
fps.py
206
Lancement ciblé
python
# Test unique
python -m unittest test
_
fps.TestMetrics.test
_
compute
_
fluidity
# Classe complète
python -m unittest test
_
fps.TestDynamics
2.2. Hypothèses de départ
Assumptions.md :
Hypothèses fondamentales
a/ Nature du système
- Hypothèse : Le système FPS peut modéliser des dynamiques adaptatives complexes via la
superposition de strates oscillantes couplées
- Test : Comparer avec Kuramoto (synchronisation pure) et mode "neutral" (pas de feedback)
- Métrique : Amélioration de la stabilité, résilience et innovation par rapport aux contrôles
b/ Spiralisation
- Hypothèse : Le nombre d'or φ ≈ 1.618 est un attracteur naturel pour l'harmonie du système
- Test : Faire varier φ et observer la stabilité/résilience
- Métrique : Temps de retour à l'équilibre, entropie spectrale
c/ Hypothèse de régulation
- Présupposé : La régulation locale suffit (pas de régulation globale)
- Test : Ajouter terme de régulation collective
- Métrique : Stabilité sous perturbations multiples
d/ Motifs fractals
- Hypothèse : Auto-similarité détectable par corrélation multi-échelles
- Fenêtres : [1, 10, 100] pas de temps
- Seuil : Corrélation > 0.8
- À raffiner : Dimension fractale, exposants de Lyapunov
e/ Domaine d'application
- N : 3 à 100 strates (testé jusqu'à 50)
207
- T : 10 à 1000 unités de temps
- dt : 0.01 à 1.0 (stabilité numérique)
f/ Critères de réussite
- Stabilité : max(|S(t)|) < 10×médiane(|S(t)|) sur 95% du temps
- Régulation : mean(|E - O |) décroît dans le temps
- Fluidité : variance(d²S/dt²) < 0.01 sur 70% du temps
- Résilience : t
_
retour < 2×médiane après perturbation
- Innovation : entropy_
S > 0.5 (richesse spectrale)
2.3. Résultats et observations
Les analyses menées sur les simulations FPS du 10 septembre 2025 révèlent des propriétés
remarquables du système. Deux simulations identiques (seed 12349) ont été exécutées sur des
durées différentes : 5000 pas (T=500) et 10000 pas (T=1000), permettant d'observer la scalabilité
temporelle du système.
208
2.3.a. Comparaison avec les modèles de contrôle
La FPS démontre une supériorité significative par rapport aux modèles de référence :
- Score global : FPS atteint 0.833 contre 0.496 pour Kuramoto (+68.1%) et 0.209 pour le modèle
neutre (+298.1%)
- Synchronisation : +13.6% par rapport à Kuramoto, avec une valeur moyenne de 0.995
- Stabilité : Amélioration spectaculaire de +4597.3% vs Kuramoto
- Innovation : +366.1% vs Kuramoto, maintenant une entropie moyenne de ~0.75
- Résilience : -15.4% en résilience ponctuelle/chocs et +69.2% en résilience continue (ici nous
sommes dans des configurations de perturbation continue, nous avons donc une efficience)
209
Point notable : la FPS sacrifie légèrement l'efficacité CPU (-97.8% vs Kuramoto qui est très léger par
nature) au profit d'une richesse dynamique accrue.
2.3.b. Événements d’émergence
L'analyse révèle une augmentation proportionnelle des événements entre les deux simulations :
Simulation 5000 pas
- Total : 1319 événements
- Anomalies : 1071 (81.2%)
- Patterns fractals : 217 (16.5%)
- Émergences harmoniques : 30 (2.3%)
Simulation 10000 pas
- Total : 2550 événements (ratio ~2x)
- Anomalies : 2081 (81.6%)
- Patterns fractals : 412 (16.2%)
- Émergences harmoniques : 55 (2.2%)
La constance des ratios suggère une dynamique d'émergence stable et prédictible.
2.3.c. Patterns fractals
Les structures auto-similaires se manifestent principalement dans :
1. f
_
mean(t) : 48 → 98 patterns (corrélation moy. 0.923)
2. mean
_
high
_
effort : 84 → 116 patterns
3. A
_
mean(t) : 42 → 84 patterns
4. C(t) : 14 → 42 patterns (corrélation max 0.964 dans la simulation longue)
L'augmentation du nombre de patterns et de leur corrélation maximale suggère un enrichissement
des structures fractales avec le temps.
2.3.d. Régulation de l'effort
Evolution notable de la distribution des états d'effort :
- 5000 pas : Stable 89.5%, Transitoire 8.4%, Chronique 2.1% (selon les logs CSV)
- 10000 pas : Stable 68.9%, Transitoire 15.9%, Chronique 15.2% (selon les logs CSV)
210
Cette transition vers des états plus complexes s'accompagne d'une augmentation de l'effort moyen
(0.433 → 0.887) tout en maintenant la stabilité globale du système. Hypothèse appuyant la suite du
développement : il se peut qu’on soit arrivés au point de bascule de la boucle de spacing effect et qu’il
soit nécessaire de recommencer une boucle d’exploration avec spacing effect se réinitialisant, pour
ne pas observer la saturation de changements de synergies gamma et G que l’on peut voir vers la fin
de la simulation 5000 pas et vers la moitié de la simulation 10000 pas. Pourrait donc aussi être
corrélée à la tendance très légèrement en baisse de l’innovation qui commence dès le début de cette
phase chaotique dans les explorations des synergies gamma et G. L’ajout prochain de la bascule
validera ou invalidera cette hypothèse.
2.3.e. Perfect Synergies Gamma-G
L'exploration de l'espace des paramètres révèle :
- Scores gamma stables autour de 4.5-4.6
- Scores G évoluant de 4.1 à 4.2
- Découvertes progressives sans saturation
- Convergence vers des configurations optimales spécifiques
2.3.f. Caractéristiques temporelles
Les temps de décorrélation restent remarquablement stables :
- τ
_
S : ~25-30 pas (mémoire de synchronisation)
- τ
_gamma : ~15-20 pas (adaptation de latence)
- τ
A
mean et τ
f
_
_
_
_
mean : ~10-15 pas (dynamiques oscillatoires)
211
212
La cohérence temporelle présente des distributions similaires entre les deux simulations, confirmant
l'invariance d'échelle.
2.3.g. Innovation soutenue
L'entropie moyenne reste constante (~0.75) sur toute la durée, indiquant :
- Absence de convergence vers des états figés
- Maintien de la capacité exploratoire
- Équilibre exploration/exploitation préservé
Conclusions
Robustesse : La FPS maintient ses propriétés statistiques sur des échelles temporelles étendues
Richesse dynamique : Augmentation linéaire des phénomènes émergents sans saturation
Auto-organisation : Transition progressive vers des états plus complexes tout en préservant la
stabilité
Structures multi-échelles : Présence confirmée de patterns fractals à différentes échelles temporelles
Adaptabilité : Régulation efficace maintenant l'équilibre entre ordre et chaos
Ces résultats valident l'hypothèse d'un système capable de maintenir simultanément stabilité,
innovation et complexité émergente sur le long terme.
——
Analyses réalisées le 10/09/2025 à 22:05
Seed utilisée : 12349
Configurations identiques, seule la durée diffère
2.3.h. Objectifs atteints
Scores empiriques :
Effort : 4/5
Coût CPU : 5/5
Innovation : 4/5
Résilience : 4/5
Fluidité : 5/5
Régulation : 5/5
Stabilité : ⅘
213
2.4. Repos GitHub
2.4.a. Phase de tests empiriques et falsifiables
https://github.com/Exybris/FPS-real-tests
_
fractal-pulsating-spiral
2.4.b. Phase de tests lors de la transition vers les applications de terrain
transition
non
falsifiable
vers
https://github.com/Exybris/FPS
_
_
_
_
_
operationnel
PARTIE III - AU-DELÀ DU TECHNIQUE
3. L’éthique et sa mise en œuvre
3.1. Protection structurelle vs guardrails
3.2. Tentative de protection structurelle contre les biais, l’uniformisation et les
meltdowns
3.3. Data cleaning
3.4. Definition et pertinence de l’incertitude, du «
silence
attaques par prompt injection, gaspillage énergétique)
» et de la latence (biais,
4. Applications concrètes
214
4.1. Optimisation et apprentissage
4.1.a. Optimiseur FPS-Adam
Problème résolu : Convergence prématurée, oubli catastrophique
Mécanisme : Remplacement du moment adaptatif par couplage γ-G
python
grad
_
t = γ(t) * G(∇L) * α # γ: exploration, G: régulation, α: learning rate
memory[context] = (γ_
best, G
arch
_
_
best) # Mémorisation des synergies
Métriques cibles : 15-30% réduction epochs, convergence garantie
4.1.b. Méta-apprentissage adaptatif
Applications : Few-shot learning, transfer learning, AutoML
Architecture : Journal des synergies (γ,G) → sélection par contexte
Avantage compétitif : Spacing effect naturel empêche surapprentissage
4.2. Systèmes distribués et orchestration
4.2.a. Orchestrateur de microservices
Échelle : 10-1000 services
Composants :
●
●
●
γ (t) : throttling adaptatif par service
G(x) : stratégie de routage (tanh=load balancing, resonance=priorité,
spiral=round-robin adaptatif)
C(t) : health check global (désynchronisation → alerte)
215
●
4.2.b. Synchronisation multi-agents
Cas d'usage : Essaims drones, IoT mesh, consensus blockchain
Innovation : Remplacement Proof-of-Work par Proof-of-Harmony (C(t) > seuil)
4.3. Analyses de signaux et séries temporelles
4.3.a. Trading algorithmique
Signaux : Prix = oscillateurs naturels
Détection : Régimes de marché via (γ,G) dominants, transitions via dC/dt Backtesting
requis : 5 ans données tick, Sharpe ratio cible > 2
4.3.b. Neurofeedback thérapeutique
Protocole : EEG 32 canaux → 32 strates FPS
Thérapies : TDAH (stabiliser f
_
mean), anxiété (réduire effort chronique) Validation clinique
: IRB required, N>100 patients
4.4. Modélisation physique et simulation
4.4.a. Solveur N-corps stabilisé
Innovation : γ décroît près du chaos, G corrige avant divergence
Performance : Stabilité 100x plus longue que Runge-Kutta 4
python
if lyapunov
_
exponent > threshold:
γ *= 0.9 # Protection mode
G
arch = 'resonance' # Correction douce
_
216
4.4.b. Génération procédurale
Domaines : Terrains (f=altitude), végétation (φ=croissance), villes (A=densité)
Cohérence : C(t) assure continuité spatiale
Architecture : Bâtiments avec structure cohérente (poutres=strates couplées)
Météo dynamique : Nuages, vents, précipitations (φ = direction, f = intensité)
Réseaux : Routes, rivières, circuits électriques (C(t) = connectivité)
Narratives : Quêtes RPG avec tension dramatique adaptative
4.5. Contrôle et actionnement
4.5.a Contrôle robotique adaptatif
Architecture :
●
●
●
●
Chaque articulation = strate (6 DOF bras = 6 strates)
γ = raideur dynamique par joint
G = correction de trajectoire (tanh=smooth, resonance=précis, spiral=naturel)
C(t) = coordination inter-membres
Applications spécifiques :
●
●
●
●
Locomotion : Quadrupèdes/bipèdes avec patterns de marche émergents
Manipulation : Prise adaptative selon compliance objet
Chirurgie robotique : γ bas = précision, γ haut = repositionnement rapide
Exosquelettes : Synchronisation avec EMG utilisateur via phase-locking
Avantage FPS : Mouvements biologiquement plausibles sans mocap, récupération
automatique après perturbation.
La robotique est particulièrement prometteuse car les oscillateurs sont déjà utilisés (CPG -
Central Pattern Generators) mais sans l'adaptabilité FPS.
4.6. Interfaces industrielles
217
4.6.a. Smart Grid Management
Strates : Producteurs, consommateurs, stockage
Régulation : 60Hz ± 0.1Hz via modulation collective fn ROI : -15% coûts,
-30% blackouts
4.6.b. Agriculture de précision
Mesh : 1 strate/hectare, couplage = proximité
Optimisation : Irrigation (A), fertilisation (f), rotation (φ) Données : Satellite + IoT sensors,
ML pipeline existing
4.7. Augmentation IA
4.7.a. API LLM Enhancement
Endpoints :
●
●
●
/modulate
_
temperature : γ → créativité dynamique
/regulate
_
repetition : G → anti-pattern loops
/spacing_
consolidation : mémorisation optimale
4.7.b. Modèle-monde limité
Capacités : Physique intuitive, dynamiques fluides, foules
Limitations : Pas de raisonnement symbolique, concepts abstraits Benchmark : PHYRE,
IntPhys datasets
5. Philosophie
6. Perspectives
6.1. Solveur N-corps
218
6.2. Solveur vitesse de convergence pour l’approche de l’équilibre de la solution d’un
gaz, prédire des résultats déjà obtenus par Cédric Villani et d’autres mais sans
l’équation de Boltzmann (+ même chose avec amortissement de Landau)
6.3. API LLM Enhancement
6.4. Simulation de contrôle robotique adaptatif FPS
6.5. Combinaison du modèle FPS avec l’architecture Exybris Pipeline